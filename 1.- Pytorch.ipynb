{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;width:100%; height:120px;align-items: center; padding:20px; box-sizing:border-box;\">\n",
    "    <h1 style=\"margin:0;padding:0;\" >PyTorch</h1>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c6/PyTorch_logo_black.svg\" \n",
    "alt=\"PyTorch.log\" style=\"height:110px;margin:0;passing:0;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n",
    "\n",
    "The features of the documentation are classified as:\n",
    "* *Stable*: These features will be maintained long-term\n",
    "* *Beta*: These features are tagged as Beta because the API may change based on user feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best ways to undertand how a package works is using simulate data and try in several situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import getitem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([70])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 4, 6) \n",
    "y = torch.rand(2, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.6453],\n",
       "         [1.0561],\n",
       "         [2.2376],\n",
       "         [2.2982]],\n",
       "\n",
       "        [[1.8947],\n",
       "         [1.7999],\n",
       "         [1.4242],\n",
       "         [1.5013]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8947],\n",
       "        [1.7999],\n",
       "        [1.4242],\n",
       "        [1.5013]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]@y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function we are defined is \n",
    "# y = sin(x1*b1 + x2*b2) + 1.6 + e\n",
    "\n",
    "size= 64*30\n",
    "b = torch.tensor([1.2, 0.8])\n",
    "x = torch.rand((size, 2))*2\n",
    "e = torch.randn(size, 1)\n",
    "y = torch.sin((x*b).sum(axis=1, keepdims=True)) + e*0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomDataset(x, y)\n",
    "data_train, data_test = random_split(data, [0.8, 0.2])\n",
    "data_train_loader = DataLoader(data_train, batch_size=1,shuffle=True)\n",
    "data_test_loader = DataLoader(data_test, batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predict = self.stack(x)\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # doesn't directly perform calculus itself, \n",
    "    # it sets a crucial flag that enables the model to learn through backpropagation,\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # The model returns the prediction using forward propagation\n",
    "        pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # sets all parameter gradients to zero.\n",
    "        # Without resetting, gradients from previous backpropagation steps would accumulate,\n",
    "        # leading to incorrect updates and potential model instability.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # It triggers the backpropagation process, \n",
    "        # Calculate the gradients of the loss with respect to the model's parameters.\n",
    "        # and save the gradients in the model's parameters\n",
    "        # To access to the gradients, you can pass\n",
    "        # getitem(model.stack, 0).weight.grad\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            loss, cumm = loss.item(), batch_idx * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{cumm:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.420252  [    0/ 1536]\n",
      "loss: 3.594970  [  100/ 1536]\n",
      "loss: 1.484627  [  200/ 1536]\n",
      "loss: 1.200437  [  300/ 1536]\n",
      "loss: 0.842341  [  400/ 1536]\n",
      "loss: 0.933452  [  500/ 1536]\n",
      "loss: 1.046643  [  600/ 1536]\n",
      "loss: 1.153662  [  700/ 1536]\n",
      "loss: 0.000322  [  800/ 1536]\n",
      "loss: 0.686198  [  900/ 1536]\n",
      "loss: 1.996563  [ 1000/ 1536]\n",
      "loss: 1.785800  [ 1100/ 1536]\n",
      "loss: 0.496343  [ 1200/ 1536]\n",
      "loss: 0.937858  [ 1300/ 1536]\n",
      "loss: 0.972958  [ 1400/ 1536]\n",
      "loss: 0.560137  [ 1500/ 1536]\n",
      "loss: 1.151197  [    0/ 1536]\n",
      "loss: 1.011783  [  100/ 1536]\n",
      "loss: 0.900735  [  200/ 1536]\n",
      "loss: 0.359326  [  300/ 1536]\n",
      "loss: 0.066955  [  400/ 1536]\n",
      "loss: 0.591582  [  500/ 1536]\n",
      "loss: 0.466826  [  600/ 1536]\n",
      "loss: 0.056516  [  700/ 1536]\n",
      "loss: 0.626278  [  800/ 1536]\n",
      "loss: 0.040350  [  900/ 1536]\n",
      "loss: 0.711107  [ 1000/ 1536]\n",
      "loss: 0.292288  [ 1100/ 1536]\n",
      "loss: 0.002764  [ 1200/ 1536]\n",
      "loss: 0.309010  [ 1300/ 1536]\n",
      "loss: 0.486706  [ 1400/ 1536]\n",
      "loss: 0.562532  [ 1500/ 1536]\n",
      "loss: 1.105213  [    0/ 1536]\n",
      "loss: 0.387184  [  100/ 1536]\n",
      "loss: 1.236295  [  200/ 1536]\n",
      "loss: 0.313377  [  300/ 1536]\n",
      "loss: 0.767245  [  400/ 1536]\n",
      "loss: 0.057925  [  500/ 1536]\n",
      "loss: 0.393591  [  600/ 1536]\n",
      "loss: 0.036975  [  700/ 1536]\n",
      "loss: 0.000308  [  800/ 1536]\n",
      "loss: 0.556355  [  900/ 1536]\n",
      "loss: 0.014107  [ 1000/ 1536]\n",
      "loss: 0.787646  [ 1100/ 1536]\n",
      "loss: 0.008087  [ 1200/ 1536]\n",
      "loss: 0.574958  [ 1300/ 1536]\n",
      "loss: 0.069063  [ 1400/ 1536]\n",
      "loss: 0.130058  [ 1500/ 1536]\n",
      "loss: 0.224033  [    0/ 1536]\n",
      "loss: 0.064544  [  100/ 1536]\n",
      "loss: 0.132719  [  200/ 1536]\n",
      "loss: 0.201638  [  300/ 1536]\n",
      "loss: 0.123667  [  400/ 1536]\n",
      "loss: 0.062617  [  500/ 1536]\n",
      "loss: 0.092169  [  600/ 1536]\n",
      "loss: 0.009252  [  700/ 1536]\n",
      "loss: 0.026702  [  800/ 1536]\n",
      "loss: 0.150916  [  900/ 1536]\n",
      "loss: 0.141549  [ 1000/ 1536]\n",
      "loss: 0.010195  [ 1100/ 1536]\n",
      "loss: 2.240754  [ 1200/ 1536]\n",
      "loss: 0.000018  [ 1300/ 1536]\n",
      "loss: 0.547037  [ 1400/ 1536]\n",
      "loss: 0.439086  [ 1500/ 1536]\n",
      "loss: 0.536886  [    0/ 1536]\n",
      "loss: 0.106313  [  100/ 1536]\n",
      "loss: 0.000917  [  200/ 1536]\n",
      "loss: 0.153302  [  300/ 1536]\n",
      "loss: 0.023669  [  400/ 1536]\n",
      "loss: 0.156793  [  500/ 1536]\n",
      "loss: 0.066961  [  600/ 1536]\n",
      "loss: 0.156113  [  700/ 1536]\n",
      "loss: 0.252147  [  800/ 1536]\n",
      "loss: 0.380245  [  900/ 1536]\n",
      "loss: 0.488847  [ 1000/ 1536]\n",
      "loss: 0.055058  [ 1100/ 1536]\n",
      "loss: 0.184338  [ 1200/ 1536]\n",
      "loss: 0.013778  [ 1300/ 1536]\n",
      "loss: 0.252715  [ 1400/ 1536]\n",
      "loss: 0.304490  [ 1500/ 1536]\n",
      "loss: 1.083335  [    0/ 1536]\n",
      "loss: 0.202445  [  100/ 1536]\n",
      "loss: 0.149334  [  200/ 1536]\n",
      "loss: 0.186200  [  300/ 1536]\n",
      "loss: 0.078687  [  400/ 1536]\n",
      "loss: 0.067052  [  500/ 1536]\n",
      "loss: 0.024567  [  600/ 1536]\n",
      "loss: 0.048031  [  700/ 1536]\n",
      "loss: 0.004819  [  800/ 1536]\n",
      "loss: 0.102201  [  900/ 1536]\n",
      "loss: 0.260552  [ 1000/ 1536]\n",
      "loss: 0.004121  [ 1100/ 1536]\n",
      "loss: 0.228933  [ 1200/ 1536]\n",
      "loss: 0.039929  [ 1300/ 1536]\n",
      "loss: 0.045836  [ 1400/ 1536]\n",
      "loss: 0.231261  [ 1500/ 1536]\n",
      "loss: 0.155282  [    0/ 1536]\n",
      "loss: 0.467936  [  100/ 1536]\n",
      "loss: 0.036253  [  200/ 1536]\n",
      "loss: 0.139577  [  300/ 1536]\n",
      "loss: 0.700109  [  400/ 1536]\n",
      "loss: 0.077799  [  500/ 1536]\n",
      "loss: 0.137063  [  600/ 1536]\n",
      "loss: 0.822317  [  700/ 1536]\n",
      "loss: 0.009268  [  800/ 1536]\n",
      "loss: 0.863070  [  900/ 1536]\n",
      "loss: 0.186487  [ 1000/ 1536]\n",
      "loss: 0.011773  [ 1100/ 1536]\n",
      "loss: 0.098337  [ 1200/ 1536]\n",
      "loss: 0.012871  [ 1300/ 1536]\n",
      "loss: 0.001252  [ 1400/ 1536]\n",
      "loss: 0.230596  [ 1500/ 1536]\n",
      "loss: 0.095878  [    0/ 1536]\n",
      "loss: 0.069106  [  100/ 1536]\n",
      "loss: 0.025718  [  200/ 1536]\n",
      "loss: 0.118623  [  300/ 1536]\n",
      "loss: 0.243677  [  400/ 1536]\n",
      "loss: 0.008711  [  500/ 1536]\n",
      "loss: 0.004471  [  600/ 1536]\n",
      "loss: 0.002259  [  700/ 1536]\n",
      "loss: 0.002164  [  800/ 1536]\n",
      "loss: 0.488625  [  900/ 1536]\n",
      "loss: 0.387215  [ 1000/ 1536]\n",
      "loss: 0.377486  [ 1100/ 1536]\n",
      "loss: 0.449759  [ 1200/ 1536]\n",
      "loss: 0.114292  [ 1300/ 1536]\n",
      "loss: 0.126856  [ 1400/ 1536]\n",
      "loss: 0.124590  [ 1500/ 1536]\n",
      "loss: 0.003038  [    0/ 1536]\n",
      "loss: 0.569589  [  100/ 1536]\n",
      "loss: 0.253811  [  200/ 1536]\n",
      "loss: 0.005579  [  300/ 1536]\n",
      "loss: 0.001480  [  400/ 1536]\n",
      "loss: 0.200175  [  500/ 1536]\n",
      "loss: 0.003570  [  600/ 1536]\n",
      "loss: 0.000964  [  700/ 1536]\n",
      "loss: 0.017746  [  800/ 1536]\n",
      "loss: 0.114067  [  900/ 1536]\n",
      "loss: 0.012311  [ 1000/ 1536]\n",
      "loss: 0.010000  [ 1100/ 1536]\n",
      "loss: 0.010483  [ 1200/ 1536]\n",
      "loss: 0.145329  [ 1300/ 1536]\n",
      "loss: 0.000802  [ 1400/ 1536]\n",
      "loss: 0.082899  [ 1500/ 1536]\n",
      "loss: 0.158830  [    0/ 1536]\n",
      "loss: 0.304042  [  100/ 1536]\n",
      "loss: 0.668579  [  200/ 1536]\n",
      "loss: 0.750591  [  300/ 1536]\n",
      "loss: 0.913083  [  400/ 1536]\n",
      "loss: 0.418828  [  500/ 1536]\n",
      "loss: 0.016329  [  600/ 1536]\n",
      "loss: 0.165063  [  700/ 1536]\n",
      "loss: 0.084898  [  800/ 1536]\n",
      "loss: 0.086499  [  900/ 1536]\n",
      "loss: 0.311620  [ 1000/ 1536]\n",
      "loss: 0.222054  [ 1100/ 1536]\n",
      "loss: 0.024545  [ 1200/ 1536]\n",
      "loss: 0.226566  [ 1300/ 1536]\n",
      "loss: 0.184413  [ 1400/ 1536]\n",
      "loss: 0.397618  [ 1500/ 1536]\n",
      "loss: 0.247626  [    0/ 1536]\n",
      "loss: 0.114108  [  100/ 1536]\n",
      "loss: 0.027735  [  200/ 1536]\n",
      "loss: 0.012109  [  300/ 1536]\n",
      "loss: 0.261377  [  400/ 1536]\n",
      "loss: 0.021138  [  500/ 1536]\n",
      "loss: 0.089545  [  600/ 1536]\n",
      "loss: 0.460280  [  700/ 1536]\n",
      "loss: 0.034220  [  800/ 1536]\n",
      "loss: 0.445564  [  900/ 1536]\n",
      "loss: 0.351374  [ 1000/ 1536]\n",
      "loss: 0.001432  [ 1100/ 1536]\n",
      "loss: 0.019758  [ 1200/ 1536]\n",
      "loss: 0.017429  [ 1300/ 1536]\n",
      "loss: 0.066530  [ 1400/ 1536]\n",
      "loss: 0.055012  [ 1500/ 1536]\n",
      "loss: 0.000047  [    0/ 1536]\n",
      "loss: 0.073614  [  100/ 1536]\n",
      "loss: 0.007441  [  200/ 1536]\n",
      "loss: 0.057941  [  300/ 1536]\n",
      "loss: 0.450504  [  400/ 1536]\n",
      "loss: 0.298981  [  500/ 1536]\n",
      "loss: 0.085599  [  600/ 1536]\n",
      "loss: 0.269583  [  700/ 1536]\n",
      "loss: 0.131235  [  800/ 1536]\n",
      "loss: 0.176606  [  900/ 1536]\n",
      "loss: 0.052622  [ 1000/ 1536]\n",
      "loss: 0.234519  [ 1100/ 1536]\n",
      "loss: 0.037337  [ 1200/ 1536]\n",
      "loss: 0.133610  [ 1300/ 1536]\n",
      "loss: 0.121077  [ 1400/ 1536]\n",
      "loss: 0.005434  [ 1500/ 1536]\n",
      "loss: 0.007548  [    0/ 1536]\n",
      "loss: 0.017660  [  100/ 1536]\n",
      "loss: 0.035718  [  200/ 1536]\n",
      "loss: 0.011133  [  300/ 1536]\n",
      "loss: 0.003098  [  400/ 1536]\n",
      "loss: 0.099861  [  500/ 1536]\n",
      "loss: 0.019924  [  600/ 1536]\n",
      "loss: 0.060803  [  700/ 1536]\n",
      "loss: 0.251922  [  800/ 1536]\n",
      "loss: 0.010344  [  900/ 1536]\n",
      "loss: 0.317684  [ 1000/ 1536]\n",
      "loss: 0.643779  [ 1100/ 1536]\n",
      "loss: 0.301857  [ 1200/ 1536]\n",
      "loss: 0.568043  [ 1300/ 1536]\n",
      "loss: 0.255224  [ 1400/ 1536]\n",
      "loss: 0.000687  [ 1500/ 1536]\n",
      "loss: 0.710262  [    0/ 1536]\n",
      "loss: 0.043544  [  100/ 1536]\n",
      "loss: 0.002299  [  200/ 1536]\n",
      "loss: 0.282049  [  300/ 1536]\n",
      "loss: 0.732459  [  400/ 1536]\n",
      "loss: 0.086974  [  500/ 1536]\n",
      "loss: 0.344792  [  600/ 1536]\n",
      "loss: 0.008249  [  700/ 1536]\n",
      "loss: 0.162279  [  800/ 1536]\n",
      "loss: 0.030983  [  900/ 1536]\n",
      "loss: 0.313797  [ 1000/ 1536]\n",
      "loss: 0.019288  [ 1100/ 1536]\n",
      "loss: 0.035809  [ 1200/ 1536]\n",
      "loss: 0.009314  [ 1300/ 1536]\n",
      "loss: 0.000009  [ 1400/ 1536]\n",
      "loss: 0.003875  [ 1500/ 1536]\n",
      "loss: 0.000002  [    0/ 1536]\n",
      "loss: 0.223445  [  100/ 1536]\n",
      "loss: 0.031278  [  200/ 1536]\n",
      "loss: 0.108209  [  300/ 1536]\n",
      "loss: 0.437053  [  400/ 1536]\n",
      "loss: 0.050962  [  500/ 1536]\n",
      "loss: 0.036293  [  600/ 1536]\n",
      "loss: 0.004375  [  700/ 1536]\n",
      "loss: 0.000156  [  800/ 1536]\n",
      "loss: 0.398073  [  900/ 1536]\n",
      "loss: 0.289339  [ 1000/ 1536]\n",
      "loss: 0.144792  [ 1100/ 1536]\n",
      "loss: 0.109979  [ 1200/ 1536]\n",
      "loss: 0.008709  [ 1300/ 1536]\n",
      "loss: 0.168357  [ 1400/ 1536]\n",
      "loss: 0.187473  [ 1500/ 1536]\n",
      "loss: 0.088040  [    0/ 1536]\n",
      "loss: 0.020263  [  100/ 1536]\n",
      "loss: 0.003514  [  200/ 1536]\n",
      "loss: 0.325340  [  300/ 1536]\n",
      "loss: 0.046199  [  400/ 1536]\n",
      "loss: 0.176651  [  500/ 1536]\n",
      "loss: 0.001540  [  600/ 1536]\n",
      "loss: 0.300017  [  700/ 1536]\n",
      "loss: 0.146412  [  800/ 1536]\n",
      "loss: 0.074453  [  900/ 1536]\n",
      "loss: 0.481751  [ 1000/ 1536]\n",
      "loss: 0.005523  [ 1100/ 1536]\n",
      "loss: 0.236087  [ 1200/ 1536]\n",
      "loss: 0.007265  [ 1300/ 1536]\n",
      "loss: 0.006434  [ 1400/ 1536]\n",
      "loss: 0.054649  [ 1500/ 1536]\n",
      "loss: 0.064632  [    0/ 1536]\n",
      "loss: 0.002254  [  100/ 1536]\n",
      "loss: 0.466236  [  200/ 1536]\n",
      "loss: 0.283594  [  300/ 1536]\n",
      "loss: 0.001352  [  400/ 1536]\n",
      "loss: 0.099092  [  500/ 1536]\n",
      "loss: 0.091270  [  600/ 1536]\n",
      "loss: 0.046255  [  700/ 1536]\n",
      "loss: 0.015054  [  800/ 1536]\n",
      "loss: 0.143978  [  900/ 1536]\n",
      "loss: 0.017723  [ 1000/ 1536]\n",
      "loss: 0.368824  [ 1100/ 1536]\n",
      "loss: 0.024610  [ 1200/ 1536]\n",
      "loss: 0.085531  [ 1300/ 1536]\n",
      "loss: 0.512861  [ 1400/ 1536]\n",
      "loss: 0.153184  [ 1500/ 1536]\n",
      "loss: 0.043321  [    0/ 1536]\n",
      "loss: 0.002538  [  100/ 1536]\n",
      "loss: 0.096321  [  200/ 1536]\n",
      "loss: 0.246486  [  300/ 1536]\n",
      "loss: 0.004111  [  400/ 1536]\n",
      "loss: 0.042397  [  500/ 1536]\n",
      "loss: 0.029127  [  600/ 1536]\n",
      "loss: 0.079615  [  700/ 1536]\n",
      "loss: 0.013632  [  800/ 1536]\n",
      "loss: 0.008839  [  900/ 1536]\n",
      "loss: 0.001022  [ 1000/ 1536]\n",
      "loss: 0.003870  [ 1100/ 1536]\n",
      "loss: 0.159074  [ 1200/ 1536]\n",
      "loss: 0.000117  [ 1300/ 1536]\n",
      "loss: 0.001278  [ 1400/ 1536]\n",
      "loss: 0.197740  [ 1500/ 1536]\n",
      "loss: 0.188010  [    0/ 1536]\n",
      "loss: 0.046583  [  100/ 1536]\n",
      "loss: 0.116345  [  200/ 1536]\n",
      "loss: 0.007733  [  300/ 1536]\n",
      "loss: 0.000120  [  400/ 1536]\n",
      "loss: 0.017917  [  500/ 1536]\n",
      "loss: 0.166565  [  600/ 1536]\n",
      "loss: 0.012812  [  700/ 1536]\n",
      "loss: 0.029837  [  800/ 1536]\n",
      "loss: 0.004427  [  900/ 1536]\n",
      "loss: 0.020957  [ 1000/ 1536]\n",
      "loss: 0.473745  [ 1100/ 1536]\n",
      "loss: 0.002398  [ 1200/ 1536]\n",
      "loss: 0.064591  [ 1300/ 1536]\n",
      "loss: 0.010542  [ 1400/ 1536]\n",
      "loss: 0.097016  [ 1500/ 1536]\n",
      "loss: 0.221449  [    0/ 1536]\n",
      "loss: 0.006076  [  100/ 1536]\n",
      "loss: 0.140025  [  200/ 1536]\n",
      "loss: 0.049420  [  300/ 1536]\n",
      "loss: 0.042940  [  400/ 1536]\n",
      "loss: 0.002499  [  500/ 1536]\n",
      "loss: 0.044424  [  600/ 1536]\n",
      "loss: 0.008589  [  700/ 1536]\n",
      "loss: 0.050296  [  800/ 1536]\n",
      "loss: 0.125271  [  900/ 1536]\n",
      "loss: 0.050304  [ 1000/ 1536]\n",
      "loss: 0.000869  [ 1100/ 1536]\n",
      "loss: 0.059959  [ 1200/ 1536]\n",
      "loss: 0.003666  [ 1300/ 1536]\n",
      "loss: 0.069313  [ 1400/ 1536]\n",
      "loss: 0.288346  [ 1500/ 1536]\n",
      "loss: 0.002870  [    0/ 1536]\n",
      "loss: 0.223026  [  100/ 1536]\n",
      "loss: 0.000597  [  200/ 1536]\n",
      "loss: 0.043038  [  300/ 1536]\n",
      "loss: 0.035904  [  400/ 1536]\n",
      "loss: 0.070403  [  500/ 1536]\n",
      "loss: 0.078383  [  600/ 1536]\n",
      "loss: 0.009603  [  700/ 1536]\n",
      "loss: 0.026254  [  800/ 1536]\n",
      "loss: 0.326557  [  900/ 1536]\n",
      "loss: 0.284347  [ 1000/ 1536]\n",
      "loss: 0.610231  [ 1100/ 1536]\n",
      "loss: 0.033280  [ 1200/ 1536]\n",
      "loss: 0.207634  [ 1300/ 1536]\n",
      "loss: 0.005701  [ 1400/ 1536]\n",
      "loss: 0.087217  [ 1500/ 1536]\n",
      "loss: 0.018263  [    0/ 1536]\n",
      "loss: 0.113391  [  100/ 1536]\n",
      "loss: 0.022325  [  200/ 1536]\n",
      "loss: 0.006658  [  300/ 1536]\n",
      "loss: 0.203877  [  400/ 1536]\n",
      "loss: 0.021435  [  500/ 1536]\n",
      "loss: 0.027940  [  600/ 1536]\n",
      "loss: 0.140831  [  700/ 1536]\n",
      "loss: 0.027496  [  800/ 1536]\n",
      "loss: 0.189953  [  900/ 1536]\n",
      "loss: 0.111746  [ 1000/ 1536]\n",
      "loss: 0.286195  [ 1100/ 1536]\n",
      "loss: 0.069947  [ 1200/ 1536]\n",
      "loss: 0.083516  [ 1300/ 1536]\n",
      "loss: 0.059006  [ 1400/ 1536]\n",
      "loss: 0.433578  [ 1500/ 1536]\n",
      "loss: 0.111520  [    0/ 1536]\n",
      "loss: 0.041444  [  100/ 1536]\n",
      "loss: 0.026045  [  200/ 1536]\n",
      "loss: 0.000445  [  300/ 1536]\n",
      "loss: 0.030512  [  400/ 1536]\n",
      "loss: 0.057567  [  500/ 1536]\n",
      "loss: 0.053171  [  600/ 1536]\n",
      "loss: 0.200011  [  700/ 1536]\n",
      "loss: 0.004657  [  800/ 1536]\n",
      "loss: 0.047860  [  900/ 1536]\n",
      "loss: 0.019309  [ 1000/ 1536]\n",
      "loss: 0.024528  [ 1100/ 1536]\n",
      "loss: 0.038634  [ 1200/ 1536]\n",
      "loss: 0.211929  [ 1300/ 1536]\n",
      "loss: 0.129900  [ 1400/ 1536]\n",
      "loss: 0.412327  [ 1500/ 1536]\n",
      "loss: 0.065941  [    0/ 1536]\n",
      "loss: 0.008221  [  100/ 1536]\n",
      "loss: 0.069702  [  200/ 1536]\n",
      "loss: 0.300403  [  300/ 1536]\n",
      "loss: 0.043150  [  400/ 1536]\n",
      "loss: 0.054740  [  500/ 1536]\n",
      "loss: 0.015592  [  600/ 1536]\n",
      "loss: 0.111472  [  700/ 1536]\n",
      "loss: 0.134006  [  800/ 1536]\n",
      "loss: 0.078547  [  900/ 1536]\n",
      "loss: 0.176034  [ 1000/ 1536]\n",
      "loss: 0.248541  [ 1100/ 1536]\n",
      "loss: 0.249089  [ 1200/ 1536]\n",
      "loss: 0.009414  [ 1300/ 1536]\n",
      "loss: 0.004794  [ 1400/ 1536]\n",
      "loss: 0.121928  [ 1500/ 1536]\n",
      "loss: 0.037062  [    0/ 1536]\n",
      "loss: 0.000006  [  100/ 1536]\n",
      "loss: 0.000000  [  200/ 1536]\n",
      "loss: 0.117518  [  300/ 1536]\n",
      "loss: 0.004144  [  400/ 1536]\n",
      "loss: 0.034988  [  500/ 1536]\n",
      "loss: 0.036245  [  600/ 1536]\n",
      "loss: 0.003213  [  700/ 1536]\n",
      "loss: 0.163455  [  800/ 1536]\n",
      "loss: 0.061455  [  900/ 1536]\n",
      "loss: 0.004548  [ 1000/ 1536]\n",
      "loss: 0.112689  [ 1100/ 1536]\n",
      "loss: 0.138666  [ 1200/ 1536]\n",
      "loss: 0.023534  [ 1300/ 1536]\n",
      "loss: 0.512862  [ 1400/ 1536]\n",
      "loss: 0.055876  [ 1500/ 1536]\n",
      "loss: 0.201167  [    0/ 1536]\n",
      "loss: 0.182836  [  100/ 1536]\n",
      "loss: 0.013587  [  200/ 1536]\n",
      "loss: 0.007298  [  300/ 1536]\n",
      "loss: 0.096017  [  400/ 1536]\n",
      "loss: 0.116024  [  500/ 1536]\n",
      "loss: 0.000551  [  600/ 1536]\n",
      "loss: 0.016844  [  700/ 1536]\n",
      "loss: 0.003810  [  800/ 1536]\n",
      "loss: 0.109754  [  900/ 1536]\n",
      "loss: 0.000299  [ 1000/ 1536]\n",
      "loss: 0.118624  [ 1100/ 1536]\n",
      "loss: 0.019379  [ 1200/ 1536]\n",
      "loss: 0.000041  [ 1300/ 1536]\n",
      "loss: 0.005168  [ 1400/ 1536]\n",
      "loss: 0.004107  [ 1500/ 1536]\n",
      "loss: 0.000395  [    0/ 1536]\n",
      "loss: 0.115681  [  100/ 1536]\n",
      "loss: 0.027918  [  200/ 1536]\n",
      "loss: 0.012934  [  300/ 1536]\n",
      "loss: 0.008548  [  400/ 1536]\n",
      "loss: 0.107406  [  500/ 1536]\n",
      "loss: 0.013688  [  600/ 1536]\n",
      "loss: 0.003770  [  700/ 1536]\n",
      "loss: 0.003053  [  800/ 1536]\n",
      "loss: 0.005088  [  900/ 1536]\n",
      "loss: 0.038268  [ 1000/ 1536]\n",
      "loss: 0.027786  [ 1100/ 1536]\n",
      "loss: 0.203435  [ 1200/ 1536]\n",
      "loss: 0.137625  [ 1300/ 1536]\n",
      "loss: 0.008195  [ 1400/ 1536]\n",
      "loss: 0.130103  [ 1500/ 1536]\n",
      "loss: 0.098775  [    0/ 1536]\n",
      "loss: 0.147294  [  100/ 1536]\n",
      "loss: 0.301927  [  200/ 1536]\n",
      "loss: 0.004126  [  300/ 1536]\n",
      "loss: 0.042442  [  400/ 1536]\n",
      "loss: 0.077799  [  500/ 1536]\n",
      "loss: 0.116300  [  600/ 1536]\n",
      "loss: 0.057105  [  700/ 1536]\n",
      "loss: 0.226592  [  800/ 1536]\n",
      "loss: 0.014258  [  900/ 1536]\n",
      "loss: 0.163769  [ 1000/ 1536]\n",
      "loss: 0.011669  [ 1100/ 1536]\n",
      "loss: 0.007706  [ 1200/ 1536]\n",
      "loss: 0.012177  [ 1300/ 1536]\n",
      "loss: 0.006991  [ 1400/ 1536]\n",
      "loss: 0.016595  [ 1500/ 1536]\n",
      "loss: 0.000498  [    0/ 1536]\n",
      "loss: 0.289311  [  100/ 1536]\n",
      "loss: 0.011121  [  200/ 1536]\n",
      "loss: 0.339223  [  300/ 1536]\n",
      "loss: 0.151874  [  400/ 1536]\n",
      "loss: 0.078937  [  500/ 1536]\n",
      "loss: 0.003308  [  600/ 1536]\n",
      "loss: 0.026337  [  700/ 1536]\n",
      "loss: 0.194745  [  800/ 1536]\n",
      "loss: 0.212097  [  900/ 1536]\n",
      "loss: 0.045211  [ 1000/ 1536]\n",
      "loss: 0.005669  [ 1100/ 1536]\n",
      "loss: 0.006335  [ 1200/ 1536]\n",
      "loss: 0.228758  [ 1300/ 1536]\n",
      "loss: 0.017776  [ 1400/ 1536]\n",
      "loss: 0.071296  [ 1500/ 1536]\n",
      "loss: 0.089199  [    0/ 1536]\n",
      "loss: 0.004089  [  100/ 1536]\n",
      "loss: 0.027524  [  200/ 1536]\n",
      "loss: 0.036724  [  300/ 1536]\n",
      "loss: 0.049303  [  400/ 1536]\n",
      "loss: 0.047280  [  500/ 1536]\n",
      "loss: 0.107370  [  600/ 1536]\n",
      "loss: 0.019379  [  700/ 1536]\n",
      "loss: 0.073770  [  800/ 1536]\n",
      "loss: 0.000534  [  900/ 1536]\n",
      "loss: 0.089355  [ 1000/ 1536]\n",
      "loss: 0.003491  [ 1100/ 1536]\n",
      "loss: 0.003227  [ 1200/ 1536]\n",
      "loss: 0.014483  [ 1300/ 1536]\n",
      "loss: 0.004525  [ 1400/ 1536]\n",
      "loss: 0.198260  [ 1500/ 1536]\n",
      "loss: 0.010367  [    0/ 1536]\n",
      "loss: 0.003406  [  100/ 1536]\n",
      "loss: 0.006384  [  200/ 1536]\n",
      "loss: 0.011701  [  300/ 1536]\n",
      "loss: 0.000529  [  400/ 1536]\n",
      "loss: 0.134733  [  500/ 1536]\n",
      "loss: 0.016630  [  600/ 1536]\n",
      "loss: 0.142458  [  700/ 1536]\n",
      "loss: 0.006078  [  800/ 1536]\n",
      "loss: 0.020014  [  900/ 1536]\n",
      "loss: 0.064249  [ 1000/ 1536]\n",
      "loss: 0.058115  [ 1100/ 1536]\n",
      "loss: 0.002714  [ 1200/ 1536]\n",
      "loss: 0.073771  [ 1300/ 1536]\n",
      "loss: 0.002500  [ 1400/ 1536]\n",
      "loss: 0.369846  [ 1500/ 1536]\n",
      "loss: 0.037955  [    0/ 1536]\n",
      "loss: 0.024697  [  100/ 1536]\n",
      "loss: 0.002811  [  200/ 1536]\n",
      "loss: 0.037907  [  300/ 1536]\n",
      "loss: 0.082043  [  400/ 1536]\n",
      "loss: 0.147809  [  500/ 1536]\n",
      "loss: 0.032109  [  600/ 1536]\n",
      "loss: 0.184300  [  700/ 1536]\n",
      "loss: 0.024953  [  800/ 1536]\n",
      "loss: 0.004115  [  900/ 1536]\n",
      "loss: 0.147733  [ 1000/ 1536]\n",
      "loss: 0.001140  [ 1100/ 1536]\n",
      "loss: 0.324288  [ 1200/ 1536]\n",
      "loss: 0.089557  [ 1300/ 1536]\n",
      "loss: 0.034380  [ 1400/ 1536]\n",
      "loss: 0.138365  [ 1500/ 1536]\n",
      "loss: 0.002695  [    0/ 1536]\n",
      "loss: 0.078337  [  100/ 1536]\n",
      "loss: 0.017826  [  200/ 1536]\n",
      "loss: 0.040737  [  300/ 1536]\n",
      "loss: 0.000979  [  400/ 1536]\n",
      "loss: 0.144933  [  500/ 1536]\n",
      "loss: 0.043713  [  600/ 1536]\n",
      "loss: 0.070858  [  700/ 1536]\n",
      "loss: 0.001344  [  800/ 1536]\n",
      "loss: 0.044248  [  900/ 1536]\n",
      "loss: 0.046199  [ 1000/ 1536]\n",
      "loss: 0.014923  [ 1100/ 1536]\n",
      "loss: 0.054336  [ 1200/ 1536]\n",
      "loss: 0.196690  [ 1300/ 1536]\n",
      "loss: 0.025512  [ 1400/ 1536]\n",
      "loss: 0.047559  [ 1500/ 1536]\n",
      "loss: 0.031717  [    0/ 1536]\n",
      "loss: 0.051472  [  100/ 1536]\n",
      "loss: 0.011140  [  200/ 1536]\n",
      "loss: 0.006130  [  300/ 1536]\n",
      "loss: 0.031559  [  400/ 1536]\n",
      "loss: 0.020507  [  500/ 1536]\n",
      "loss: 0.000054  [  600/ 1536]\n",
      "loss: 0.130130  [  700/ 1536]\n",
      "loss: 0.010276  [  800/ 1536]\n",
      "loss: 0.163906  [  900/ 1536]\n",
      "loss: 0.281074  [ 1000/ 1536]\n",
      "loss: 0.024620  [ 1100/ 1536]\n",
      "loss: 0.085398  [ 1200/ 1536]\n",
      "loss: 0.072720  [ 1300/ 1536]\n",
      "loss: 0.096562  [ 1400/ 1536]\n",
      "loss: 0.443886  [ 1500/ 1536]\n",
      "loss: 0.258001  [    0/ 1536]\n",
      "loss: 0.060730  [  100/ 1536]\n",
      "loss: 0.005359  [  200/ 1536]\n",
      "loss: 0.000195  [  300/ 1536]\n",
      "loss: 0.064513  [  400/ 1536]\n",
      "loss: 0.380846  [  500/ 1536]\n",
      "loss: 0.000044  [  600/ 1536]\n",
      "loss: 0.018343  [  700/ 1536]\n",
      "loss: 0.005427  [  800/ 1536]\n",
      "loss: 0.049078  [  900/ 1536]\n",
      "loss: 0.000212  [ 1000/ 1536]\n",
      "loss: 0.031102  [ 1100/ 1536]\n",
      "loss: 0.016984  [ 1200/ 1536]\n",
      "loss: 0.000466  [ 1300/ 1536]\n",
      "loss: 0.139933  [ 1400/ 1536]\n",
      "loss: 0.017211  [ 1500/ 1536]\n",
      "loss: 0.003251  [    0/ 1536]\n",
      "loss: 0.031506  [  100/ 1536]\n",
      "loss: 0.052204  [  200/ 1536]\n",
      "loss: 0.047360  [  300/ 1536]\n",
      "loss: 0.125129  [  400/ 1536]\n",
      "loss: 0.052082  [  500/ 1536]\n",
      "loss: 0.006289  [  600/ 1536]\n",
      "loss: 0.000425  [  700/ 1536]\n",
      "loss: 0.033059  [  800/ 1536]\n",
      "loss: 0.068146  [  900/ 1536]\n",
      "loss: 0.091404  [ 1000/ 1536]\n",
      "loss: 0.002283  [ 1100/ 1536]\n",
      "loss: 0.001324  [ 1200/ 1536]\n",
      "loss: 0.000838  [ 1300/ 1536]\n",
      "loss: 0.065957  [ 1400/ 1536]\n",
      "loss: 0.033515  [ 1500/ 1536]\n",
      "loss: 0.008416  [    0/ 1536]\n",
      "loss: 0.048616  [  100/ 1536]\n",
      "loss: 0.063851  [  200/ 1536]\n",
      "loss: 0.021390  [  300/ 1536]\n",
      "loss: 0.047010  [  400/ 1536]\n",
      "loss: 0.003908  [  500/ 1536]\n",
      "loss: 0.071293  [  600/ 1536]\n",
      "loss: 0.027306  [  700/ 1536]\n",
      "loss: 0.000939  [  800/ 1536]\n",
      "loss: 0.119606  [  900/ 1536]\n",
      "loss: 0.000025  [ 1000/ 1536]\n",
      "loss: 0.011633  [ 1100/ 1536]\n",
      "loss: 0.054609  [ 1200/ 1536]\n",
      "loss: 0.000879  [ 1300/ 1536]\n",
      "loss: 0.016225  [ 1400/ 1536]\n",
      "loss: 0.072945  [ 1500/ 1536]\n",
      "loss: 0.116131  [    0/ 1536]\n",
      "loss: 0.018524  [  100/ 1536]\n",
      "loss: 0.198482  [  200/ 1536]\n",
      "loss: 0.019336  [  300/ 1536]\n",
      "loss: 0.037903  [  400/ 1536]\n",
      "loss: 0.046804  [  500/ 1536]\n",
      "loss: 0.029394  [  600/ 1536]\n",
      "loss: 0.025464  [  700/ 1536]\n",
      "loss: 0.000060  [  800/ 1536]\n",
      "loss: 0.130938  [  900/ 1536]\n",
      "loss: 0.000051  [ 1000/ 1536]\n",
      "loss: 0.012587  [ 1100/ 1536]\n",
      "loss: 0.001150  [ 1200/ 1536]\n",
      "loss: 0.068275  [ 1300/ 1536]\n",
      "loss: 0.000022  [ 1400/ 1536]\n",
      "loss: 0.039049  [ 1500/ 1536]\n",
      "loss: 0.065795  [    0/ 1536]\n",
      "loss: 0.007616  [  100/ 1536]\n",
      "loss: 0.113554  [  200/ 1536]\n",
      "loss: 0.017546  [  300/ 1536]\n",
      "loss: 0.000563  [  400/ 1536]\n",
      "loss: 0.106915  [  500/ 1536]\n",
      "loss: 0.010341  [  600/ 1536]\n",
      "loss: 0.104587  [  700/ 1536]\n",
      "loss: 0.034229  [  800/ 1536]\n",
      "loss: 0.021016  [  900/ 1536]\n",
      "loss: 0.015881  [ 1000/ 1536]\n",
      "loss: 0.115014  [ 1100/ 1536]\n",
      "loss: 0.006148  [ 1200/ 1536]\n",
      "loss: 0.024135  [ 1300/ 1536]\n",
      "loss: 0.007811  [ 1400/ 1536]\n",
      "loss: 0.000003  [ 1500/ 1536]\n",
      "loss: 0.062448  [    0/ 1536]\n",
      "loss: 0.000032  [  100/ 1536]\n",
      "loss: 0.120513  [  200/ 1536]\n",
      "loss: 0.020241  [  300/ 1536]\n",
      "loss: 0.015472  [  400/ 1536]\n",
      "loss: 0.024459  [  500/ 1536]\n",
      "loss: 0.144255  [  600/ 1536]\n",
      "loss: 0.081940  [  700/ 1536]\n",
      "loss: 0.002142  [  800/ 1536]\n",
      "loss: 0.000021  [  900/ 1536]\n",
      "loss: 0.065439  [ 1000/ 1536]\n",
      "loss: 0.132294  [ 1100/ 1536]\n",
      "loss: 0.000612  [ 1200/ 1536]\n",
      "loss: 0.086579  [ 1300/ 1536]\n",
      "loss: 0.222495  [ 1400/ 1536]\n",
      "loss: 0.006549  [ 1500/ 1536]\n",
      "loss: 0.022129  [    0/ 1536]\n",
      "loss: 0.009302  [  100/ 1536]\n",
      "loss: 0.072596  [  200/ 1536]\n",
      "loss: 0.000447  [  300/ 1536]\n",
      "loss: 0.109815  [  400/ 1536]\n",
      "loss: 0.109575  [  500/ 1536]\n",
      "loss: 0.020276  [  600/ 1536]\n",
      "loss: 0.006425  [  700/ 1536]\n",
      "loss: 0.010761  [  800/ 1536]\n",
      "loss: 0.061858  [  900/ 1536]\n",
      "loss: 0.238919  [ 1000/ 1536]\n",
      "loss: 0.002083  [ 1100/ 1536]\n",
      "loss: 0.040453  [ 1200/ 1536]\n",
      "loss: 0.036778  [ 1300/ 1536]\n",
      "loss: 0.015548  [ 1400/ 1536]\n",
      "loss: 0.050592  [ 1500/ 1536]\n",
      "loss: 0.067295  [    0/ 1536]\n",
      "loss: 0.033446  [  100/ 1536]\n",
      "loss: 0.028799  [  200/ 1536]\n",
      "loss: 0.378341  [  300/ 1536]\n",
      "loss: 0.022258  [  400/ 1536]\n",
      "loss: 0.067981  [  500/ 1536]\n",
      "loss: 0.028091  [  600/ 1536]\n",
      "loss: 0.009321  [  700/ 1536]\n",
      "loss: 0.272469  [  800/ 1536]\n",
      "loss: 0.153906  [  900/ 1536]\n",
      "loss: 0.317395  [ 1000/ 1536]\n",
      "loss: 0.018848  [ 1100/ 1536]\n",
      "loss: 0.012470  [ 1200/ 1536]\n",
      "loss: 0.006087  [ 1300/ 1536]\n",
      "loss: 0.007563  [ 1400/ 1536]\n",
      "loss: 0.004340  [ 1500/ 1536]\n",
      "loss: 0.002297  [    0/ 1536]\n",
      "loss: 0.007955  [  100/ 1536]\n",
      "loss: 0.118969  [  200/ 1536]\n",
      "loss: 0.022471  [  300/ 1536]\n",
      "loss: 0.025299  [  400/ 1536]\n",
      "loss: 0.004860  [  500/ 1536]\n",
      "loss: 0.029203  [  600/ 1536]\n",
      "loss: 0.000667  [  700/ 1536]\n",
      "loss: 0.006778  [  800/ 1536]\n",
      "loss: 0.013218  [  900/ 1536]\n",
      "loss: 0.030430  [ 1000/ 1536]\n",
      "loss: 0.001389  [ 1100/ 1536]\n",
      "loss: 0.118300  [ 1200/ 1536]\n",
      "loss: 0.036462  [ 1300/ 1536]\n",
      "loss: 0.068485  [ 1400/ 1536]\n",
      "loss: 0.081947  [ 1500/ 1536]\n",
      "loss: 0.000089  [    0/ 1536]\n",
      "loss: 0.169043  [  100/ 1536]\n",
      "loss: 0.025402  [  200/ 1536]\n",
      "loss: 0.000013  [  300/ 1536]\n",
      "loss: 0.049833  [  400/ 1536]\n",
      "loss: 0.072916  [  500/ 1536]\n",
      "loss: 0.154592  [  600/ 1536]\n",
      "loss: 0.008985  [  700/ 1536]\n",
      "loss: 0.134003  [  800/ 1536]\n",
      "loss: 0.054186  [  900/ 1536]\n",
      "loss: 0.027242  [ 1000/ 1536]\n",
      "loss: 0.086289  [ 1100/ 1536]\n",
      "loss: 0.043881  [ 1200/ 1536]\n",
      "loss: 0.036324  [ 1300/ 1536]\n",
      "loss: 0.362062  [ 1400/ 1536]\n",
      "loss: 0.045859  [ 1500/ 1536]\n",
      "loss: 0.002583  [    0/ 1536]\n",
      "loss: 0.027114  [  100/ 1536]\n",
      "loss: 0.039467  [  200/ 1536]\n",
      "loss: 0.078526  [  300/ 1536]\n",
      "loss: 0.119173  [  400/ 1536]\n",
      "loss: 0.679504  [  500/ 1536]\n",
      "loss: 0.025045  [  600/ 1536]\n",
      "loss: 0.051509  [  700/ 1536]\n",
      "loss: 0.000747  [  800/ 1536]\n",
      "loss: 0.024834  [  900/ 1536]\n",
      "loss: 0.096502  [ 1000/ 1536]\n",
      "loss: 0.018347  [ 1100/ 1536]\n",
      "loss: 0.015200  [ 1200/ 1536]\n",
      "loss: 0.060435  [ 1300/ 1536]\n",
      "loss: 0.086499  [ 1400/ 1536]\n",
      "loss: 0.010910  [ 1500/ 1536]\n",
      "loss: 0.000127  [    0/ 1536]\n",
      "loss: 0.106741  [  100/ 1536]\n",
      "loss: 0.090144  [  200/ 1536]\n",
      "loss: 0.007644  [  300/ 1536]\n",
      "loss: 0.162767  [  400/ 1536]\n",
      "loss: 0.050697  [  500/ 1536]\n",
      "loss: 0.000711  [  600/ 1536]\n",
      "loss: 0.000020  [  700/ 1536]\n",
      "loss: 0.139126  [  800/ 1536]\n",
      "loss: 0.169403  [  900/ 1536]\n",
      "loss: 0.015143  [ 1000/ 1536]\n",
      "loss: 0.001101  [ 1100/ 1536]\n",
      "loss: 0.004927  [ 1200/ 1536]\n",
      "loss: 0.086313  [ 1300/ 1536]\n",
      "loss: 0.051096  [ 1400/ 1536]\n",
      "loss: 0.009342  [ 1500/ 1536]\n",
      "loss: 0.004473  [    0/ 1536]\n",
      "loss: 0.084275  [  100/ 1536]\n",
      "loss: 0.000328  [  200/ 1536]\n",
      "loss: 0.023685  [  300/ 1536]\n",
      "loss: 0.015434  [  400/ 1536]\n",
      "loss: 0.023921  [  500/ 1536]\n",
      "loss: 0.024241  [  600/ 1536]\n",
      "loss: 0.018787  [  700/ 1536]\n",
      "loss: 0.014066  [  800/ 1536]\n",
      "loss: 0.093629  [  900/ 1536]\n",
      "loss: 0.015865  [ 1000/ 1536]\n",
      "loss: 0.028778  [ 1100/ 1536]\n",
      "loss: 0.007788  [ 1200/ 1536]\n",
      "loss: 0.028320  [ 1300/ 1536]\n",
      "loss: 0.006461  [ 1400/ 1536]\n",
      "loss: 0.056179  [ 1500/ 1536]\n",
      "loss: 0.000017  [    0/ 1536]\n",
      "loss: 0.015551  [  100/ 1536]\n",
      "loss: 0.136410  [  200/ 1536]\n",
      "loss: 0.011941  [  300/ 1536]\n",
      "loss: 0.000868  [  400/ 1536]\n",
      "loss: 0.160139  [  500/ 1536]\n",
      "loss: 0.004911  [  600/ 1536]\n",
      "loss: 0.000991  [  700/ 1536]\n",
      "loss: 0.240765  [  800/ 1536]\n",
      "loss: 0.110964  [  900/ 1536]\n",
      "loss: 0.109108  [ 1000/ 1536]\n",
      "loss: 0.018979  [ 1100/ 1536]\n",
      "loss: 0.001180  [ 1200/ 1536]\n",
      "loss: 0.020833  [ 1300/ 1536]\n",
      "loss: 0.011976  [ 1400/ 1536]\n",
      "loss: 0.125583  [ 1500/ 1536]\n",
      "loss: 0.128208  [    0/ 1536]\n",
      "loss: 0.001534  [  100/ 1536]\n",
      "loss: 0.051037  [  200/ 1536]\n",
      "loss: 0.042888  [  300/ 1536]\n",
      "loss: 0.011097  [  400/ 1536]\n",
      "loss: 0.019375  [  500/ 1536]\n",
      "loss: 0.029601  [  600/ 1536]\n",
      "loss: 0.000337  [  700/ 1536]\n",
      "loss: 0.129823  [  800/ 1536]\n",
      "loss: 0.318516  [  900/ 1536]\n",
      "loss: 0.019462  [ 1000/ 1536]\n",
      "loss: 0.001955  [ 1100/ 1536]\n",
      "loss: 0.121822  [ 1200/ 1536]\n",
      "loss: 0.033615  [ 1300/ 1536]\n",
      "loss: 0.003019  [ 1400/ 1536]\n",
      "loss: 0.040912  [ 1500/ 1536]\n",
      "loss: 0.001143  [    0/ 1536]\n",
      "loss: 0.090129  [  100/ 1536]\n",
      "loss: 0.000275  [  200/ 1536]\n",
      "loss: 0.000439  [  300/ 1536]\n",
      "loss: 0.130012  [  400/ 1536]\n",
      "loss: 0.000092  [  500/ 1536]\n",
      "loss: 0.000035  [  600/ 1536]\n",
      "loss: 0.037885  [  700/ 1536]\n",
      "loss: 0.250167  [  800/ 1536]\n",
      "loss: 0.058821  [  900/ 1536]\n",
      "loss: 0.035746  [ 1000/ 1536]\n",
      "loss: 0.024426  [ 1100/ 1536]\n",
      "loss: 0.008350  [ 1200/ 1536]\n",
      "loss: 0.184664  [ 1300/ 1536]\n",
      "loss: 0.001170  [ 1400/ 1536]\n",
      "loss: 0.001725  [ 1500/ 1536]\n",
      "loss: 0.106475  [    0/ 1536]\n",
      "loss: 0.015996  [  100/ 1536]\n",
      "loss: 0.033006  [  200/ 1536]\n",
      "loss: 0.019931  [  300/ 1536]\n",
      "loss: 0.013065  [  400/ 1536]\n",
      "loss: 0.002175  [  500/ 1536]\n",
      "loss: 0.001244  [  600/ 1536]\n",
      "loss: 0.186446  [  700/ 1536]\n",
      "loss: 0.039998  [  800/ 1536]\n",
      "loss: 0.041028  [  900/ 1536]\n",
      "loss: 0.118303  [ 1000/ 1536]\n",
      "loss: 0.003737  [ 1100/ 1536]\n",
      "loss: 0.002487  [ 1200/ 1536]\n",
      "loss: 0.044760  [ 1300/ 1536]\n",
      "loss: 0.334279  [ 1400/ 1536]\n",
      "loss: 0.002602  [ 1500/ 1536]\n",
      "loss: 0.071653  [    0/ 1536]\n",
      "loss: 0.000225  [  100/ 1536]\n",
      "loss: 0.052394  [  200/ 1536]\n",
      "loss: 0.022497  [  300/ 1536]\n",
      "loss: 0.045349  [  400/ 1536]\n",
      "loss: 0.000086  [  500/ 1536]\n",
      "loss: 0.142514  [  600/ 1536]\n",
      "loss: 0.029246  [  700/ 1536]\n",
      "loss: 0.072325  [  800/ 1536]\n",
      "loss: 0.020428  [  900/ 1536]\n",
      "loss: 0.033720  [ 1000/ 1536]\n",
      "loss: 0.006560  [ 1100/ 1536]\n",
      "loss: 0.138574  [ 1200/ 1536]\n",
      "loss: 0.040452  [ 1300/ 1536]\n",
      "loss: 0.031900  [ 1400/ 1536]\n",
      "loss: 0.049183  [ 1500/ 1536]\n",
      "loss: 0.002326  [    0/ 1536]\n",
      "loss: 0.000139  [  100/ 1536]\n",
      "loss: 0.390995  [  200/ 1536]\n",
      "loss: 0.011674  [  300/ 1536]\n",
      "loss: 0.013489  [  400/ 1536]\n",
      "loss: 0.001528  [  500/ 1536]\n",
      "loss: 0.082330  [  600/ 1536]\n",
      "loss: 0.096688  [  700/ 1536]\n",
      "loss: 0.024896  [  800/ 1536]\n",
      "loss: 0.017920  [  900/ 1536]\n",
      "loss: 0.011563  [ 1000/ 1536]\n",
      "loss: 0.000129  [ 1100/ 1536]\n",
      "loss: 0.003351  [ 1200/ 1536]\n",
      "loss: 0.099206  [ 1300/ 1536]\n",
      "loss: 0.017542  [ 1400/ 1536]\n",
      "loss: 0.002309  [ 1500/ 1536]\n",
      "loss: 0.175492  [    0/ 1536]\n",
      "loss: 0.127879  [  100/ 1536]\n",
      "loss: 0.114639  [  200/ 1536]\n",
      "loss: 0.016287  [  300/ 1536]\n",
      "loss: 0.005203  [  400/ 1536]\n",
      "loss: 0.008013  [  500/ 1536]\n",
      "loss: 0.014018  [  600/ 1536]\n",
      "loss: 0.012388  [  700/ 1536]\n",
      "loss: 0.151940  [  800/ 1536]\n",
      "loss: 0.119002  [  900/ 1536]\n",
      "loss: 0.006063  [ 1000/ 1536]\n",
      "loss: 0.000678  [ 1100/ 1536]\n",
      "loss: 0.028870  [ 1200/ 1536]\n",
      "loss: 0.002068  [ 1300/ 1536]\n",
      "loss: 0.000023  [ 1400/ 1536]\n",
      "loss: 0.110489  [ 1500/ 1536]\n",
      "loss: 0.000016  [    0/ 1536]\n",
      "loss: 0.003919  [  100/ 1536]\n",
      "loss: 0.214020  [  200/ 1536]\n",
      "loss: 0.083181  [  300/ 1536]\n",
      "loss: 0.001139  [  400/ 1536]\n",
      "loss: 0.147500  [  500/ 1536]\n",
      "loss: 0.004832  [  600/ 1536]\n",
      "loss: 0.054311  [  700/ 1536]\n",
      "loss: 0.028590  [  800/ 1536]\n",
      "loss: 0.061816  [  900/ 1536]\n",
      "loss: 0.057246  [ 1000/ 1536]\n",
      "loss: 0.120633  [ 1100/ 1536]\n",
      "loss: 0.059849  [ 1200/ 1536]\n",
      "loss: 0.032214  [ 1300/ 1536]\n",
      "loss: 0.108605  [ 1400/ 1536]\n",
      "loss: 0.128490  [ 1500/ 1536]\n",
      "loss: 0.004064  [    0/ 1536]\n",
      "loss: 0.012192  [  100/ 1536]\n",
      "loss: 0.004837  [  200/ 1536]\n",
      "loss: 0.003218  [  300/ 1536]\n",
      "loss: 0.000186  [  400/ 1536]\n",
      "loss: 0.000944  [  500/ 1536]\n",
      "loss: 0.117881  [  600/ 1536]\n",
      "loss: 0.185511  [  700/ 1536]\n",
      "loss: 0.142571  [  800/ 1536]\n",
      "loss: 0.049204  [  900/ 1536]\n",
      "loss: 0.172987  [ 1000/ 1536]\n",
      "loss: 0.031999  [ 1100/ 1536]\n",
      "loss: 0.220483  [ 1200/ 1536]\n",
      "loss: 0.034321  [ 1300/ 1536]\n",
      "loss: 0.005588  [ 1400/ 1536]\n",
      "loss: 0.000326  [ 1500/ 1536]\n",
      "loss: 0.018208  [    0/ 1536]\n",
      "loss: 0.000853  [  100/ 1536]\n",
      "loss: 0.006440  [  200/ 1536]\n",
      "loss: 0.036893  [  300/ 1536]\n",
      "loss: 0.019301  [  400/ 1536]\n",
      "loss: 0.004501  [  500/ 1536]\n",
      "loss: 0.061513  [  600/ 1536]\n",
      "loss: 0.006304  [  700/ 1536]\n",
      "loss: 0.007260  [  800/ 1536]\n",
      "loss: 0.001883  [  900/ 1536]\n",
      "loss: 0.002699  [ 1000/ 1536]\n",
      "loss: 0.000394  [ 1100/ 1536]\n",
      "loss: 0.100971  [ 1200/ 1536]\n",
      "loss: 0.009785  [ 1300/ 1536]\n",
      "loss: 0.002230  [ 1400/ 1536]\n",
      "loss: 0.013166  [ 1500/ 1536]\n",
      "loss: 0.102886  [    0/ 1536]\n",
      "loss: 0.037257  [  100/ 1536]\n",
      "loss: 0.003918  [  200/ 1536]\n",
      "loss: 0.011357  [  300/ 1536]\n",
      "loss: 0.166679  [  400/ 1536]\n",
      "loss: 0.001215  [  500/ 1536]\n",
      "loss: 0.004853  [  600/ 1536]\n",
      "loss: 0.001208  [  700/ 1536]\n",
      "loss: 0.000701  [  800/ 1536]\n",
      "loss: 0.142483  [  900/ 1536]\n",
      "loss: 0.285665  [ 1000/ 1536]\n",
      "loss: 0.030295  [ 1100/ 1536]\n",
      "loss: 0.025692  [ 1200/ 1536]\n",
      "loss: 0.176035  [ 1300/ 1536]\n",
      "loss: 0.000000  [ 1400/ 1536]\n",
      "loss: 0.080352  [ 1500/ 1536]\n",
      "loss: 0.000032  [    0/ 1536]\n",
      "loss: 0.049008  [  100/ 1536]\n",
      "loss: 0.004166  [  200/ 1536]\n",
      "loss: 0.192553  [  300/ 1536]\n",
      "loss: 0.091844  [  400/ 1536]\n",
      "loss: 0.000549  [  500/ 1536]\n",
      "loss: 0.009290  [  600/ 1536]\n",
      "loss: 0.013751  [  700/ 1536]\n",
      "loss: 0.034951  [  800/ 1536]\n",
      "loss: 0.008222  [  900/ 1536]\n",
      "loss: 0.026106  [ 1000/ 1536]\n",
      "loss: 0.002534  [ 1100/ 1536]\n",
      "loss: 0.133078  [ 1200/ 1536]\n",
      "loss: 0.020602  [ 1300/ 1536]\n",
      "loss: 0.081496  [ 1400/ 1536]\n",
      "loss: 0.000263  [ 1500/ 1536]\n",
      "loss: 0.003719  [    0/ 1536]\n",
      "loss: 0.049669  [  100/ 1536]\n",
      "loss: 0.178938  [  200/ 1536]\n",
      "loss: 0.000136  [  300/ 1536]\n",
      "loss: 0.020577  [  400/ 1536]\n",
      "loss: 0.042958  [  500/ 1536]\n",
      "loss: 0.001824  [  600/ 1536]\n",
      "loss: 0.051689  [  700/ 1536]\n",
      "loss: 0.008749  [  800/ 1536]\n",
      "loss: 0.095604  [  900/ 1536]\n",
      "loss: 0.029178  [ 1000/ 1536]\n",
      "loss: 0.025668  [ 1100/ 1536]\n",
      "loss: 0.005787  [ 1200/ 1536]\n",
      "loss: 0.022177  [ 1300/ 1536]\n",
      "loss: 0.050935  [ 1400/ 1536]\n",
      "loss: 0.006119  [ 1500/ 1536]\n",
      "loss: 0.000366  [    0/ 1536]\n",
      "loss: 0.035028  [  100/ 1536]\n",
      "loss: 0.050414  [  200/ 1536]\n",
      "loss: 0.002108  [  300/ 1536]\n",
      "loss: 0.000635  [  400/ 1536]\n",
      "loss: 0.002882  [  500/ 1536]\n",
      "loss: 0.020495  [  600/ 1536]\n",
      "loss: 0.001986  [  700/ 1536]\n",
      "loss: 0.143163  [  800/ 1536]\n",
      "loss: 0.000847  [  900/ 1536]\n",
      "loss: 0.107823  [ 1000/ 1536]\n",
      "loss: 0.089402  [ 1100/ 1536]\n",
      "loss: 0.027913  [ 1200/ 1536]\n",
      "loss: 0.007238  [ 1300/ 1536]\n",
      "loss: 0.021824  [ 1400/ 1536]\n",
      "loss: 0.017565  [ 1500/ 1536]\n",
      "loss: 0.013000  [    0/ 1536]\n",
      "loss: 0.019424  [  100/ 1536]\n",
      "loss: 0.005603  [  200/ 1536]\n",
      "loss: 0.025086  [  300/ 1536]\n",
      "loss: 0.078799  [  400/ 1536]\n",
      "loss: 0.002858  [  500/ 1536]\n",
      "loss: 0.031008  [  600/ 1536]\n",
      "loss: 0.174705  [  700/ 1536]\n",
      "loss: 0.003012  [  800/ 1536]\n",
      "loss: 0.051960  [  900/ 1536]\n",
      "loss: 0.014557  [ 1000/ 1536]\n",
      "loss: 0.027730  [ 1100/ 1536]\n",
      "loss: 0.000235  [ 1200/ 1536]\n",
      "loss: 0.000416  [ 1300/ 1536]\n",
      "loss: 0.017062  [ 1400/ 1536]\n",
      "loss: 0.023000  [ 1500/ 1536]\n",
      "loss: 0.048822  [    0/ 1536]\n",
      "loss: 0.005163  [  100/ 1536]\n",
      "loss: 0.020006  [  200/ 1536]\n",
      "loss: 0.033364  [  300/ 1536]\n",
      "loss: 0.000843  [  400/ 1536]\n",
      "loss: 0.009446  [  500/ 1536]\n",
      "loss: 0.194431  [  600/ 1536]\n",
      "loss: 0.051049  [  700/ 1536]\n",
      "loss: 0.007126  [  800/ 1536]\n",
      "loss: 0.265808  [  900/ 1536]\n",
      "loss: 0.131204  [ 1000/ 1536]\n",
      "loss: 0.093534  [ 1100/ 1536]\n",
      "loss: 0.216023  [ 1200/ 1536]\n",
      "loss: 0.000450  [ 1300/ 1536]\n",
      "loss: 0.031038  [ 1400/ 1536]\n",
      "loss: 0.013777  [ 1500/ 1536]\n",
      "loss: 0.251717  [    0/ 1536]\n",
      "loss: 0.027863  [  100/ 1536]\n",
      "loss: 0.108038  [  200/ 1536]\n",
      "loss: 0.000716  [  300/ 1536]\n",
      "loss: 0.002883  [  400/ 1536]\n",
      "loss: 0.000073  [  500/ 1536]\n",
      "loss: 0.059152  [  600/ 1536]\n",
      "loss: 0.000011  [  700/ 1536]\n",
      "loss: 0.017261  [  800/ 1536]\n",
      "loss: 0.000015  [  900/ 1536]\n",
      "loss: 0.004040  [ 1000/ 1536]\n",
      "loss: 0.017909  [ 1100/ 1536]\n",
      "loss: 0.062069  [ 1200/ 1536]\n",
      "loss: 0.027504  [ 1300/ 1536]\n",
      "loss: 0.030840  [ 1400/ 1536]\n",
      "loss: 0.006627  [ 1500/ 1536]\n",
      "loss: 0.055490  [    0/ 1536]\n",
      "loss: 0.077604  [  100/ 1536]\n",
      "loss: 0.001671  [  200/ 1536]\n",
      "loss: 0.001094  [  300/ 1536]\n",
      "loss: 0.001016  [  400/ 1536]\n",
      "loss: 0.001968  [  500/ 1536]\n",
      "loss: 0.195101  [  600/ 1536]\n",
      "loss: 0.169198  [  700/ 1536]\n",
      "loss: 0.017437  [  800/ 1536]\n",
      "loss: 0.017718  [  900/ 1536]\n",
      "loss: 0.040739  [ 1000/ 1536]\n",
      "loss: 0.005697  [ 1100/ 1536]\n",
      "loss: 0.001704  [ 1200/ 1536]\n",
      "loss: 0.168397  [ 1300/ 1536]\n",
      "loss: 0.013217  [ 1400/ 1536]\n",
      "loss: 0.075907  [ 1500/ 1536]\n",
      "loss: 0.000031  [    0/ 1536]\n",
      "loss: 0.019056  [  100/ 1536]\n",
      "loss: 0.060078  [  200/ 1536]\n",
      "loss: 0.014648  [  300/ 1536]\n",
      "loss: 0.004888  [  400/ 1536]\n",
      "loss: 0.001028  [  500/ 1536]\n",
      "loss: 0.122056  [  600/ 1536]\n",
      "loss: 0.036162  [  700/ 1536]\n",
      "loss: 0.053272  [  800/ 1536]\n",
      "loss: 0.021516  [  900/ 1536]\n",
      "loss: 0.021260  [ 1000/ 1536]\n",
      "loss: 0.004613  [ 1100/ 1536]\n",
      "loss: 0.059209  [ 1200/ 1536]\n",
      "loss: 0.004698  [ 1300/ 1536]\n",
      "loss: 0.053422  [ 1400/ 1536]\n",
      "loss: 0.006691  [ 1500/ 1536]\n",
      "loss: 0.007494  [    0/ 1536]\n",
      "loss: 0.040635  [  100/ 1536]\n",
      "loss: 0.042305  [  200/ 1536]\n",
      "loss: 0.116541  [  300/ 1536]\n",
      "loss: 0.015061  [  400/ 1536]\n",
      "loss: 0.003320  [  500/ 1536]\n",
      "loss: 0.022358  [  600/ 1536]\n",
      "loss: 0.055789  [  700/ 1536]\n",
      "loss: 0.052997  [  800/ 1536]\n",
      "loss: 0.008966  [  900/ 1536]\n",
      "loss: 0.105922  [ 1000/ 1536]\n",
      "loss: 0.000000  [ 1100/ 1536]\n",
      "loss: 0.002409  [ 1200/ 1536]\n",
      "loss: 0.018823  [ 1300/ 1536]\n",
      "loss: 0.017379  [ 1400/ 1536]\n",
      "loss: 0.012882  [ 1500/ 1536]\n",
      "loss: 0.225453  [    0/ 1536]\n",
      "loss: 0.056247  [  100/ 1536]\n",
      "loss: 0.009571  [  200/ 1536]\n",
      "loss: 0.011268  [  300/ 1536]\n",
      "loss: 0.011491  [  400/ 1536]\n",
      "loss: 0.000036  [  500/ 1536]\n",
      "loss: 0.081906  [  600/ 1536]\n",
      "loss: 0.022196  [  700/ 1536]\n",
      "loss: 0.005199  [  800/ 1536]\n",
      "loss: 0.115994  [  900/ 1536]\n",
      "loss: 0.007909  [ 1000/ 1536]\n",
      "loss: 0.002642  [ 1100/ 1536]\n",
      "loss: 0.001913  [ 1200/ 1536]\n",
      "loss: 0.000602  [ 1300/ 1536]\n",
      "loss: 0.082039  [ 1400/ 1536]\n",
      "loss: 0.122444  [ 1500/ 1536]\n",
      "loss: 0.075305  [    0/ 1536]\n",
      "loss: 0.078671  [  100/ 1536]\n",
      "loss: 0.048488  [  200/ 1536]\n",
      "loss: 0.061816  [  300/ 1536]\n",
      "loss: 0.037125  [  400/ 1536]\n",
      "loss: 0.004862  [  500/ 1536]\n",
      "loss: 0.001462  [  600/ 1536]\n",
      "loss: 0.060049  [  700/ 1536]\n",
      "loss: 0.000738  [  800/ 1536]\n",
      "loss: 0.000454  [  900/ 1536]\n",
      "loss: 0.002117  [ 1000/ 1536]\n",
      "loss: 0.002766  [ 1100/ 1536]\n",
      "loss: 0.022268  [ 1200/ 1536]\n",
      "loss: 0.058125  [ 1300/ 1536]\n",
      "loss: 0.008263  [ 1400/ 1536]\n",
      "loss: 0.014101  [ 1500/ 1536]\n",
      "loss: 0.102508  [    0/ 1536]\n",
      "loss: 0.035686  [  100/ 1536]\n",
      "loss: 0.028999  [  200/ 1536]\n",
      "loss: 0.121409  [  300/ 1536]\n",
      "loss: 0.009048  [  400/ 1536]\n",
      "loss: 0.116857  [  500/ 1536]\n",
      "loss: 0.004937  [  600/ 1536]\n",
      "loss: 0.009708  [  700/ 1536]\n",
      "loss: 0.112678  [  800/ 1536]\n",
      "loss: 0.005332  [  900/ 1536]\n",
      "loss: 0.017919  [ 1000/ 1536]\n",
      "loss: 0.017436  [ 1100/ 1536]\n",
      "loss: 0.012426  [ 1200/ 1536]\n",
      "loss: 0.000319  [ 1300/ 1536]\n",
      "loss: 0.000666  [ 1400/ 1536]\n",
      "loss: 0.078629  [ 1500/ 1536]\n",
      "loss: 0.012611  [    0/ 1536]\n",
      "loss: 0.009244  [  100/ 1536]\n",
      "loss: 0.016304  [  200/ 1536]\n",
      "loss: 0.005761  [  300/ 1536]\n",
      "loss: 0.062970  [  400/ 1536]\n",
      "loss: 0.000708  [  500/ 1536]\n",
      "loss: 0.007659  [  600/ 1536]\n",
      "loss: 0.032644  [  700/ 1536]\n",
      "loss: 0.361904  [  800/ 1536]\n",
      "loss: 0.016096  [  900/ 1536]\n",
      "loss: 0.030856  [ 1000/ 1536]\n",
      "loss: 0.021834  [ 1100/ 1536]\n",
      "loss: 0.028842  [ 1200/ 1536]\n",
      "loss: 0.017116  [ 1300/ 1536]\n",
      "loss: 0.038994  [ 1400/ 1536]\n",
      "loss: 0.006891  [ 1500/ 1536]\n",
      "loss: 0.001852  [    0/ 1536]\n",
      "loss: 0.046249  [  100/ 1536]\n",
      "loss: 0.167328  [  200/ 1536]\n",
      "loss: 0.000762  [  300/ 1536]\n",
      "loss: 0.343206  [  400/ 1536]\n",
      "loss: 0.001252  [  500/ 1536]\n",
      "loss: 0.004606  [  600/ 1536]\n",
      "loss: 0.085344  [  700/ 1536]\n",
      "loss: 0.000968  [  800/ 1536]\n",
      "loss: 0.003520  [  900/ 1536]\n",
      "loss: 0.075686  [ 1000/ 1536]\n",
      "loss: 0.000514  [ 1100/ 1536]\n",
      "loss: 0.019323  [ 1200/ 1536]\n",
      "loss: 0.001643  [ 1300/ 1536]\n",
      "loss: 0.020600  [ 1400/ 1536]\n",
      "loss: 0.001110  [ 1500/ 1536]\n",
      "loss: 0.102408  [    0/ 1536]\n",
      "loss: 0.023841  [  100/ 1536]\n",
      "loss: 0.001020  [  200/ 1536]\n",
      "loss: 0.028433  [  300/ 1536]\n",
      "loss: 0.049383  [  400/ 1536]\n",
      "loss: 0.037503  [  500/ 1536]\n",
      "loss: 0.042438  [  600/ 1536]\n",
      "loss: 0.017232  [  700/ 1536]\n",
      "loss: 0.023556  [  800/ 1536]\n",
      "loss: 0.068806  [  900/ 1536]\n",
      "loss: 0.022202  [ 1000/ 1536]\n",
      "loss: 0.000814  [ 1100/ 1536]\n",
      "loss: 0.011712  [ 1200/ 1536]\n",
      "loss: 0.027501  [ 1300/ 1536]\n",
      "loss: 0.001439  [ 1400/ 1536]\n",
      "loss: 0.000326  [ 1500/ 1536]\n",
      "loss: 0.008351  [    0/ 1536]\n",
      "loss: 0.057977  [  100/ 1536]\n",
      "loss: 0.060361  [  200/ 1536]\n",
      "loss: 0.045398  [  300/ 1536]\n",
      "loss: 0.014285  [  400/ 1536]\n",
      "loss: 0.024956  [  500/ 1536]\n",
      "loss: 0.173149  [  600/ 1536]\n",
      "loss: 0.004672  [  700/ 1536]\n",
      "loss: 0.170807  [  800/ 1536]\n",
      "loss: 0.003397  [  900/ 1536]\n",
      "loss: 0.005801  [ 1000/ 1536]\n",
      "loss: 0.006942  [ 1100/ 1536]\n",
      "loss: 0.013323  [ 1200/ 1536]\n",
      "loss: 0.000794  [ 1300/ 1536]\n",
      "loss: 0.000717  [ 1400/ 1536]\n",
      "loss: 0.150160  [ 1500/ 1536]\n",
      "loss: 0.015302  [    0/ 1536]\n",
      "loss: 0.002112  [  100/ 1536]\n",
      "loss: 0.003451  [  200/ 1536]\n",
      "loss: 0.018399  [  300/ 1536]\n",
      "loss: 0.029298  [  400/ 1536]\n",
      "loss: 0.006749  [  500/ 1536]\n",
      "loss: 0.052783  [  600/ 1536]\n",
      "loss: 0.005522  [  700/ 1536]\n",
      "loss: 0.107606  [  800/ 1536]\n",
      "loss: 0.000353  [  900/ 1536]\n",
      "loss: 0.008232  [ 1000/ 1536]\n",
      "loss: 0.004835  [ 1100/ 1536]\n",
      "loss: 0.006906  [ 1200/ 1536]\n",
      "loss: 0.000829  [ 1300/ 1536]\n",
      "loss: 0.051509  [ 1400/ 1536]\n",
      "loss: 0.100759  [ 1500/ 1536]\n",
      "loss: 0.002583  [    0/ 1536]\n",
      "loss: 0.023705  [  100/ 1536]\n",
      "loss: 0.036161  [  200/ 1536]\n",
      "loss: 0.000733  [  300/ 1536]\n",
      "loss: 0.137128  [  400/ 1536]\n",
      "loss: 0.000359  [  500/ 1536]\n",
      "loss: 0.000103  [  600/ 1536]\n",
      "loss: 0.152893  [  700/ 1536]\n",
      "loss: 0.003018  [  800/ 1536]\n",
      "loss: 0.094627  [  900/ 1536]\n",
      "loss: 0.105201  [ 1000/ 1536]\n",
      "loss: 0.001350  [ 1100/ 1536]\n",
      "loss: 0.024904  [ 1200/ 1536]\n",
      "loss: 0.015968  [ 1300/ 1536]\n",
      "loss: 0.063022  [ 1400/ 1536]\n",
      "loss: 0.001425  [ 1500/ 1536]\n",
      "loss: 0.040893  [    0/ 1536]\n",
      "loss: 0.002240  [  100/ 1536]\n",
      "loss: 0.005555  [  200/ 1536]\n",
      "loss: 0.025519  [  300/ 1536]\n",
      "loss: 0.053361  [  400/ 1536]\n",
      "loss: 0.037438  [  500/ 1536]\n",
      "loss: 0.008995  [  600/ 1536]\n",
      "loss: 0.092482  [  700/ 1536]\n",
      "loss: 0.000276  [  800/ 1536]\n",
      "loss: 0.007975  [  900/ 1536]\n",
      "loss: 0.011889  [ 1000/ 1536]\n",
      "loss: 0.003230  [ 1100/ 1536]\n",
      "loss: 0.000322  [ 1200/ 1536]\n",
      "loss: 0.224636  [ 1300/ 1536]\n",
      "loss: 0.110495  [ 1400/ 1536]\n",
      "loss: 0.072195  [ 1500/ 1536]\n",
      "loss: 0.327540  [    0/ 1536]\n",
      "loss: 0.000815  [  100/ 1536]\n",
      "loss: 0.078810  [  200/ 1536]\n",
      "loss: 0.011322  [  300/ 1536]\n",
      "loss: 0.069375  [  400/ 1536]\n",
      "loss: 0.002372  [  500/ 1536]\n",
      "loss: 0.040186  [  600/ 1536]\n",
      "loss: 0.002325  [  700/ 1536]\n",
      "loss: 0.086301  [  800/ 1536]\n",
      "loss: 0.003205  [  900/ 1536]\n",
      "loss: 0.047538  [ 1000/ 1536]\n",
      "loss: 0.001185  [ 1100/ 1536]\n",
      "loss: 0.006462  [ 1200/ 1536]\n",
      "loss: 0.002214  [ 1300/ 1536]\n",
      "loss: 0.001084  [ 1400/ 1536]\n",
      "loss: 0.059268  [ 1500/ 1536]\n",
      "loss: 0.011951  [    0/ 1536]\n",
      "loss: 0.013648  [  100/ 1536]\n",
      "loss: 0.000011  [  200/ 1536]\n",
      "loss: 0.059957  [  300/ 1536]\n",
      "loss: 0.007968  [  400/ 1536]\n",
      "loss: 0.000143  [  500/ 1536]\n",
      "loss: 0.003906  [  600/ 1536]\n",
      "loss: 0.095325  [  700/ 1536]\n",
      "loss: 0.000075  [  800/ 1536]\n",
      "loss: 0.027941  [  900/ 1536]\n",
      "loss: 0.096509  [ 1000/ 1536]\n",
      "loss: 0.007156  [ 1100/ 1536]\n",
      "loss: 0.005380  [ 1200/ 1536]\n",
      "loss: 0.156585  [ 1300/ 1536]\n",
      "loss: 0.063861  [ 1400/ 1536]\n",
      "loss: 0.002398  [ 1500/ 1536]\n",
      "loss: 0.059232  [    0/ 1536]\n",
      "loss: 0.007720  [  100/ 1536]\n",
      "loss: 0.019376  [  200/ 1536]\n",
      "loss: 0.000994  [  300/ 1536]\n",
      "loss: 0.056510  [  400/ 1536]\n",
      "loss: 0.000999  [  500/ 1536]\n",
      "loss: 0.071112  [  600/ 1536]\n",
      "loss: 0.007603  [  700/ 1536]\n",
      "loss: 0.007342  [  800/ 1536]\n",
      "loss: 0.011365  [  900/ 1536]\n",
      "loss: 0.123919  [ 1000/ 1536]\n",
      "loss: 0.011429  [ 1100/ 1536]\n",
      "loss: 0.017817  [ 1200/ 1536]\n",
      "loss: 0.000452  [ 1300/ 1536]\n",
      "loss: 0.011018  [ 1400/ 1536]\n",
      "loss: 0.016271  [ 1500/ 1536]\n",
      "loss: 0.177105  [    0/ 1536]\n",
      "loss: 0.004738  [  100/ 1536]\n",
      "loss: 0.107694  [  200/ 1536]\n",
      "loss: 0.194188  [  300/ 1536]\n",
      "loss: 0.007057  [  400/ 1536]\n",
      "loss: 0.058984  [  500/ 1536]\n",
      "loss: 0.001901  [  600/ 1536]\n",
      "loss: 0.016452  [  700/ 1536]\n",
      "loss: 0.140784  [  800/ 1536]\n",
      "loss: 0.037775  [  900/ 1536]\n",
      "loss: 0.020623  [ 1000/ 1536]\n",
      "loss: 0.001093  [ 1100/ 1536]\n",
      "loss: 0.014494  [ 1200/ 1536]\n",
      "loss: 0.036361  [ 1300/ 1536]\n",
      "loss: 0.013845  [ 1400/ 1536]\n",
      "loss: 0.000002  [ 1500/ 1536]\n",
      "loss: 0.045237  [    0/ 1536]\n",
      "loss: 0.042828  [  100/ 1536]\n",
      "loss: 0.000305  [  200/ 1536]\n",
      "loss: 0.039151  [  300/ 1536]\n",
      "loss: 0.012338  [  400/ 1536]\n",
      "loss: 0.074461  [  500/ 1536]\n",
      "loss: 0.000561  [  600/ 1536]\n",
      "loss: 0.002086  [  700/ 1536]\n",
      "loss: 0.000696  [  800/ 1536]\n",
      "loss: 0.023176  [  900/ 1536]\n",
      "loss: 0.020190  [ 1000/ 1536]\n",
      "loss: 0.001281  [ 1100/ 1536]\n",
      "loss: 0.026670  [ 1200/ 1536]\n",
      "loss: 0.026479  [ 1300/ 1536]\n",
      "loss: 0.088679  [ 1400/ 1536]\n",
      "loss: 0.001432  [ 1500/ 1536]\n",
      "loss: 0.377589  [    0/ 1536]\n",
      "loss: 0.074710  [  100/ 1536]\n",
      "loss: 0.004538  [  200/ 1536]\n",
      "loss: 0.019982  [  300/ 1536]\n",
      "loss: 0.074309  [  400/ 1536]\n",
      "loss: 0.042828  [  500/ 1536]\n",
      "loss: 0.117402  [  600/ 1536]\n",
      "loss: 0.079648  [  700/ 1536]\n",
      "loss: 0.051785  [  800/ 1536]\n",
      "loss: 0.068332  [  900/ 1536]\n",
      "loss: 0.000216  [ 1000/ 1536]\n",
      "loss: 0.002767  [ 1100/ 1536]\n",
      "loss: 0.000010  [ 1200/ 1536]\n",
      "loss: 0.000031  [ 1300/ 1536]\n",
      "loss: 0.001435  [ 1400/ 1536]\n",
      "loss: 0.020080  [ 1500/ 1536]\n",
      "loss: 0.006753  [    0/ 1536]\n",
      "loss: 0.045547  [  100/ 1536]\n",
      "loss: 0.007716  [  200/ 1536]\n",
      "loss: 0.068800  [  300/ 1536]\n",
      "loss: 0.058054  [  400/ 1536]\n",
      "loss: 0.000005  [  500/ 1536]\n",
      "loss: 0.029884  [  600/ 1536]\n",
      "loss: 0.027809  [  700/ 1536]\n",
      "loss: 0.002438  [  800/ 1536]\n",
      "loss: 0.082600  [  900/ 1536]\n",
      "loss: 0.015742  [ 1000/ 1536]\n",
      "loss: 0.016456  [ 1100/ 1536]\n",
      "loss: 0.008767  [ 1200/ 1536]\n",
      "loss: 0.136087  [ 1300/ 1536]\n",
      "loss: 0.057166  [ 1400/ 1536]\n",
      "loss: 0.036987  [ 1500/ 1536]\n",
      "loss: 0.005199  [    0/ 1536]\n",
      "loss: 0.128205  [  100/ 1536]\n",
      "loss: 0.046650  [  200/ 1536]\n",
      "loss: 0.011669  [  300/ 1536]\n",
      "loss: 0.045389  [  400/ 1536]\n",
      "loss: 0.104170  [  500/ 1536]\n",
      "loss: 0.020517  [  600/ 1536]\n",
      "loss: 0.000045  [  700/ 1536]\n",
      "loss: 0.016521  [  800/ 1536]\n",
      "loss: 0.050375  [  900/ 1536]\n",
      "loss: 0.003512  [ 1000/ 1536]\n",
      "loss: 0.006845  [ 1100/ 1536]\n",
      "loss: 0.010495  [ 1200/ 1536]\n",
      "loss: 0.001475  [ 1300/ 1536]\n",
      "loss: 0.176429  [ 1400/ 1536]\n",
      "loss: 0.002376  [ 1500/ 1536]\n",
      "loss: 0.000808  [    0/ 1536]\n",
      "loss: 0.007527  [  100/ 1536]\n",
      "loss: 0.011960  [  200/ 1536]\n",
      "loss: 0.112404  [  300/ 1536]\n",
      "loss: 0.012625  [  400/ 1536]\n",
      "loss: 0.004733  [  500/ 1536]\n",
      "loss: 0.063843  [  600/ 1536]\n",
      "loss: 0.003398  [  700/ 1536]\n",
      "loss: 0.020422  [  800/ 1536]\n",
      "loss: 0.011642  [  900/ 1536]\n",
      "loss: 0.000038  [ 1000/ 1536]\n",
      "loss: 0.033407  [ 1100/ 1536]\n",
      "loss: 0.060224  [ 1200/ 1536]\n",
      "loss: 0.102700  [ 1300/ 1536]\n",
      "loss: 0.007646  [ 1400/ 1536]\n",
      "loss: 0.032899  [ 1500/ 1536]\n",
      "loss: 0.024939  [    0/ 1536]\n",
      "loss: 0.008644  [  100/ 1536]\n",
      "loss: 0.015381  [  200/ 1536]\n",
      "loss: 0.000056  [  300/ 1536]\n",
      "loss: 0.009746  [  400/ 1536]\n",
      "loss: 0.055224  [  500/ 1536]\n",
      "loss: 0.014908  [  600/ 1536]\n",
      "loss: 0.050286  [  700/ 1536]\n",
      "loss: 0.010535  [  800/ 1536]\n",
      "loss: 0.001041  [  900/ 1536]\n",
      "loss: 0.012146  [ 1000/ 1536]\n",
      "loss: 0.033734  [ 1100/ 1536]\n",
      "loss: 0.002488  [ 1200/ 1536]\n",
      "loss: 0.010469  [ 1300/ 1536]\n",
      "loss: 0.011567  [ 1400/ 1536]\n",
      "loss: 0.019016  [ 1500/ 1536]\n",
      "loss: 0.037285  [    0/ 1536]\n",
      "loss: 0.000575  [  100/ 1536]\n",
      "loss: 0.000490  [  200/ 1536]\n",
      "loss: 0.039515  [  300/ 1536]\n",
      "loss: 0.002469  [  400/ 1536]\n",
      "loss: 0.025676  [  500/ 1536]\n",
      "loss: 0.016050  [  600/ 1536]\n",
      "loss: 0.003932  [  700/ 1536]\n",
      "loss: 0.000042  [  800/ 1536]\n",
      "loss: 0.065773  [  900/ 1536]\n",
      "loss: 0.003989  [ 1000/ 1536]\n",
      "loss: 0.039844  [ 1100/ 1536]\n",
      "loss: 0.021110  [ 1200/ 1536]\n",
      "loss: 0.002621  [ 1300/ 1536]\n",
      "loss: 0.225798  [ 1400/ 1536]\n",
      "loss: 0.004521  [ 1500/ 1536]\n",
      "loss: 0.000000  [    0/ 1536]\n",
      "loss: 0.066855  [  100/ 1536]\n",
      "loss: 0.043413  [  200/ 1536]\n",
      "loss: 0.005916  [  300/ 1536]\n",
      "loss: 0.080836  [  400/ 1536]\n",
      "loss: 0.073498  [  500/ 1536]\n",
      "loss: 0.000095  [  600/ 1536]\n",
      "loss: 0.018840  [  700/ 1536]\n",
      "loss: 0.105396  [  800/ 1536]\n",
      "loss: 0.015154  [  900/ 1536]\n",
      "loss: 0.105713  [ 1000/ 1536]\n",
      "loss: 0.015517  [ 1100/ 1536]\n",
      "loss: 0.053287  [ 1200/ 1536]\n",
      "loss: 0.009632  [ 1300/ 1536]\n",
      "loss: 0.005104  [ 1400/ 1536]\n",
      "loss: 0.006813  [ 1500/ 1536]\n",
      "loss: 0.000500  [    0/ 1536]\n",
      "loss: 0.015582  [  100/ 1536]\n",
      "loss: 0.002264  [  200/ 1536]\n",
      "loss: 0.014946  [  300/ 1536]\n",
      "loss: 0.001748  [  400/ 1536]\n",
      "loss: 0.052121  [  500/ 1536]\n",
      "loss: 0.039931  [  600/ 1536]\n",
      "loss: 0.084472  [  700/ 1536]\n",
      "loss: 0.103411  [  800/ 1536]\n",
      "loss: 0.055185  [  900/ 1536]\n",
      "loss: 0.001205  [ 1000/ 1536]\n",
      "loss: 0.043746  [ 1100/ 1536]\n",
      "loss: 0.024191  [ 1200/ 1536]\n",
      "loss: 0.000747  [ 1300/ 1536]\n",
      "loss: 0.004060  [ 1400/ 1536]\n",
      "loss: 0.007861  [ 1500/ 1536]\n",
      "loss: 0.000413  [    0/ 1536]\n",
      "loss: 0.110085  [  100/ 1536]\n",
      "loss: 0.015312  [  200/ 1536]\n",
      "loss: 0.004188  [  300/ 1536]\n",
      "loss: 0.001546  [  400/ 1536]\n",
      "loss: 0.003442  [  500/ 1536]\n",
      "loss: 0.018855  [  600/ 1536]\n",
      "loss: 0.004773  [  700/ 1536]\n",
      "loss: 0.020698  [  800/ 1536]\n",
      "loss: 0.063382  [  900/ 1536]\n",
      "loss: 0.000771  [ 1000/ 1536]\n",
      "loss: 0.006299  [ 1100/ 1536]\n",
      "loss: 0.000816  [ 1200/ 1536]\n",
      "loss: 0.031322  [ 1300/ 1536]\n",
      "loss: 0.000769  [ 1400/ 1536]\n",
      "loss: 0.027105  [ 1500/ 1536]\n",
      "loss: 0.017956  [    0/ 1536]\n",
      "loss: 0.084439  [  100/ 1536]\n",
      "loss: 0.151687  [  200/ 1536]\n",
      "loss: 0.025180  [  300/ 1536]\n",
      "loss: 0.026444  [  400/ 1536]\n",
      "loss: 0.002965  [  500/ 1536]\n",
      "loss: 0.088514  [  600/ 1536]\n",
      "loss: 0.000762  [  700/ 1536]\n",
      "loss: 0.021733  [  800/ 1536]\n",
      "loss: 0.009258  [  900/ 1536]\n",
      "loss: 0.000002  [ 1000/ 1536]\n",
      "loss: 0.035589  [ 1100/ 1536]\n",
      "loss: 0.001056  [ 1200/ 1536]\n",
      "loss: 0.106769  [ 1300/ 1536]\n",
      "loss: 0.028857  [ 1400/ 1536]\n",
      "loss: 0.072071  [ 1500/ 1536]\n",
      "loss: 0.013220  [    0/ 1536]\n",
      "loss: 0.001072  [  100/ 1536]\n",
      "loss: 0.077643  [  200/ 1536]\n",
      "loss: 0.028212  [  300/ 1536]\n",
      "loss: 0.001102  [  400/ 1536]\n",
      "loss: 0.000308  [  500/ 1536]\n",
      "loss: 0.001810  [  600/ 1536]\n",
      "loss: 0.269397  [  700/ 1536]\n",
      "loss: 0.002304  [  800/ 1536]\n",
      "loss: 0.003924  [  900/ 1536]\n",
      "loss: 0.006799  [ 1000/ 1536]\n",
      "loss: 0.178845  [ 1100/ 1536]\n",
      "loss: 0.243458  [ 1200/ 1536]\n",
      "loss: 0.132367  [ 1300/ 1536]\n",
      "loss: 0.292506  [ 1400/ 1536]\n",
      "loss: 0.119065  [ 1500/ 1536]\n",
      "loss: 0.076857  [    0/ 1536]\n",
      "loss: 0.108408  [  100/ 1536]\n",
      "loss: 0.002583  [  200/ 1536]\n",
      "loss: 0.085887  [  300/ 1536]\n",
      "loss: 0.036481  [  400/ 1536]\n",
      "loss: 0.061840  [  500/ 1536]\n",
      "loss: 0.025643  [  600/ 1536]\n",
      "loss: 0.001164  [  700/ 1536]\n",
      "loss: 0.028416  [  800/ 1536]\n",
      "loss: 0.009384  [  900/ 1536]\n",
      "loss: 0.016647  [ 1000/ 1536]\n",
      "loss: 0.036343  [ 1100/ 1536]\n",
      "loss: 0.015732  [ 1200/ 1536]\n",
      "loss: 0.207184  [ 1300/ 1536]\n",
      "loss: 0.005772  [ 1400/ 1536]\n",
      "loss: 0.034456  [ 1500/ 1536]\n",
      "loss: 0.000122  [    0/ 1536]\n",
      "loss: 0.050135  [  100/ 1536]\n",
      "loss: 0.060428  [  200/ 1536]\n",
      "loss: 0.041712  [  300/ 1536]\n",
      "loss: 0.033188  [  400/ 1536]\n",
      "loss: 0.003660  [  500/ 1536]\n",
      "loss: 0.015238  [  600/ 1536]\n",
      "loss: 0.001706  [  700/ 1536]\n",
      "loss: 0.016467  [  800/ 1536]\n",
      "loss: 0.002067  [  900/ 1536]\n",
      "loss: 0.456991  [ 1000/ 1536]\n",
      "loss: 0.006740  [ 1100/ 1536]\n",
      "loss: 0.047997  [ 1200/ 1536]\n",
      "loss: 0.024746  [ 1300/ 1536]\n",
      "loss: 0.000087  [ 1400/ 1536]\n",
      "loss: 0.001192  [ 1500/ 1536]\n",
      "loss: 0.003444  [    0/ 1536]\n",
      "loss: 0.012712  [  100/ 1536]\n",
      "loss: 0.077586  [  200/ 1536]\n",
      "loss: 0.012999  [  300/ 1536]\n",
      "loss: 0.043546  [  400/ 1536]\n",
      "loss: 0.003399  [  500/ 1536]\n",
      "loss: 0.553820  [  600/ 1536]\n",
      "loss: 0.000801  [  700/ 1536]\n",
      "loss: 0.050240  [  800/ 1536]\n",
      "loss: 0.119883  [  900/ 1536]\n",
      "loss: 0.095726  [ 1000/ 1536]\n",
      "loss: 0.000260  [ 1100/ 1536]\n",
      "loss: 0.056587  [ 1200/ 1536]\n",
      "loss: 0.060330  [ 1300/ 1536]\n",
      "loss: 0.042735  [ 1400/ 1536]\n",
      "loss: 0.145306  [ 1500/ 1536]\n",
      "loss: 0.000092  [    0/ 1536]\n",
      "loss: 0.018016  [  100/ 1536]\n",
      "loss: 0.064177  [  200/ 1536]\n",
      "loss: 0.003311  [  300/ 1536]\n",
      "loss: 0.007627  [  400/ 1536]\n",
      "loss: 0.035487  [  500/ 1536]\n",
      "loss: 0.026145  [  600/ 1536]\n",
      "loss: 0.019034  [  700/ 1536]\n",
      "loss: 0.013420  [  800/ 1536]\n",
      "loss: 0.118235  [  900/ 1536]\n",
      "loss: 0.049384  [ 1000/ 1536]\n",
      "loss: 0.004674  [ 1100/ 1536]\n",
      "loss: 0.001466  [ 1200/ 1536]\n",
      "loss: 0.102265  [ 1300/ 1536]\n",
      "loss: 0.008108  [ 1400/ 1536]\n",
      "loss: 0.019405  [ 1500/ 1536]\n",
      "loss: 0.070426  [    0/ 1536]\n",
      "loss: 0.248465  [  100/ 1536]\n",
      "loss: 0.017258  [  200/ 1536]\n",
      "loss: 0.013121  [  300/ 1536]\n",
      "loss: 0.080695  [  400/ 1536]\n",
      "loss: 0.051990  [  500/ 1536]\n",
      "loss: 0.048167  [  600/ 1536]\n",
      "loss: 0.051966  [  700/ 1536]\n",
      "loss: 0.050975  [  800/ 1536]\n",
      "loss: 0.007554  [  900/ 1536]\n",
      "loss: 0.125929  [ 1000/ 1536]\n",
      "loss: 0.019974  [ 1100/ 1536]\n",
      "loss: 0.192924  [ 1200/ 1536]\n",
      "loss: 0.002896  [ 1300/ 1536]\n",
      "loss: 0.040834  [ 1400/ 1536]\n",
      "loss: 0.017205  [ 1500/ 1536]\n",
      "loss: 0.027038  [    0/ 1536]\n",
      "loss: 0.012578  [  100/ 1536]\n",
      "loss: 0.010851  [  200/ 1536]\n",
      "loss: 0.011395  [  300/ 1536]\n",
      "loss: 0.005917  [  400/ 1536]\n",
      "loss: 0.022315  [  500/ 1536]\n",
      "loss: 0.011725  [  600/ 1536]\n",
      "loss: 0.069919  [  700/ 1536]\n",
      "loss: 0.001107  [  800/ 1536]\n",
      "loss: 0.094567  [  900/ 1536]\n",
      "loss: 0.006472  [ 1000/ 1536]\n",
      "loss: 0.005838  [ 1100/ 1536]\n",
      "loss: 0.108374  [ 1200/ 1536]\n",
      "loss: 0.289134  [ 1300/ 1536]\n",
      "loss: 0.019190  [ 1400/ 1536]\n",
      "loss: 0.035836  [ 1500/ 1536]\n",
      "loss: 0.001535  [    0/ 1536]\n",
      "loss: 0.012465  [  100/ 1536]\n",
      "loss: 0.118441  [  200/ 1536]\n",
      "loss: 0.001233  [  300/ 1536]\n",
      "loss: 0.005776  [  400/ 1536]\n",
      "loss: 0.023220  [  500/ 1536]\n",
      "loss: 0.048071  [  600/ 1536]\n",
      "loss: 0.052131  [  700/ 1536]\n",
      "loss: 0.082426  [  800/ 1536]\n",
      "loss: 0.053739  [  900/ 1536]\n",
      "loss: 0.027456  [ 1000/ 1536]\n",
      "loss: 0.130757  [ 1100/ 1536]\n",
      "loss: 0.075032  [ 1200/ 1536]\n",
      "loss: 0.001203  [ 1300/ 1536]\n",
      "loss: 0.002940  [ 1400/ 1536]\n",
      "loss: 0.002289  [ 1500/ 1536]\n",
      "loss: 0.002272  [    0/ 1536]\n",
      "loss: 0.076444  [  100/ 1536]\n",
      "loss: 0.005513  [  200/ 1536]\n",
      "loss: 0.071561  [  300/ 1536]\n",
      "loss: 0.001600  [  400/ 1536]\n",
      "loss: 0.020246  [  500/ 1536]\n",
      "loss: 0.081722  [  600/ 1536]\n",
      "loss: 0.039807  [  700/ 1536]\n",
      "loss: 0.137287  [  800/ 1536]\n",
      "loss: 0.066922  [  900/ 1536]\n",
      "loss: 0.049710  [ 1000/ 1536]\n",
      "loss: 0.032981  [ 1100/ 1536]\n",
      "loss: 0.015583  [ 1200/ 1536]\n",
      "loss: 0.000067  [ 1300/ 1536]\n",
      "loss: 0.068073  [ 1400/ 1536]\n",
      "loss: 0.104555  [ 1500/ 1536]\n",
      "loss: 0.004168  [    0/ 1536]\n",
      "loss: 0.090961  [  100/ 1536]\n",
      "loss: 0.035273  [  200/ 1536]\n",
      "loss: 0.009318  [  300/ 1536]\n",
      "loss: 0.001884  [  400/ 1536]\n",
      "loss: 0.028773  [  500/ 1536]\n",
      "loss: 0.005897  [  600/ 1536]\n",
      "loss: 0.005719  [  700/ 1536]\n",
      "loss: 0.060584  [  800/ 1536]\n",
      "loss: 0.000075  [  900/ 1536]\n",
      "loss: 0.043517  [ 1000/ 1536]\n",
      "loss: 0.094940  [ 1100/ 1536]\n",
      "loss: 0.014216  [ 1200/ 1536]\n",
      "loss: 0.010316  [ 1300/ 1536]\n",
      "loss: 0.038171  [ 1400/ 1536]\n",
      "loss: 0.000018  [ 1500/ 1536]\n",
      "loss: 0.007021  [    0/ 1536]\n",
      "loss: 0.000306  [  100/ 1536]\n",
      "loss: 0.026249  [  200/ 1536]\n",
      "loss: 0.000655  [  300/ 1536]\n",
      "loss: 0.032867  [  400/ 1536]\n",
      "loss: 0.088400  [  500/ 1536]\n",
      "loss: 0.182161  [  600/ 1536]\n",
      "loss: 0.000387  [  700/ 1536]\n",
      "loss: 0.002469  [  800/ 1536]\n",
      "loss: 0.189950  [  900/ 1536]\n",
      "loss: 0.001362  [ 1000/ 1536]\n",
      "loss: 0.085317  [ 1100/ 1536]\n",
      "loss: 0.004103  [ 1200/ 1536]\n",
      "loss: 0.036820  [ 1300/ 1536]\n",
      "loss: 0.002593  [ 1400/ 1536]\n",
      "loss: 0.000605  [ 1500/ 1536]\n",
      "loss: 0.020901  [    0/ 1536]\n",
      "loss: 0.001427  [  100/ 1536]\n",
      "loss: 0.000008  [  200/ 1536]\n",
      "loss: 0.001330  [  300/ 1536]\n",
      "loss: 0.036107  [  400/ 1536]\n",
      "loss: 0.237805  [  500/ 1536]\n",
      "loss: 0.075967  [  600/ 1536]\n",
      "loss: 0.187472  [  700/ 1536]\n",
      "loss: 0.001475  [  800/ 1536]\n",
      "loss: 0.000079  [  900/ 1536]\n",
      "loss: 0.069491  [ 1000/ 1536]\n",
      "loss: 0.012971  [ 1100/ 1536]\n",
      "loss: 0.018546  [ 1200/ 1536]\n",
      "loss: 0.052148  [ 1300/ 1536]\n",
      "loss: 0.086105  [ 1400/ 1536]\n",
      "loss: 0.065044  [ 1500/ 1536]\n",
      "loss: 0.019873  [    0/ 1536]\n",
      "loss: 0.005535  [  100/ 1536]\n",
      "loss: 0.042879  [  200/ 1536]\n",
      "loss: 0.028076  [  300/ 1536]\n",
      "loss: 0.055051  [  400/ 1536]\n",
      "loss: 0.019932  [  500/ 1536]\n",
      "loss: 0.028239  [  600/ 1536]\n",
      "loss: 0.000617  [  700/ 1536]\n",
      "loss: 0.009381  [  800/ 1536]\n",
      "loss: 0.004281  [  900/ 1536]\n",
      "loss: 0.035199  [ 1000/ 1536]\n",
      "loss: 0.023379  [ 1100/ 1536]\n",
      "loss: 0.000102  [ 1200/ 1536]\n",
      "loss: 0.005605  [ 1300/ 1536]\n",
      "loss: 0.130826  [ 1400/ 1536]\n",
      "loss: 0.047448  [ 1500/ 1536]\n",
      "loss: 0.212818  [    0/ 1536]\n",
      "loss: 0.064790  [  100/ 1536]\n",
      "loss: 0.021733  [  200/ 1536]\n",
      "loss: 0.015583  [  300/ 1536]\n",
      "loss: 0.051641  [  400/ 1536]\n",
      "loss: 0.027478  [  500/ 1536]\n",
      "loss: 0.041987  [  600/ 1536]\n",
      "loss: 0.026150  [  700/ 1536]\n",
      "loss: 0.117124  [  800/ 1536]\n",
      "loss: 0.085374  [  900/ 1536]\n",
      "loss: 0.027183  [ 1000/ 1536]\n",
      "loss: 0.126590  [ 1100/ 1536]\n",
      "loss: 0.000123  [ 1200/ 1536]\n",
      "loss: 0.055251  [ 1300/ 1536]\n",
      "loss: 0.155299  [ 1400/ 1536]\n",
      "loss: 0.024897  [ 1500/ 1536]\n",
      "loss: 0.002676  [    0/ 1536]\n",
      "loss: 0.019174  [  100/ 1536]\n",
      "loss: 0.057119  [  200/ 1536]\n",
      "loss: 0.126412  [  300/ 1536]\n",
      "loss: 0.130942  [  400/ 1536]\n",
      "loss: 0.003842  [  500/ 1536]\n",
      "loss: 0.023071  [  600/ 1536]\n",
      "loss: 0.002238  [  700/ 1536]\n",
      "loss: 0.003298  [  800/ 1536]\n",
      "loss: 0.146065  [  900/ 1536]\n",
      "loss: 0.089369  [ 1000/ 1536]\n",
      "loss: 0.066675  [ 1100/ 1536]\n",
      "loss: 0.002565  [ 1200/ 1536]\n",
      "loss: 0.008460  [ 1300/ 1536]\n",
      "loss: 0.280289  [ 1400/ 1536]\n",
      "loss: 0.008166  [ 1500/ 1536]\n",
      "loss: 0.016119  [    0/ 1536]\n",
      "loss: 0.039245  [  100/ 1536]\n",
      "loss: 0.042329  [  200/ 1536]\n",
      "loss: 0.057556  [  300/ 1536]\n",
      "loss: 0.003007  [  400/ 1536]\n",
      "loss: 0.020074  [  500/ 1536]\n",
      "loss: 0.071303  [  600/ 1536]\n",
      "loss: 0.238928  [  700/ 1536]\n",
      "loss: 0.009718  [  800/ 1536]\n",
      "loss: 0.000342  [  900/ 1536]\n",
      "loss: 0.000740  [ 1000/ 1536]\n",
      "loss: 0.001276  [ 1100/ 1536]\n",
      "loss: 0.017317  [ 1200/ 1536]\n",
      "loss: 0.002923  [ 1300/ 1536]\n",
      "loss: 0.066926  [ 1400/ 1536]\n",
      "loss: 0.000419  [ 1500/ 1536]\n",
      "loss: 0.024946  [    0/ 1536]\n",
      "loss: 0.162657  [  100/ 1536]\n",
      "loss: 0.018206  [  200/ 1536]\n",
      "loss: 0.012320  [  300/ 1536]\n",
      "loss: 0.019057  [  400/ 1536]\n",
      "loss: 0.007141  [  500/ 1536]\n",
      "loss: 0.000115  [  600/ 1536]\n",
      "loss: 0.009213  [  700/ 1536]\n",
      "loss: 0.000427  [  800/ 1536]\n",
      "loss: 0.023393  [  900/ 1536]\n",
      "loss: 0.003377  [ 1000/ 1536]\n",
      "loss: 0.004592  [ 1100/ 1536]\n",
      "loss: 0.118043  [ 1200/ 1536]\n",
      "loss: 0.056930  [ 1300/ 1536]\n",
      "loss: 0.000903  [ 1400/ 1536]\n",
      "loss: 0.096295  [ 1500/ 1536]\n",
      "loss: 0.037587  [    0/ 1536]\n",
      "loss: 0.037742  [  100/ 1536]\n",
      "loss: 0.022624  [  200/ 1536]\n",
      "loss: 0.002105  [  300/ 1536]\n",
      "loss: 0.002255  [  400/ 1536]\n",
      "loss: 0.055196  [  500/ 1536]\n",
      "loss: 0.007349  [  600/ 1536]\n",
      "loss: 0.088924  [  700/ 1536]\n",
      "loss: 0.256622  [  800/ 1536]\n",
      "loss: 0.003635  [  900/ 1536]\n",
      "loss: 0.060850  [ 1000/ 1536]\n",
      "loss: 0.116638  [ 1100/ 1536]\n",
      "loss: 0.002891  [ 1200/ 1536]\n",
      "loss: 0.065216  [ 1300/ 1536]\n",
      "loss: 0.039003  [ 1400/ 1536]\n",
      "loss: 0.031337  [ 1500/ 1536]\n",
      "loss: 0.203737  [    0/ 1536]\n",
      "loss: 0.001234  [  100/ 1536]\n",
      "loss: 0.095083  [  200/ 1536]\n",
      "loss: 0.000291  [  300/ 1536]\n",
      "loss: 0.009756  [  400/ 1536]\n",
      "loss: 0.000762  [  500/ 1536]\n",
      "loss: 0.135149  [  600/ 1536]\n",
      "loss: 0.003696  [  700/ 1536]\n",
      "loss: 0.242419  [  800/ 1536]\n",
      "loss: 0.004754  [  900/ 1536]\n",
      "loss: 0.002769  [ 1000/ 1536]\n",
      "loss: 0.178683  [ 1100/ 1536]\n",
      "loss: 0.003296  [ 1200/ 1536]\n",
      "loss: 0.020314  [ 1300/ 1536]\n",
      "loss: 0.051486  [ 1400/ 1536]\n",
      "loss: 0.117751  [ 1500/ 1536]\n",
      "loss: 0.003706  [    0/ 1536]\n",
      "loss: 0.007602  [  100/ 1536]\n",
      "loss: 0.005916  [  200/ 1536]\n",
      "loss: 0.148937  [  300/ 1536]\n",
      "loss: 0.039254  [  400/ 1536]\n",
      "loss: 0.051050  [  500/ 1536]\n",
      "loss: 0.259918  [  600/ 1536]\n",
      "loss: 0.001744  [  700/ 1536]\n",
      "loss: 0.108254  [  800/ 1536]\n",
      "loss: 0.076361  [  900/ 1536]\n",
      "loss: 0.100030  [ 1000/ 1536]\n",
      "loss: 0.001115  [ 1100/ 1536]\n",
      "loss: 0.004161  [ 1200/ 1536]\n",
      "loss: 0.007855  [ 1300/ 1536]\n",
      "loss: 0.017237  [ 1400/ 1536]\n",
      "loss: 0.000754  [ 1500/ 1536]\n",
      "loss: 0.045576  [    0/ 1536]\n",
      "loss: 0.002001  [  100/ 1536]\n",
      "loss: 0.001430  [  200/ 1536]\n",
      "loss: 0.001147  [  300/ 1536]\n",
      "loss: 0.012272  [  400/ 1536]\n",
      "loss: 0.013477  [  500/ 1536]\n",
      "loss: 0.108531  [  600/ 1536]\n",
      "loss: 0.024281  [  700/ 1536]\n",
      "loss: 0.141949  [  800/ 1536]\n",
      "loss: 0.001244  [  900/ 1536]\n",
      "loss: 0.001524  [ 1000/ 1536]\n",
      "loss: 0.001618  [ 1100/ 1536]\n",
      "loss: 0.050432  [ 1200/ 1536]\n",
      "loss: 0.000015  [ 1300/ 1536]\n",
      "loss: 0.010740  [ 1400/ 1536]\n",
      "loss: 0.042802  [ 1500/ 1536]\n",
      "loss: 0.003485  [    0/ 1536]\n",
      "loss: 0.015054  [  100/ 1536]\n",
      "loss: 0.013930  [  200/ 1536]\n",
      "loss: 0.164283  [  300/ 1536]\n",
      "loss: 0.064659  [  400/ 1536]\n",
      "loss: 0.128860  [  500/ 1536]\n",
      "loss: 0.022865  [  600/ 1536]\n",
      "loss: 0.013314  [  700/ 1536]\n",
      "loss: 0.013902  [  800/ 1536]\n",
      "loss: 0.083924  [  900/ 1536]\n",
      "loss: 0.013358  [ 1000/ 1536]\n",
      "loss: 0.193333  [ 1100/ 1536]\n",
      "loss: 0.000274  [ 1200/ 1536]\n",
      "loss: 0.094980  [ 1300/ 1536]\n",
      "loss: 0.085118  [ 1400/ 1536]\n",
      "loss: 0.010211  [ 1500/ 1536]\n",
      "loss: 0.013716  [    0/ 1536]\n",
      "loss: 0.107797  [  100/ 1536]\n",
      "loss: 0.007217  [  200/ 1536]\n",
      "loss: 0.055103  [  300/ 1536]\n",
      "loss: 0.010798  [  400/ 1536]\n",
      "loss: 0.001143  [  500/ 1536]\n",
      "loss: 0.016470  [  600/ 1536]\n",
      "loss: 0.015823  [  700/ 1536]\n",
      "loss: 0.049923  [  800/ 1536]\n",
      "loss: 0.050043  [  900/ 1536]\n",
      "loss: 0.064974  [ 1000/ 1536]\n",
      "loss: 0.010049  [ 1100/ 1536]\n",
      "loss: 0.037063  [ 1200/ 1536]\n",
      "loss: 0.008550  [ 1300/ 1536]\n",
      "loss: 0.043737  [ 1400/ 1536]\n",
      "loss: 0.024426  [ 1500/ 1536]\n",
      "loss: 0.037323  [    0/ 1536]\n",
      "loss: 0.023737  [  100/ 1536]\n",
      "loss: 0.005689  [  200/ 1536]\n",
      "loss: 0.031860  [  300/ 1536]\n",
      "loss: 0.052412  [  400/ 1536]\n",
      "loss: 0.000154  [  500/ 1536]\n",
      "loss: 0.106916  [  600/ 1536]\n",
      "loss: 0.007124  [  700/ 1536]\n",
      "loss: 0.036273  [  800/ 1536]\n",
      "loss: 0.004441  [  900/ 1536]\n",
      "loss: 0.021249  [ 1000/ 1536]\n",
      "loss: 0.227053  [ 1100/ 1536]\n",
      "loss: 0.005047  [ 1200/ 1536]\n",
      "loss: 0.034069  [ 1300/ 1536]\n",
      "loss: 0.003663  [ 1400/ 1536]\n",
      "loss: 0.274991  [ 1500/ 1536]\n",
      "loss: 0.026895  [    0/ 1536]\n",
      "loss: 0.002747  [  100/ 1536]\n",
      "loss: 0.102141  [  200/ 1536]\n",
      "loss: 0.007197  [  300/ 1536]\n",
      "loss: 0.016427  [  400/ 1536]\n",
      "loss: 0.001601  [  500/ 1536]\n",
      "loss: 0.018432  [  600/ 1536]\n",
      "loss: 0.106696  [  700/ 1536]\n",
      "loss: 0.010164  [  800/ 1536]\n",
      "loss: 0.011623  [  900/ 1536]\n",
      "loss: 0.060394  [ 1000/ 1536]\n",
      "loss: 0.000654  [ 1100/ 1536]\n",
      "loss: 0.000447  [ 1200/ 1536]\n",
      "loss: 0.001413  [ 1300/ 1536]\n",
      "loss: 0.004963  [ 1400/ 1536]\n",
      "loss: 0.000376  [ 1500/ 1536]\n",
      "loss: 0.067296  [    0/ 1536]\n",
      "loss: 0.001702  [  100/ 1536]\n",
      "loss: 0.004733  [  200/ 1536]\n",
      "loss: 0.076306  [  300/ 1536]\n",
      "loss: 0.000032  [  400/ 1536]\n",
      "loss: 0.007037  [  500/ 1536]\n",
      "loss: 0.084065  [  600/ 1536]\n",
      "loss: 0.245524  [  700/ 1536]\n",
      "loss: 0.001868  [  800/ 1536]\n",
      "loss: 0.003780  [  900/ 1536]\n",
      "loss: 0.001698  [ 1000/ 1536]\n",
      "loss: 0.011043  [ 1100/ 1536]\n",
      "loss: 0.189566  [ 1200/ 1536]\n",
      "loss: 0.071293  [ 1300/ 1536]\n",
      "loss: 0.020050  [ 1400/ 1536]\n",
      "loss: 0.047945  [ 1500/ 1536]\n",
      "loss: 0.128520  [    0/ 1536]\n",
      "loss: 0.187930  [  100/ 1536]\n",
      "loss: 0.007352  [  200/ 1536]\n",
      "loss: 0.000053  [  300/ 1536]\n",
      "loss: 0.003202  [  400/ 1536]\n",
      "loss: 0.052838  [  500/ 1536]\n",
      "loss: 0.008499  [  600/ 1536]\n",
      "loss: 0.126519  [  700/ 1536]\n",
      "loss: 0.056901  [  800/ 1536]\n",
      "loss: 0.007379  [  900/ 1536]\n",
      "loss: 0.240893  [ 1000/ 1536]\n",
      "loss: 0.041057  [ 1100/ 1536]\n",
      "loss: 0.000185  [ 1200/ 1536]\n",
      "loss: 0.011422  [ 1300/ 1536]\n",
      "loss: 0.001305  [ 1400/ 1536]\n",
      "loss: 0.016340  [ 1500/ 1536]\n",
      "loss: 0.005387  [    0/ 1536]\n",
      "loss: 0.102396  [  100/ 1536]\n",
      "loss: 0.153739  [  200/ 1536]\n",
      "loss: 0.000746  [  300/ 1536]\n",
      "loss: 0.047087  [  400/ 1536]\n",
      "loss: 0.022965  [  500/ 1536]\n",
      "loss: 0.124672  [  600/ 1536]\n",
      "loss: 0.093924  [  700/ 1536]\n",
      "loss: 0.000011  [  800/ 1536]\n",
      "loss: 0.085073  [  900/ 1536]\n",
      "loss: 0.002243  [ 1000/ 1536]\n",
      "loss: 0.007333  [ 1100/ 1536]\n",
      "loss: 0.397999  [ 1200/ 1536]\n",
      "loss: 0.018504  [ 1300/ 1536]\n",
      "loss: 0.077895  [ 1400/ 1536]\n",
      "loss: 0.101549  [ 1500/ 1536]\n",
      "loss: 0.024088  [    0/ 1536]\n",
      "loss: 0.001083  [  100/ 1536]\n",
      "loss: 0.062590  [  200/ 1536]\n",
      "loss: 0.011645  [  300/ 1536]\n",
      "loss: 0.191166  [  400/ 1536]\n",
      "loss: 0.038918  [  500/ 1536]\n",
      "loss: 0.041199  [  600/ 1536]\n",
      "loss: 0.009020  [  700/ 1536]\n",
      "loss: 0.075357  [  800/ 1536]\n",
      "loss: 0.023437  [  900/ 1536]\n",
      "loss: 0.000085  [ 1000/ 1536]\n",
      "loss: 0.088308  [ 1100/ 1536]\n",
      "loss: 0.007501  [ 1200/ 1536]\n",
      "loss: 0.041405  [ 1300/ 1536]\n",
      "loss: 0.017547  [ 1400/ 1536]\n",
      "loss: 0.072803  [ 1500/ 1536]\n",
      "loss: 0.121346  [    0/ 1536]\n",
      "loss: 0.001413  [  100/ 1536]\n",
      "loss: 0.096135  [  200/ 1536]\n",
      "loss: 0.011765  [  300/ 1536]\n",
      "loss: 0.011516  [  400/ 1536]\n",
      "loss: 0.078180  [  500/ 1536]\n",
      "loss: 0.000613  [  600/ 1536]\n",
      "loss: 0.029510  [  700/ 1536]\n",
      "loss: 0.009463  [  800/ 1536]\n",
      "loss: 0.000145  [  900/ 1536]\n",
      "loss: 0.002065  [ 1000/ 1536]\n",
      "loss: 0.000240  [ 1100/ 1536]\n",
      "loss: 0.006724  [ 1200/ 1536]\n",
      "loss: 0.035958  [ 1300/ 1536]\n",
      "loss: 0.132245  [ 1400/ 1536]\n",
      "loss: 0.014224  [ 1500/ 1536]\n",
      "loss: 0.106701  [    0/ 1536]\n",
      "loss: 0.019907  [  100/ 1536]\n",
      "loss: 0.027029  [  200/ 1536]\n",
      "loss: 0.025495  [  300/ 1536]\n",
      "loss: 0.038815  [  400/ 1536]\n",
      "loss: 0.015429  [  500/ 1536]\n",
      "loss: 0.106701  [  600/ 1536]\n",
      "loss: 0.214171  [  700/ 1536]\n",
      "loss: 0.087147  [  800/ 1536]\n",
      "loss: 0.048987  [  900/ 1536]\n",
      "loss: 0.000085  [ 1000/ 1536]\n",
      "loss: 0.006250  [ 1100/ 1536]\n",
      "loss: 0.000295  [ 1200/ 1536]\n",
      "loss: 0.001972  [ 1300/ 1536]\n",
      "loss: 0.059507  [ 1400/ 1536]\n",
      "loss: 0.008527  [ 1500/ 1536]\n",
      "loss: 0.054558  [    0/ 1536]\n",
      "loss: 0.068650  [  100/ 1536]\n",
      "loss: 0.000884  [  200/ 1536]\n",
      "loss: 0.012983  [  300/ 1536]\n",
      "loss: 0.008252  [  400/ 1536]\n",
      "loss: 0.182921  [  500/ 1536]\n",
      "loss: 0.009323  [  600/ 1536]\n",
      "loss: 0.002683  [  700/ 1536]\n",
      "loss: 0.014566  [  800/ 1536]\n",
      "loss: 0.033568  [  900/ 1536]\n",
      "loss: 0.003819  [ 1000/ 1536]\n",
      "loss: 0.000977  [ 1100/ 1536]\n",
      "loss: 0.320931  [ 1200/ 1536]\n",
      "loss: 0.018945  [ 1300/ 1536]\n",
      "loss: 0.184461  [ 1400/ 1536]\n",
      "loss: 0.013223  [ 1500/ 1536]\n",
      "loss: 0.011038  [    0/ 1536]\n",
      "loss: 0.009298  [  100/ 1536]\n",
      "loss: 0.004975  [  200/ 1536]\n",
      "loss: 0.011987  [  300/ 1536]\n",
      "loss: 0.036845  [  400/ 1536]\n",
      "loss: 0.041502  [  500/ 1536]\n",
      "loss: 0.038877  [  600/ 1536]\n",
      "loss: 0.018326  [  700/ 1536]\n",
      "loss: 0.032377  [  800/ 1536]\n",
      "loss: 0.010750  [  900/ 1536]\n",
      "loss: 0.115003  [ 1000/ 1536]\n",
      "loss: 0.019166  [ 1100/ 1536]\n",
      "loss: 0.007084  [ 1200/ 1536]\n",
      "loss: 0.023025  [ 1300/ 1536]\n",
      "loss: 0.043968  [ 1400/ 1536]\n",
      "loss: 0.025766  [ 1500/ 1536]\n",
      "loss: 0.004007  [    0/ 1536]\n",
      "loss: 0.003406  [  100/ 1536]\n",
      "loss: 0.000305  [  200/ 1536]\n",
      "loss: 0.008850  [  300/ 1536]\n",
      "loss: 0.005825  [  400/ 1536]\n",
      "loss: 0.021395  [  500/ 1536]\n",
      "loss: 0.064523  [  600/ 1536]\n",
      "loss: 0.019352  [  700/ 1536]\n",
      "loss: 0.015751  [  800/ 1536]\n",
      "loss: 0.023902  [  900/ 1536]\n",
      "loss: 0.014748  [ 1000/ 1536]\n",
      "loss: 0.001207  [ 1100/ 1536]\n",
      "loss: 0.000159  [ 1200/ 1536]\n",
      "loss: 0.021083  [ 1300/ 1536]\n",
      "loss: 0.003606  [ 1400/ 1536]\n",
      "loss: 0.041932  [ 1500/ 1536]\n",
      "loss: 0.043613  [    0/ 1536]\n",
      "loss: 0.006169  [  100/ 1536]\n",
      "loss: 0.136452  [  200/ 1536]\n",
      "loss: 0.005824  [  300/ 1536]\n",
      "loss: 0.083045  [  400/ 1536]\n",
      "loss: 0.002262  [  500/ 1536]\n",
      "loss: 0.133680  [  600/ 1536]\n",
      "loss: 0.000497  [  700/ 1536]\n",
      "loss: 0.002474  [  800/ 1536]\n",
      "loss: 0.000040  [  900/ 1536]\n",
      "loss: 0.094477  [ 1000/ 1536]\n",
      "loss: 0.000005  [ 1100/ 1536]\n",
      "loss: 0.003185  [ 1200/ 1536]\n",
      "loss: 0.034353  [ 1300/ 1536]\n",
      "loss: 0.006527  [ 1400/ 1536]\n",
      "loss: 0.016578  [ 1500/ 1536]\n",
      "loss: 0.030266  [    0/ 1536]\n",
      "loss: 0.001310  [  100/ 1536]\n",
      "loss: 0.007000  [  200/ 1536]\n",
      "loss: 0.011584  [  300/ 1536]\n",
      "loss: 0.179510  [  400/ 1536]\n",
      "loss: 0.001647  [  500/ 1536]\n",
      "loss: 0.013859  [  600/ 1536]\n",
      "loss: 0.016923  [  700/ 1536]\n",
      "loss: 0.045108  [  800/ 1536]\n",
      "loss: 0.141366  [  900/ 1536]\n",
      "loss: 0.000027  [ 1000/ 1536]\n",
      "loss: 0.162024  [ 1100/ 1536]\n",
      "loss: 0.001308  [ 1200/ 1536]\n",
      "loss: 0.002589  [ 1300/ 1536]\n",
      "loss: 0.001174  [ 1400/ 1536]\n",
      "loss: 0.012552  [ 1500/ 1536]\n",
      "loss: 0.033772  [    0/ 1536]\n",
      "loss: 0.005877  [  100/ 1536]\n",
      "loss: 0.170902  [  200/ 1536]\n",
      "loss: 0.000189  [  300/ 1536]\n",
      "loss: 0.001900  [  400/ 1536]\n",
      "loss: 0.100151  [  500/ 1536]\n",
      "loss: 0.021327  [  600/ 1536]\n",
      "loss: 0.182914  [  700/ 1536]\n",
      "loss: 0.161428  [  800/ 1536]\n",
      "loss: 0.011282  [  900/ 1536]\n",
      "loss: 0.001034  [ 1000/ 1536]\n",
      "loss: 0.065891  [ 1100/ 1536]\n",
      "loss: 0.032919  [ 1200/ 1536]\n",
      "loss: 0.001047  [ 1300/ 1536]\n",
      "loss: 0.123112  [ 1400/ 1536]\n",
      "loss: 0.001908  [ 1500/ 1536]\n",
      "loss: 0.010721  [    0/ 1536]\n",
      "loss: 0.189633  [  100/ 1536]\n",
      "loss: 0.001774  [  200/ 1536]\n",
      "loss: 0.009254  [  300/ 1536]\n",
      "loss: 0.006361  [  400/ 1536]\n",
      "loss: 0.005864  [  500/ 1536]\n",
      "loss: 0.000952  [  600/ 1536]\n",
      "loss: 0.003079  [  700/ 1536]\n",
      "loss: 0.078531  [  800/ 1536]\n",
      "loss: 0.037622  [  900/ 1536]\n",
      "loss: 0.019865  [ 1000/ 1536]\n",
      "loss: 0.108777  [ 1100/ 1536]\n",
      "loss: 0.001534  [ 1200/ 1536]\n",
      "loss: 0.006731  [ 1300/ 1536]\n",
      "loss: 0.009547  [ 1400/ 1536]\n",
      "loss: 0.089224  [ 1500/ 1536]\n",
      "loss: 0.075201  [    0/ 1536]\n",
      "loss: 0.068077  [  100/ 1536]\n",
      "loss: 0.052960  [  200/ 1536]\n",
      "loss: 0.000000  [  300/ 1536]\n",
      "loss: 0.083326  [  400/ 1536]\n",
      "loss: 0.039169  [  500/ 1536]\n",
      "loss: 0.002949  [  600/ 1536]\n",
      "loss: 0.001264  [  700/ 1536]\n",
      "loss: 0.010683  [  800/ 1536]\n",
      "loss: 0.069656  [  900/ 1536]\n",
      "loss: 0.000093  [ 1000/ 1536]\n",
      "loss: 0.000008  [ 1100/ 1536]\n",
      "loss: 0.032777  [ 1200/ 1536]\n",
      "loss: 0.003672  [ 1300/ 1536]\n",
      "loss: 0.094052  [ 1400/ 1536]\n",
      "loss: 0.070388  [ 1500/ 1536]\n",
      "loss: 0.073677  [    0/ 1536]\n",
      "loss: 0.178738  [  100/ 1536]\n",
      "loss: 0.003986  [  200/ 1536]\n",
      "loss: 0.021482  [  300/ 1536]\n",
      "loss: 0.000050  [  400/ 1536]\n",
      "loss: 0.009150  [  500/ 1536]\n",
      "loss: 0.007983  [  600/ 1536]\n",
      "loss: 0.084551  [  700/ 1536]\n",
      "loss: 0.004781  [  800/ 1536]\n",
      "loss: 0.069141  [  900/ 1536]\n",
      "loss: 0.067896  [ 1000/ 1536]\n",
      "loss: 0.014565  [ 1100/ 1536]\n",
      "loss: 0.000001  [ 1200/ 1536]\n",
      "loss: 0.002865  [ 1300/ 1536]\n",
      "loss: 0.046796  [ 1400/ 1536]\n",
      "loss: 0.005331  [ 1500/ 1536]\n",
      "loss: 0.000197  [    0/ 1536]\n",
      "loss: 0.042576  [  100/ 1536]\n",
      "loss: 0.287624  [  200/ 1536]\n",
      "loss: 0.000055  [  300/ 1536]\n",
      "loss: 0.001692  [  400/ 1536]\n",
      "loss: 0.092828  [  500/ 1536]\n",
      "loss: 0.043083  [  600/ 1536]\n",
      "loss: 0.005419  [  700/ 1536]\n",
      "loss: 0.147240  [  800/ 1536]\n",
      "loss: 0.092279  [  900/ 1536]\n",
      "loss: 0.006806  [ 1000/ 1536]\n",
      "loss: 0.006318  [ 1100/ 1536]\n",
      "loss: 0.003656  [ 1200/ 1536]\n",
      "loss: 0.000171  [ 1300/ 1536]\n",
      "loss: 0.003831  [ 1400/ 1536]\n",
      "loss: 0.007801  [ 1500/ 1536]\n",
      "loss: 0.000000  [    0/ 1536]\n",
      "loss: 0.003271  [  100/ 1536]\n",
      "loss: 0.213675  [  200/ 1536]\n",
      "loss: 0.161473  [  300/ 1536]\n",
      "loss: 0.001343  [  400/ 1536]\n",
      "loss: 0.047359  [  500/ 1536]\n",
      "loss: 0.004386  [  600/ 1536]\n",
      "loss: 0.026163  [  700/ 1536]\n",
      "loss: 0.011670  [  800/ 1536]\n",
      "loss: 0.000149  [  900/ 1536]\n",
      "loss: 0.000378  [ 1000/ 1536]\n",
      "loss: 0.217484  [ 1100/ 1536]\n",
      "loss: 0.000004  [ 1200/ 1536]\n",
      "loss: 0.014127  [ 1300/ 1536]\n",
      "loss: 0.023002  [ 1400/ 1536]\n",
      "loss: 0.014840  [ 1500/ 1536]\n",
      "loss: 0.015171  [    0/ 1536]\n",
      "loss: 0.000506  [  100/ 1536]\n",
      "loss: 0.023980  [  200/ 1536]\n",
      "loss: 0.142471  [  300/ 1536]\n",
      "loss: 0.009701  [  400/ 1536]\n",
      "loss: 0.000000  [  500/ 1536]\n",
      "loss: 0.092237  [  600/ 1536]\n",
      "loss: 0.075346  [  700/ 1536]\n",
      "loss: 0.047814  [  800/ 1536]\n",
      "loss: 0.196160  [  900/ 1536]\n",
      "loss: 0.024280  [ 1000/ 1536]\n",
      "loss: 0.000394  [ 1100/ 1536]\n",
      "loss: 0.022187  [ 1200/ 1536]\n",
      "loss: 0.215068  [ 1300/ 1536]\n",
      "loss: 0.015665  [ 1400/ 1536]\n",
      "loss: 0.013101  [ 1500/ 1536]\n",
      "loss: 0.034296  [    0/ 1536]\n",
      "loss: 0.019922  [  100/ 1536]\n",
      "loss: 0.000868  [  200/ 1536]\n",
      "loss: 0.007519  [  300/ 1536]\n",
      "loss: 0.004429  [  400/ 1536]\n",
      "loss: 0.002534  [  500/ 1536]\n",
      "loss: 0.001746  [  600/ 1536]\n",
      "loss: 0.082748  [  700/ 1536]\n",
      "loss: 0.002614  [  800/ 1536]\n",
      "loss: 0.001092  [  900/ 1536]\n",
      "loss: 0.002920  [ 1000/ 1536]\n",
      "loss: 0.048557  [ 1100/ 1536]\n",
      "loss: 0.008018  [ 1200/ 1536]\n",
      "loss: 0.000094  [ 1300/ 1536]\n",
      "loss: 0.067966  [ 1400/ 1536]\n",
      "loss: 0.008164  [ 1500/ 1536]\n",
      "loss: 0.000202  [    0/ 1536]\n",
      "loss: 0.015825  [  100/ 1536]\n",
      "loss: 0.012323  [  200/ 1536]\n",
      "loss: 0.077494  [  300/ 1536]\n",
      "loss: 0.001723  [  400/ 1536]\n",
      "loss: 0.034757  [  500/ 1536]\n",
      "loss: 0.124535  [  600/ 1536]\n",
      "loss: 0.000189  [  700/ 1536]\n",
      "loss: 0.002400  [  800/ 1536]\n",
      "loss: 0.025461  [  900/ 1536]\n",
      "loss: 0.002266  [ 1000/ 1536]\n",
      "loss: 0.000790  [ 1100/ 1536]\n",
      "loss: 0.034858  [ 1200/ 1536]\n",
      "loss: 0.000341  [ 1300/ 1536]\n",
      "loss: 0.104501  [ 1400/ 1536]\n",
      "loss: 0.003305  [ 1500/ 1536]\n",
      "loss: 0.270788  [    0/ 1536]\n",
      "loss: 0.086475  [  100/ 1536]\n",
      "loss: 0.002632  [  200/ 1536]\n",
      "loss: 0.000164  [  300/ 1536]\n",
      "loss: 0.000031  [  400/ 1536]\n",
      "loss: 0.000313  [  500/ 1536]\n",
      "loss: 0.002693  [  600/ 1536]\n",
      "loss: 0.131527  [  700/ 1536]\n",
      "loss: 0.067152  [  800/ 1536]\n",
      "loss: 0.105681  [  900/ 1536]\n",
      "loss: 0.013245  [ 1000/ 1536]\n",
      "loss: 0.030818  [ 1100/ 1536]\n",
      "loss: 0.012188  [ 1200/ 1536]\n",
      "loss: 0.009852  [ 1300/ 1536]\n",
      "loss: 0.018462  [ 1400/ 1536]\n",
      "loss: 0.008003  [ 1500/ 1536]\n",
      "loss: 0.013375  [    0/ 1536]\n",
      "loss: 0.016443  [  100/ 1536]\n",
      "loss: 0.004192  [  200/ 1536]\n",
      "loss: 0.017365  [  300/ 1536]\n",
      "loss: 0.000032  [  400/ 1536]\n",
      "loss: 0.056547  [  500/ 1536]\n",
      "loss: 0.067841  [  600/ 1536]\n",
      "loss: 0.005461  [  700/ 1536]\n",
      "loss: 0.001739  [  800/ 1536]\n",
      "loss: 0.050180  [  900/ 1536]\n",
      "loss: 0.137958  [ 1000/ 1536]\n",
      "loss: 0.128795  [ 1100/ 1536]\n",
      "loss: 0.040415  [ 1200/ 1536]\n",
      "loss: 0.002276  [ 1300/ 1536]\n",
      "loss: 0.078400  [ 1400/ 1536]\n",
      "loss: 0.036756  [ 1500/ 1536]\n",
      "loss: 0.000074  [    0/ 1536]\n",
      "loss: 0.104438  [  100/ 1536]\n",
      "loss: 0.122751  [  200/ 1536]\n",
      "loss: 0.001277  [  300/ 1536]\n",
      "loss: 0.005344  [  400/ 1536]\n",
      "loss: 0.154728  [  500/ 1536]\n",
      "loss: 0.094841  [  600/ 1536]\n",
      "loss: 0.047604  [  700/ 1536]\n",
      "loss: 0.254511  [  800/ 1536]\n",
      "loss: 0.012155  [  900/ 1536]\n",
      "loss: 0.062223  [ 1000/ 1536]\n",
      "loss: 0.052606  [ 1100/ 1536]\n",
      "loss: 0.010876  [ 1200/ 1536]\n",
      "loss: 0.004791  [ 1300/ 1536]\n",
      "loss: 0.000440  [ 1400/ 1536]\n",
      "loss: 0.021084  [ 1500/ 1536]\n",
      "loss: 0.005727  [    0/ 1536]\n",
      "loss: 0.023548  [  100/ 1536]\n",
      "loss: 0.006213  [  200/ 1536]\n",
      "loss: 0.026345  [  300/ 1536]\n",
      "loss: 0.000104  [  400/ 1536]\n",
      "loss: 0.067756  [  500/ 1536]\n",
      "loss: 0.018339  [  600/ 1536]\n",
      "loss: 0.002266  [  700/ 1536]\n",
      "loss: 0.000802  [  800/ 1536]\n",
      "loss: 0.004411  [  900/ 1536]\n",
      "loss: 0.003600  [ 1000/ 1536]\n",
      "loss: 0.001800  [ 1100/ 1536]\n",
      "loss: 0.005555  [ 1200/ 1536]\n",
      "loss: 0.013158  [ 1300/ 1536]\n",
      "loss: 0.055092  [ 1400/ 1536]\n",
      "loss: 0.000851  [ 1500/ 1536]\n",
      "loss: 0.007424  [    0/ 1536]\n",
      "loss: 0.096013  [  100/ 1536]\n",
      "loss: 0.031201  [  200/ 1536]\n",
      "loss: 0.189199  [  300/ 1536]\n",
      "loss: 0.002076  [  400/ 1536]\n",
      "loss: 0.077506  [  500/ 1536]\n",
      "loss: 0.203440  [  600/ 1536]\n",
      "loss: 0.000015  [  700/ 1536]\n",
      "loss: 0.037851  [  800/ 1536]\n",
      "loss: 0.108472  [  900/ 1536]\n",
      "loss: 0.038995  [ 1000/ 1536]\n",
      "loss: 0.000005  [ 1100/ 1536]\n",
      "loss: 0.000405  [ 1200/ 1536]\n",
      "loss: 0.097096  [ 1300/ 1536]\n",
      "loss: 0.007609  [ 1400/ 1536]\n",
      "loss: 0.004368  [ 1500/ 1536]\n",
      "loss: 0.005955  [    0/ 1536]\n",
      "loss: 0.001469  [  100/ 1536]\n",
      "loss: 0.000774  [  200/ 1536]\n",
      "loss: 0.137543  [  300/ 1536]\n",
      "loss: 0.038374  [  400/ 1536]\n",
      "loss: 0.000022  [  500/ 1536]\n",
      "loss: 0.015256  [  600/ 1536]\n",
      "loss: 0.013619  [  700/ 1536]\n",
      "loss: 0.000131  [  800/ 1536]\n",
      "loss: 0.065671  [  900/ 1536]\n",
      "loss: 0.002907  [ 1000/ 1536]\n",
      "loss: 0.080323  [ 1100/ 1536]\n",
      "loss: 0.000193  [ 1200/ 1536]\n",
      "loss: 0.025612  [ 1300/ 1536]\n",
      "loss: 0.072080  [ 1400/ 1536]\n",
      "loss: 0.040329  [ 1500/ 1536]\n",
      "loss: 0.172298  [    0/ 1536]\n",
      "loss: 0.129594  [  100/ 1536]\n",
      "loss: 0.069156  [  200/ 1536]\n",
      "loss: 0.044099  [  300/ 1536]\n",
      "loss: 0.067931  [  400/ 1536]\n",
      "loss: 0.053084  [  500/ 1536]\n",
      "loss: 0.008940  [  600/ 1536]\n",
      "loss: 0.028113  [  700/ 1536]\n",
      "loss: 0.006007  [  800/ 1536]\n",
      "loss: 0.021605  [  900/ 1536]\n",
      "loss: 0.001555  [ 1000/ 1536]\n",
      "loss: 0.003779  [ 1100/ 1536]\n",
      "loss: 0.039959  [ 1200/ 1536]\n",
      "loss: 0.002702  [ 1300/ 1536]\n",
      "loss: 0.012558  [ 1400/ 1536]\n",
      "loss: 0.122464  [ 1500/ 1536]\n",
      "loss: 0.000242  [    0/ 1536]\n",
      "loss: 0.026508  [  100/ 1536]\n",
      "loss: 0.171860  [  200/ 1536]\n",
      "loss: 0.029938  [  300/ 1536]\n",
      "loss: 0.000263  [  400/ 1536]\n",
      "loss: 0.016022  [  500/ 1536]\n",
      "loss: 0.079388  [  600/ 1536]\n",
      "loss: 0.085797  [  700/ 1536]\n",
      "loss: 0.006342  [  800/ 1536]\n",
      "loss: 0.002450  [  900/ 1536]\n",
      "loss: 0.039811  [ 1000/ 1536]\n",
      "loss: 0.028769  [ 1100/ 1536]\n",
      "loss: 0.003423  [ 1200/ 1536]\n",
      "loss: 0.009205  [ 1300/ 1536]\n",
      "loss: 0.091834  [ 1400/ 1536]\n",
      "loss: 0.001855  [ 1500/ 1536]\n",
      "loss: 0.007360  [    0/ 1536]\n",
      "loss: 0.000004  [  100/ 1536]\n",
      "loss: 0.059259  [  200/ 1536]\n",
      "loss: 0.000010  [  300/ 1536]\n",
      "loss: 0.002358  [  400/ 1536]\n",
      "loss: 0.039947  [  500/ 1536]\n",
      "loss: 0.024513  [  600/ 1536]\n",
      "loss: 0.000543  [  700/ 1536]\n",
      "loss: 0.000432  [  800/ 1536]\n",
      "loss: 0.019736  [  900/ 1536]\n",
      "loss: 0.128084  [ 1000/ 1536]\n",
      "loss: 0.039314  [ 1100/ 1536]\n",
      "loss: 0.016587  [ 1200/ 1536]\n",
      "loss: 0.103475  [ 1300/ 1536]\n",
      "loss: 0.000826  [ 1400/ 1536]\n",
      "loss: 0.106773  [ 1500/ 1536]\n",
      "loss: 0.000492  [    0/ 1536]\n",
      "loss: 0.115892  [  100/ 1536]\n",
      "loss: 0.038564  [  200/ 1536]\n",
      "loss: 0.066396  [  300/ 1536]\n",
      "loss: 0.003474  [  400/ 1536]\n",
      "loss: 0.043382  [  500/ 1536]\n",
      "loss: 0.014620  [  600/ 1536]\n",
      "loss: 0.046169  [  700/ 1536]\n",
      "loss: 0.032244  [  800/ 1536]\n",
      "loss: 0.047785  [  900/ 1536]\n",
      "loss: 0.000307  [ 1000/ 1536]\n",
      "loss: 0.110196  [ 1100/ 1536]\n",
      "loss: 0.002949  [ 1200/ 1536]\n",
      "loss: 0.000030  [ 1300/ 1536]\n",
      "loss: 0.031152  [ 1400/ 1536]\n",
      "loss: 0.021002  [ 1500/ 1536]\n",
      "loss: 0.089542  [    0/ 1536]\n",
      "loss: 0.090699  [  100/ 1536]\n",
      "loss: 0.002785  [  200/ 1536]\n",
      "loss: 0.007310  [  300/ 1536]\n",
      "loss: 0.121349  [  400/ 1536]\n",
      "loss: 0.030022  [  500/ 1536]\n",
      "loss: 0.014135  [  600/ 1536]\n",
      "loss: 0.023350  [  700/ 1536]\n",
      "loss: 0.043444  [  800/ 1536]\n",
      "loss: 0.144365  [  900/ 1536]\n",
      "loss: 0.015008  [ 1000/ 1536]\n",
      "loss: 0.116595  [ 1100/ 1536]\n",
      "loss: 0.000012  [ 1200/ 1536]\n",
      "loss: 0.442505  [ 1300/ 1536]\n",
      "loss: 0.278846  [ 1400/ 1536]\n",
      "loss: 0.005519  [ 1500/ 1536]\n",
      "loss: 0.000169  [    0/ 1536]\n",
      "loss: 0.001981  [  100/ 1536]\n",
      "loss: 0.012129  [  200/ 1536]\n",
      "loss: 0.000584  [  300/ 1536]\n",
      "loss: 0.022323  [  400/ 1536]\n",
      "loss: 0.004811  [  500/ 1536]\n",
      "loss: 0.031530  [  600/ 1536]\n",
      "loss: 0.120056  [  700/ 1536]\n",
      "loss: 0.017586  [  800/ 1536]\n",
      "loss: 0.026089  [  900/ 1536]\n",
      "loss: 0.001021  [ 1000/ 1536]\n",
      "loss: 0.019273  [ 1100/ 1536]\n",
      "loss: 0.000275  [ 1200/ 1536]\n",
      "loss: 0.061395  [ 1300/ 1536]\n",
      "loss: 0.084330  [ 1400/ 1536]\n",
      "loss: 0.009084  [ 1500/ 1536]\n",
      "loss: 0.001263  [    0/ 1536]\n",
      "loss: 0.004432  [  100/ 1536]\n",
      "loss: 0.039800  [  200/ 1536]\n",
      "loss: 0.001697  [  300/ 1536]\n",
      "loss: 0.049455  [  400/ 1536]\n",
      "loss: 0.029883  [  500/ 1536]\n",
      "loss: 0.011306  [  600/ 1536]\n",
      "loss: 0.136941  [  700/ 1536]\n",
      "loss: 0.022693  [  800/ 1536]\n",
      "loss: 0.012637  [  900/ 1536]\n",
      "loss: 0.029738  [ 1000/ 1536]\n",
      "loss: 0.011812  [ 1100/ 1536]\n",
      "loss: 0.006884  [ 1200/ 1536]\n",
      "loss: 0.003041  [ 1300/ 1536]\n",
      "loss: 0.052110  [ 1400/ 1536]\n",
      "loss: 0.022649  [ 1500/ 1536]\n",
      "loss: 0.099997  [    0/ 1536]\n",
      "loss: 0.026989  [  100/ 1536]\n",
      "loss: 0.001297  [  200/ 1536]\n",
      "loss: 0.061940  [  300/ 1536]\n",
      "loss: 0.033222  [  400/ 1536]\n",
      "loss: 0.029941  [  500/ 1536]\n",
      "loss: 0.012196  [  600/ 1536]\n",
      "loss: 0.002762  [  700/ 1536]\n",
      "loss: 0.000446  [  800/ 1536]\n",
      "loss: 0.006896  [  900/ 1536]\n",
      "loss: 0.006906  [ 1000/ 1536]\n",
      "loss: 0.002934  [ 1100/ 1536]\n",
      "loss: 0.002488  [ 1200/ 1536]\n",
      "loss: 0.000150  [ 1300/ 1536]\n",
      "loss: 0.057134  [ 1400/ 1536]\n",
      "loss: 0.019366  [ 1500/ 1536]\n",
      "loss: 0.004723  [    0/ 1536]\n",
      "loss: 0.002438  [  100/ 1536]\n",
      "loss: 0.021351  [  200/ 1536]\n",
      "loss: 0.022023  [  300/ 1536]\n",
      "loss: 0.122718  [  400/ 1536]\n",
      "loss: 0.005763  [  500/ 1536]\n",
      "loss: 0.013484  [  600/ 1536]\n",
      "loss: 0.016897  [  700/ 1536]\n",
      "loss: 0.011664  [  800/ 1536]\n",
      "loss: 0.009085  [  900/ 1536]\n",
      "loss: 0.012064  [ 1000/ 1536]\n",
      "loss: 0.008336  [ 1100/ 1536]\n",
      "loss: 0.001384  [ 1200/ 1536]\n",
      "loss: 0.015843  [ 1300/ 1536]\n",
      "loss: 0.000230  [ 1400/ 1536]\n",
      "loss: 0.000022  [ 1500/ 1536]\n",
      "loss: 0.022256  [    0/ 1536]\n",
      "loss: 0.066802  [  100/ 1536]\n",
      "loss: 0.113138  [  200/ 1536]\n",
      "loss: 0.006667  [  300/ 1536]\n",
      "loss: 0.102608  [  400/ 1536]\n",
      "loss: 0.046629  [  500/ 1536]\n",
      "loss: 0.000137  [  600/ 1536]\n",
      "loss: 0.002991  [  700/ 1536]\n",
      "loss: 0.026482  [  800/ 1536]\n",
      "loss: 0.004997  [  900/ 1536]\n",
      "loss: 0.011209  [ 1000/ 1536]\n",
      "loss: 0.150797  [ 1100/ 1536]\n",
      "loss: 0.360774  [ 1200/ 1536]\n",
      "loss: 0.005964  [ 1300/ 1536]\n",
      "loss: 0.003540  [ 1400/ 1536]\n",
      "loss: 0.150521  [ 1500/ 1536]\n",
      "loss: 0.007719  [    0/ 1536]\n",
      "loss: 0.000008  [  100/ 1536]\n",
      "loss: 0.002442  [  200/ 1536]\n",
      "loss: 0.047250  [  300/ 1536]\n",
      "loss: 0.008880  [  400/ 1536]\n",
      "loss: 0.003662  [  500/ 1536]\n",
      "loss: 0.162994  [  600/ 1536]\n",
      "loss: 0.013236  [  700/ 1536]\n",
      "loss: 0.090705  [  800/ 1536]\n",
      "loss: 0.000753  [  900/ 1536]\n",
      "loss: 0.078461  [ 1000/ 1536]\n",
      "loss: 0.000807  [ 1100/ 1536]\n",
      "loss: 0.003863  [ 1200/ 1536]\n",
      "loss: 0.099066  [ 1300/ 1536]\n",
      "loss: 0.020974  [ 1400/ 1536]\n",
      "loss: 0.104688  [ 1500/ 1536]\n",
      "loss: 0.001741  [    0/ 1536]\n",
      "loss: 0.005271  [  100/ 1536]\n",
      "loss: 0.018456  [  200/ 1536]\n",
      "loss: 0.004777  [  300/ 1536]\n",
      "loss: 0.007300  [  400/ 1536]\n",
      "loss: 0.004921  [  500/ 1536]\n",
      "loss: 0.204300  [  600/ 1536]\n",
      "loss: 0.000592  [  700/ 1536]\n",
      "loss: 0.000362  [  800/ 1536]\n",
      "loss: 0.019086  [  900/ 1536]\n",
      "loss: 0.000996  [ 1000/ 1536]\n",
      "loss: 0.063528  [ 1100/ 1536]\n",
      "loss: 0.155884  [ 1200/ 1536]\n",
      "loss: 0.095196  [ 1300/ 1536]\n",
      "loss: 0.005330  [ 1400/ 1536]\n",
      "loss: 0.122295  [ 1500/ 1536]\n",
      "loss: 0.003839  [    0/ 1536]\n",
      "loss: 0.106184  [  100/ 1536]\n",
      "loss: 0.002973  [  200/ 1536]\n",
      "loss: 0.032278  [  300/ 1536]\n",
      "loss: 0.080515  [  400/ 1536]\n",
      "loss: 0.031725  [  500/ 1536]\n",
      "loss: 0.000405  [  600/ 1536]\n",
      "loss: 0.013672  [  700/ 1536]\n",
      "loss: 0.047750  [  800/ 1536]\n",
      "loss: 0.000594  [  900/ 1536]\n",
      "loss: 0.009667  [ 1000/ 1536]\n",
      "loss: 0.006131  [ 1100/ 1536]\n",
      "loss: 0.008889  [ 1200/ 1536]\n",
      "loss: 0.019735  [ 1300/ 1536]\n",
      "loss: 0.000005  [ 1400/ 1536]\n",
      "loss: 0.011606  [ 1500/ 1536]\n",
      "loss: 0.060153  [    0/ 1536]\n",
      "loss: 0.037259  [  100/ 1536]\n",
      "loss: 0.055223  [  200/ 1536]\n",
      "loss: 0.005415  [  300/ 1536]\n",
      "loss: 0.071372  [  400/ 1536]\n",
      "loss: 0.010395  [  500/ 1536]\n",
      "loss: 0.004230  [  600/ 1536]\n",
      "loss: 0.038044  [  700/ 1536]\n",
      "loss: 0.002683  [  800/ 1536]\n",
      "loss: 0.000085  [  900/ 1536]\n",
      "loss: 0.001582  [ 1000/ 1536]\n",
      "loss: 0.103875  [ 1100/ 1536]\n",
      "loss: 0.003632  [ 1200/ 1536]\n",
      "loss: 0.001711  [ 1300/ 1536]\n",
      "loss: 0.029595  [ 1400/ 1536]\n",
      "loss: 0.076498  [ 1500/ 1536]\n",
      "loss: 0.131213  [    0/ 1536]\n",
      "loss: 0.196087  [  100/ 1536]\n",
      "loss: 0.124674  [  200/ 1536]\n",
      "loss: 0.047285  [  300/ 1536]\n",
      "loss: 0.010763  [  400/ 1536]\n",
      "loss: 0.007472  [  500/ 1536]\n",
      "loss: 0.009587  [  600/ 1536]\n",
      "loss: 0.046660  [  700/ 1536]\n",
      "loss: 0.071838  [  800/ 1536]\n",
      "loss: 0.011282  [  900/ 1536]\n",
      "loss: 0.009606  [ 1000/ 1536]\n",
      "loss: 0.092163  [ 1100/ 1536]\n",
      "loss: 0.011908  [ 1200/ 1536]\n",
      "loss: 0.133896  [ 1300/ 1536]\n",
      "loss: 0.019068  [ 1400/ 1536]\n",
      "loss: 0.003159  [ 1500/ 1536]\n",
      "loss: 0.004906  [    0/ 1536]\n",
      "loss: 0.086535  [  100/ 1536]\n",
      "loss: 0.326515  [  200/ 1536]\n",
      "loss: 0.001935  [  300/ 1536]\n",
      "loss: 0.000813  [  400/ 1536]\n",
      "loss: 0.089497  [  500/ 1536]\n",
      "loss: 0.000050  [  600/ 1536]\n",
      "loss: 0.176975  [  700/ 1536]\n",
      "loss: 0.022290  [  800/ 1536]\n",
      "loss: 0.012607  [  900/ 1536]\n",
      "loss: 0.052679  [ 1000/ 1536]\n",
      "loss: 0.035512  [ 1100/ 1536]\n",
      "loss: 0.099585  [ 1200/ 1536]\n",
      "loss: 0.026507  [ 1300/ 1536]\n",
      "loss: 0.000710  [ 1400/ 1536]\n",
      "loss: 0.027613  [ 1500/ 1536]\n",
      "loss: 0.030051  [    0/ 1536]\n",
      "loss: 0.122838  [  100/ 1536]\n",
      "loss: 0.000816  [  200/ 1536]\n",
      "loss: 0.040313  [  300/ 1536]\n",
      "loss: 0.005781  [  400/ 1536]\n",
      "loss: 0.100024  [  500/ 1536]\n",
      "loss: 0.198898  [  600/ 1536]\n",
      "loss: 0.003175  [  700/ 1536]\n",
      "loss: 0.004389  [  800/ 1536]\n",
      "loss: 0.000113  [  900/ 1536]\n",
      "loss: 0.006028  [ 1000/ 1536]\n",
      "loss: 0.005336  [ 1100/ 1536]\n",
      "loss: 0.040064  [ 1200/ 1536]\n",
      "loss: 0.074299  [ 1300/ 1536]\n",
      "loss: 0.003320  [ 1400/ 1536]\n",
      "loss: 0.002509  [ 1500/ 1536]\n",
      "loss: 0.012231  [    0/ 1536]\n",
      "loss: 0.015935  [  100/ 1536]\n",
      "loss: 0.092182  [  200/ 1536]\n",
      "loss: 0.019326  [  300/ 1536]\n",
      "loss: 0.009096  [  400/ 1536]\n",
      "loss: 0.000005  [  500/ 1536]\n",
      "loss: 0.020696  [  600/ 1536]\n",
      "loss: 0.042173  [  700/ 1536]\n",
      "loss: 0.023746  [  800/ 1536]\n",
      "loss: 0.063686  [  900/ 1536]\n",
      "loss: 0.012449  [ 1000/ 1536]\n",
      "loss: 0.000043  [ 1100/ 1536]\n",
      "loss: 0.004759  [ 1200/ 1536]\n",
      "loss: 0.011810  [ 1300/ 1536]\n",
      "loss: 0.025612  [ 1400/ 1536]\n",
      "loss: 0.000479  [ 1500/ 1536]\n",
      "loss: 0.018193  [    0/ 1536]\n",
      "loss: 0.104749  [  100/ 1536]\n",
      "loss: 0.049473  [  200/ 1536]\n",
      "loss: 0.077943  [  300/ 1536]\n",
      "loss: 0.031337  [  400/ 1536]\n",
      "loss: 0.006469  [  500/ 1536]\n",
      "loss: 0.000258  [  600/ 1536]\n",
      "loss: 0.007226  [  700/ 1536]\n",
      "loss: 0.010153  [  800/ 1536]\n",
      "loss: 0.003774  [  900/ 1536]\n",
      "loss: 0.002012  [ 1000/ 1536]\n",
      "loss: 0.006284  [ 1100/ 1536]\n",
      "loss: 0.000026  [ 1200/ 1536]\n",
      "loss: 0.027410  [ 1300/ 1536]\n",
      "loss: 0.000099  [ 1400/ 1536]\n",
      "loss: 0.071852  [ 1500/ 1536]\n",
      "loss: 0.002356  [    0/ 1536]\n",
      "loss: 0.000451  [  100/ 1536]\n",
      "loss: 0.000083  [  200/ 1536]\n",
      "loss: 0.087918  [  300/ 1536]\n",
      "loss: 0.029497  [  400/ 1536]\n",
      "loss: 0.022076  [  500/ 1536]\n",
      "loss: 0.000006  [  600/ 1536]\n",
      "loss: 0.081540  [  700/ 1536]\n",
      "loss: 0.077857  [  800/ 1536]\n",
      "loss: 0.003732  [  900/ 1536]\n",
      "loss: 0.001746  [ 1000/ 1536]\n",
      "loss: 0.012349  [ 1100/ 1536]\n",
      "loss: 0.002291  [ 1200/ 1536]\n",
      "loss: 0.002826  [ 1300/ 1536]\n",
      "loss: 0.013736  [ 1400/ 1536]\n",
      "loss: 0.013314  [ 1500/ 1536]\n",
      "loss: 0.121481  [    0/ 1536]\n",
      "loss: 0.000339  [  100/ 1536]\n",
      "loss: 0.284426  [  200/ 1536]\n",
      "loss: 0.000386  [  300/ 1536]\n",
      "loss: 0.027168  [  400/ 1536]\n",
      "loss: 0.166143  [  500/ 1536]\n",
      "loss: 0.004089  [  600/ 1536]\n",
      "loss: 0.156301  [  700/ 1536]\n",
      "loss: 0.040136  [  800/ 1536]\n",
      "loss: 0.005493  [  900/ 1536]\n",
      "loss: 0.017536  [ 1000/ 1536]\n",
      "loss: 0.012346  [ 1100/ 1536]\n",
      "loss: 0.053780  [ 1200/ 1536]\n",
      "loss: 0.004410  [ 1300/ 1536]\n",
      "loss: 0.030877  [ 1400/ 1536]\n",
      "loss: 0.000002  [ 1500/ 1536]\n",
      "loss: 0.021790  [    0/ 1536]\n",
      "loss: 0.072399  [  100/ 1536]\n",
      "loss: 0.243317  [  200/ 1536]\n",
      "loss: 0.000422  [  300/ 1536]\n",
      "loss: 0.003176  [  400/ 1536]\n",
      "loss: 0.004238  [  500/ 1536]\n",
      "loss: 0.000536  [  600/ 1536]\n",
      "loss: 0.010046  [  700/ 1536]\n",
      "loss: 0.013224  [  800/ 1536]\n",
      "loss: 0.033249  [  900/ 1536]\n",
      "loss: 0.007493  [ 1000/ 1536]\n",
      "loss: 0.008711  [ 1100/ 1536]\n",
      "loss: 0.010867  [ 1200/ 1536]\n",
      "loss: 0.001021  [ 1300/ 1536]\n",
      "loss: 0.314661  [ 1400/ 1536]\n",
      "loss: 0.004412  [ 1500/ 1536]\n",
      "loss: 0.043168  [    0/ 1536]\n",
      "loss: 0.007894  [  100/ 1536]\n",
      "loss: 0.008719  [  200/ 1536]\n",
      "loss: 0.011044  [  300/ 1536]\n",
      "loss: 0.018291  [  400/ 1536]\n",
      "loss: 0.005351  [  500/ 1536]\n",
      "loss: 0.238434  [  600/ 1536]\n",
      "loss: 0.001038  [  700/ 1536]\n",
      "loss: 0.000703  [  800/ 1536]\n",
      "loss: 0.004837  [  900/ 1536]\n",
      "loss: 0.014444  [ 1000/ 1536]\n",
      "loss: 0.004082  [ 1100/ 1536]\n",
      "loss: 0.005528  [ 1200/ 1536]\n",
      "loss: 0.171601  [ 1300/ 1536]\n",
      "loss: 0.050089  [ 1400/ 1536]\n",
      "loss: 0.000165  [ 1500/ 1536]\n",
      "loss: 0.003654  [    0/ 1536]\n",
      "loss: 0.000019  [  100/ 1536]\n",
      "loss: 0.002429  [  200/ 1536]\n",
      "loss: 0.163915  [  300/ 1536]\n",
      "loss: 0.000511  [  400/ 1536]\n",
      "loss: 0.070942  [  500/ 1536]\n",
      "loss: 0.000032  [  600/ 1536]\n",
      "loss: 0.012822  [  700/ 1536]\n",
      "loss: 0.008426  [  800/ 1536]\n",
      "loss: 0.031882  [  900/ 1536]\n",
      "loss: 0.047865  [ 1000/ 1536]\n",
      "loss: 0.012275  [ 1100/ 1536]\n",
      "loss: 0.015506  [ 1200/ 1536]\n",
      "loss: 0.000238  [ 1300/ 1536]\n",
      "loss: 0.029338  [ 1400/ 1536]\n",
      "loss: 0.007217  [ 1500/ 1536]\n",
      "loss: 0.062272  [    0/ 1536]\n",
      "loss: 0.197800  [  100/ 1536]\n",
      "loss: 0.000003  [  200/ 1536]\n",
      "loss: 0.124240  [  300/ 1536]\n",
      "loss: 0.031847  [  400/ 1536]\n",
      "loss: 0.048636  [  500/ 1536]\n",
      "loss: 0.000039  [  600/ 1536]\n",
      "loss: 0.014717  [  700/ 1536]\n",
      "loss: 0.006384  [  800/ 1536]\n",
      "loss: 0.055566  [  900/ 1536]\n",
      "loss: 0.000790  [ 1000/ 1536]\n",
      "loss: 0.219269  [ 1100/ 1536]\n",
      "loss: 0.072546  [ 1200/ 1536]\n",
      "loss: 0.007909  [ 1300/ 1536]\n",
      "loss: 0.013839  [ 1400/ 1536]\n",
      "loss: 0.000118  [ 1500/ 1536]\n",
      "loss: 0.003748  [    0/ 1536]\n",
      "loss: 0.002350  [  100/ 1536]\n",
      "loss: 0.204265  [  200/ 1536]\n",
      "loss: 0.016716  [  300/ 1536]\n",
      "loss: 0.099418  [  400/ 1536]\n",
      "loss: 0.110738  [  500/ 1536]\n",
      "loss: 0.000938  [  600/ 1536]\n",
      "loss: 0.002635  [  700/ 1536]\n",
      "loss: 0.000196  [  800/ 1536]\n",
      "loss: 0.008314  [  900/ 1536]\n",
      "loss: 0.037124  [ 1000/ 1536]\n",
      "loss: 0.111949  [ 1100/ 1536]\n",
      "loss: 0.045071  [ 1200/ 1536]\n",
      "loss: 0.003708  [ 1300/ 1536]\n",
      "loss: 0.018689  [ 1400/ 1536]\n",
      "loss: 0.095315  [ 1500/ 1536]\n",
      "loss: 0.001292  [    0/ 1536]\n",
      "loss: 0.014054  [  100/ 1536]\n",
      "loss: 0.001574  [  200/ 1536]\n",
      "loss: 0.220748  [  300/ 1536]\n",
      "loss: 0.205813  [  400/ 1536]\n",
      "loss: 0.007391  [  500/ 1536]\n",
      "loss: 0.006985  [  600/ 1536]\n",
      "loss: 0.006409  [  700/ 1536]\n",
      "loss: 0.094786  [  800/ 1536]\n",
      "loss: 0.012620  [  900/ 1536]\n",
      "loss: 0.023493  [ 1000/ 1536]\n",
      "loss: 0.074443  [ 1100/ 1536]\n",
      "loss: 0.025637  [ 1200/ 1536]\n",
      "loss: 0.037759  [ 1300/ 1536]\n",
      "loss: 0.027453  [ 1400/ 1536]\n",
      "loss: 0.047933  [ 1500/ 1536]\n",
      "loss: 0.027643  [    0/ 1536]\n",
      "loss: 0.002068  [  100/ 1536]\n",
      "loss: 0.006277  [  200/ 1536]\n",
      "loss: 0.166844  [  300/ 1536]\n",
      "loss: 0.016273  [  400/ 1536]\n",
      "loss: 0.000046  [  500/ 1536]\n",
      "loss: 0.071143  [  600/ 1536]\n",
      "loss: 0.040448  [  700/ 1536]\n",
      "loss: 0.014920  [  800/ 1536]\n",
      "loss: 0.070184  [  900/ 1536]\n",
      "loss: 0.008427  [ 1000/ 1536]\n",
      "loss: 0.001282  [ 1100/ 1536]\n",
      "loss: 0.169785  [ 1200/ 1536]\n",
      "loss: 0.001883  [ 1300/ 1536]\n",
      "loss: 0.013142  [ 1400/ 1536]\n",
      "loss: 0.081457  [ 1500/ 1536]\n",
      "loss: 0.012000  [    0/ 1536]\n",
      "loss: 0.112968  [  100/ 1536]\n",
      "loss: 0.040276  [  200/ 1536]\n",
      "loss: 0.099636  [  300/ 1536]\n",
      "loss: 0.052042  [  400/ 1536]\n",
      "loss: 0.083633  [  500/ 1536]\n",
      "loss: 0.012234  [  600/ 1536]\n",
      "loss: 0.064928  [  700/ 1536]\n",
      "loss: 0.073588  [  800/ 1536]\n",
      "loss: 0.023976  [  900/ 1536]\n",
      "loss: 0.019784  [ 1000/ 1536]\n",
      "loss: 0.003866  [ 1100/ 1536]\n",
      "loss: 0.024444  [ 1200/ 1536]\n",
      "loss: 0.008394  [ 1300/ 1536]\n",
      "loss: 0.015875  [ 1400/ 1536]\n",
      "loss: 0.000246  [ 1500/ 1536]\n",
      "loss: 0.030932  [    0/ 1536]\n",
      "loss: 0.003508  [  100/ 1536]\n",
      "loss: 0.000179  [  200/ 1536]\n",
      "loss: 0.000144  [  300/ 1536]\n",
      "loss: 0.009922  [  400/ 1536]\n",
      "loss: 0.000051  [  500/ 1536]\n",
      "loss: 0.000010  [  600/ 1536]\n",
      "loss: 0.024320  [  700/ 1536]\n",
      "loss: 0.001740  [  800/ 1536]\n",
      "loss: 0.165605  [  900/ 1536]\n",
      "loss: 0.136291  [ 1000/ 1536]\n",
      "loss: 0.022708  [ 1100/ 1536]\n",
      "loss: 0.029912  [ 1200/ 1536]\n",
      "loss: 0.001049  [ 1300/ 1536]\n",
      "loss: 0.000582  [ 1400/ 1536]\n",
      "loss: 0.044058  [ 1500/ 1536]\n",
      "loss: 0.110717  [    0/ 1536]\n",
      "loss: 0.008249  [  100/ 1536]\n",
      "loss: 0.076096  [  200/ 1536]\n",
      "loss: 0.065596  [  300/ 1536]\n",
      "loss: 0.082277  [  400/ 1536]\n",
      "loss: 0.007625  [  500/ 1536]\n",
      "loss: 0.162959  [  600/ 1536]\n",
      "loss: 0.005395  [  700/ 1536]\n",
      "loss: 0.008047  [  800/ 1536]\n",
      "loss: 0.000221  [  900/ 1536]\n",
      "loss: 0.069671  [ 1000/ 1536]\n",
      "loss: 0.071136  [ 1100/ 1536]\n",
      "loss: 0.003164  [ 1200/ 1536]\n",
      "loss: 0.025744  [ 1300/ 1536]\n",
      "loss: 0.238450  [ 1400/ 1536]\n",
      "loss: 0.003722  [ 1500/ 1536]\n",
      "loss: 0.023057  [    0/ 1536]\n",
      "loss: 0.238166  [  100/ 1536]\n",
      "loss: 0.102790  [  200/ 1536]\n",
      "loss: 0.001718  [  300/ 1536]\n",
      "loss: 0.009554  [  400/ 1536]\n",
      "loss: 0.006199  [  500/ 1536]\n",
      "loss: 0.102620  [  600/ 1536]\n",
      "loss: 0.037661  [  700/ 1536]\n",
      "loss: 0.038773  [  800/ 1536]\n",
      "loss: 0.004546  [  900/ 1536]\n",
      "loss: 0.000239  [ 1000/ 1536]\n",
      "loss: 0.016360  [ 1100/ 1536]\n",
      "loss: 0.041670  [ 1200/ 1536]\n",
      "loss: 0.225747  [ 1300/ 1536]\n",
      "loss: 0.013829  [ 1400/ 1536]\n",
      "loss: 0.007506  [ 1500/ 1536]\n",
      "loss: 0.000592  [    0/ 1536]\n",
      "loss: 0.058261  [  100/ 1536]\n",
      "loss: 0.000558  [  200/ 1536]\n",
      "loss: 0.077482  [  300/ 1536]\n",
      "loss: 0.001105  [  400/ 1536]\n",
      "loss: 0.004201  [  500/ 1536]\n",
      "loss: 0.006524  [  600/ 1536]\n",
      "loss: 0.042268  [  700/ 1536]\n",
      "loss: 0.001002  [  800/ 1536]\n",
      "loss: 0.117041  [  900/ 1536]\n",
      "loss: 0.025673  [ 1000/ 1536]\n",
      "loss: 0.009034  [ 1100/ 1536]\n",
      "loss: 0.244509  [ 1200/ 1536]\n",
      "loss: 0.008781  [ 1300/ 1536]\n",
      "loss: 0.000565  [ 1400/ 1536]\n",
      "loss: 0.009668  [ 1500/ 1536]\n",
      "loss: 0.023613  [    0/ 1536]\n",
      "loss: 0.087017  [  100/ 1536]\n",
      "loss: 0.008258  [  200/ 1536]\n",
      "loss: 0.083372  [  300/ 1536]\n",
      "loss: 0.002488  [  400/ 1536]\n",
      "loss: 0.005930  [  500/ 1536]\n",
      "loss: 0.229089  [  600/ 1536]\n",
      "loss: 0.132587  [  700/ 1536]\n",
      "loss: 0.202300  [  800/ 1536]\n",
      "loss: 0.084450  [  900/ 1536]\n",
      "loss: 0.105985  [ 1000/ 1536]\n",
      "loss: 0.000046  [ 1100/ 1536]\n",
      "loss: 0.098736  [ 1200/ 1536]\n",
      "loss: 0.155701  [ 1300/ 1536]\n",
      "loss: 0.042743  [ 1400/ 1536]\n",
      "loss: 0.074806  [ 1500/ 1536]\n",
      "loss: 0.028829  [    0/ 1536]\n",
      "loss: 0.000147  [  100/ 1536]\n",
      "loss: 0.015240  [  200/ 1536]\n",
      "loss: 0.002274  [  300/ 1536]\n",
      "loss: 0.028726  [  400/ 1536]\n",
      "loss: 0.025424  [  500/ 1536]\n",
      "loss: 0.026048  [  600/ 1536]\n",
      "loss: 0.018409  [  700/ 1536]\n",
      "loss: 0.000566  [  800/ 1536]\n",
      "loss: 0.005879  [  900/ 1536]\n",
      "loss: 0.000940  [ 1000/ 1536]\n",
      "loss: 0.173244  [ 1100/ 1536]\n",
      "loss: 0.003616  [ 1200/ 1536]\n",
      "loss: 0.146014  [ 1300/ 1536]\n",
      "loss: 0.002633  [ 1400/ 1536]\n",
      "loss: 0.004669  [ 1500/ 1536]\n",
      "loss: 0.005872  [    0/ 1536]\n",
      "loss: 0.244652  [  100/ 1536]\n",
      "loss: 0.009570  [  200/ 1536]\n",
      "loss: 0.118267  [  300/ 1536]\n",
      "loss: 0.001124  [  400/ 1536]\n",
      "loss: 0.006252  [  500/ 1536]\n",
      "loss: 0.004791  [  600/ 1536]\n",
      "loss: 0.025862  [  700/ 1536]\n",
      "loss: 0.036496  [  800/ 1536]\n",
      "loss: 0.009932  [  900/ 1536]\n",
      "loss: 0.003945  [ 1000/ 1536]\n",
      "loss: 0.042336  [ 1100/ 1536]\n",
      "loss: 0.005901  [ 1200/ 1536]\n",
      "loss: 0.006184  [ 1300/ 1536]\n",
      "loss: 0.034695  [ 1400/ 1536]\n",
      "loss: 0.026250  [ 1500/ 1536]\n",
      "loss: 0.052909  [    0/ 1536]\n",
      "loss: 0.027145  [  100/ 1536]\n",
      "loss: 0.003074  [  200/ 1536]\n",
      "loss: 0.036523  [  300/ 1536]\n",
      "loss: 0.014977  [  400/ 1536]\n",
      "loss: 0.105177  [  500/ 1536]\n",
      "loss: 0.006929  [  600/ 1536]\n",
      "loss: 0.060794  [  700/ 1536]\n",
      "loss: 0.220839  [  800/ 1536]\n",
      "loss: 0.048523  [  900/ 1536]\n",
      "loss: 0.030417  [ 1000/ 1536]\n",
      "loss: 0.012650  [ 1100/ 1536]\n",
      "loss: 0.036845  [ 1200/ 1536]\n",
      "loss: 0.011057  [ 1300/ 1536]\n",
      "loss: 0.092154  [ 1400/ 1536]\n",
      "loss: 0.002505  [ 1500/ 1536]\n",
      "loss: 0.156945  [    0/ 1536]\n",
      "loss: 0.051324  [  100/ 1536]\n",
      "loss: 0.034469  [  200/ 1536]\n",
      "loss: 0.026798  [  300/ 1536]\n",
      "loss: 0.023865  [  400/ 1536]\n",
      "loss: 0.444373  [  500/ 1536]\n",
      "loss: 0.006818  [  600/ 1536]\n",
      "loss: 0.000002  [  700/ 1536]\n",
      "loss: 0.014995  [  800/ 1536]\n",
      "loss: 0.160498  [  900/ 1536]\n",
      "loss: 0.000044  [ 1000/ 1536]\n",
      "loss: 0.006157  [ 1100/ 1536]\n",
      "loss: 0.057017  [ 1200/ 1536]\n",
      "loss: 0.002130  [ 1300/ 1536]\n",
      "loss: 0.114650  [ 1400/ 1536]\n",
      "loss: 0.036675  [ 1500/ 1536]\n",
      "loss: 0.009833  [    0/ 1536]\n",
      "loss: 0.063316  [  100/ 1536]\n",
      "loss: 0.000671  [  200/ 1536]\n",
      "loss: 0.005776  [  300/ 1536]\n",
      "loss: 0.056306  [  400/ 1536]\n",
      "loss: 0.000014  [  500/ 1536]\n",
      "loss: 0.029070  [  600/ 1536]\n",
      "loss: 0.033609  [  700/ 1536]\n",
      "loss: 0.095391  [  800/ 1536]\n",
      "loss: 0.062194  [  900/ 1536]\n",
      "loss: 0.005086  [ 1000/ 1536]\n",
      "loss: 0.000524  [ 1100/ 1536]\n",
      "loss: 0.044700  [ 1200/ 1536]\n",
      "loss: 0.007803  [ 1300/ 1536]\n",
      "loss: 0.008249  [ 1400/ 1536]\n",
      "loss: 0.018024  [ 1500/ 1536]\n",
      "loss: 0.039335  [    0/ 1536]\n",
      "loss: 0.016482  [  100/ 1536]\n",
      "loss: 0.023665  [  200/ 1536]\n",
      "loss: 0.015663  [  300/ 1536]\n",
      "loss: 0.001476  [  400/ 1536]\n",
      "loss: 0.112100  [  500/ 1536]\n",
      "loss: 0.030562  [  600/ 1536]\n",
      "loss: 0.000113  [  700/ 1536]\n",
      "loss: 0.060077  [  800/ 1536]\n",
      "loss: 0.041740  [  900/ 1536]\n",
      "loss: 0.028167  [ 1000/ 1536]\n",
      "loss: 0.012806  [ 1100/ 1536]\n",
      "loss: 0.146898  [ 1200/ 1536]\n",
      "loss: 0.049897  [ 1300/ 1536]\n",
      "loss: 0.099531  [ 1400/ 1536]\n",
      "loss: 0.016840  [ 1500/ 1536]\n",
      "loss: 0.022585  [    0/ 1536]\n",
      "loss: 0.090831  [  100/ 1536]\n",
      "loss: 0.018374  [  200/ 1536]\n",
      "loss: 0.048556  [  300/ 1536]\n",
      "loss: 0.002030  [  400/ 1536]\n",
      "loss: 0.000036  [  500/ 1536]\n",
      "loss: 0.042530  [  600/ 1536]\n",
      "loss: 0.012135  [  700/ 1536]\n",
      "loss: 0.021188  [  800/ 1536]\n",
      "loss: 0.000544  [  900/ 1536]\n",
      "loss: 0.022671  [ 1000/ 1536]\n",
      "loss: 0.001121  [ 1100/ 1536]\n",
      "loss: 0.001230  [ 1200/ 1536]\n",
      "loss: 0.016240  [ 1300/ 1536]\n",
      "loss: 0.128615  [ 1400/ 1536]\n",
      "loss: 0.003700  [ 1500/ 1536]\n",
      "loss: 0.041538  [    0/ 1536]\n",
      "loss: 0.043431  [  100/ 1536]\n",
      "loss: 0.000943  [  200/ 1536]\n",
      "loss: 0.078292  [  300/ 1536]\n",
      "loss: 0.001403  [  400/ 1536]\n",
      "loss: 0.003502  [  500/ 1536]\n",
      "loss: 0.135173  [  600/ 1536]\n",
      "loss: 0.009701  [  700/ 1536]\n",
      "loss: 0.002782  [  800/ 1536]\n",
      "loss: 0.199145  [  900/ 1536]\n",
      "loss: 0.047972  [ 1000/ 1536]\n",
      "loss: 0.041179  [ 1100/ 1536]\n",
      "loss: 0.028779  [ 1200/ 1536]\n",
      "loss: 0.010282  [ 1300/ 1536]\n",
      "loss: 0.005743  [ 1400/ 1536]\n",
      "loss: 0.073069  [ 1500/ 1536]\n",
      "loss: 0.201757  [    0/ 1536]\n",
      "loss: 0.027573  [  100/ 1536]\n",
      "loss: 0.000128  [  200/ 1536]\n",
      "loss: 0.027023  [  300/ 1536]\n",
      "loss: 0.000745  [  400/ 1536]\n",
      "loss: 0.328116  [  500/ 1536]\n",
      "loss: 0.000040  [  600/ 1536]\n",
      "loss: 0.241531  [  700/ 1536]\n",
      "loss: 0.009609  [  800/ 1536]\n",
      "loss: 0.101912  [  900/ 1536]\n",
      "loss: 0.001968  [ 1000/ 1536]\n",
      "loss: 0.073267  [ 1100/ 1536]\n",
      "loss: 0.022306  [ 1200/ 1536]\n",
      "loss: 0.028877  [ 1300/ 1536]\n",
      "loss: 0.017928  [ 1400/ 1536]\n",
      "loss: 0.050355  [ 1500/ 1536]\n",
      "loss: 0.000034  [    0/ 1536]\n",
      "loss: 0.012192  [  100/ 1536]\n",
      "loss: 0.025626  [  200/ 1536]\n",
      "loss: 0.081952  [  300/ 1536]\n",
      "loss: 0.000192  [  400/ 1536]\n",
      "loss: 0.017275  [  500/ 1536]\n",
      "loss: 0.074858  [  600/ 1536]\n",
      "loss: 0.021195  [  700/ 1536]\n",
      "loss: 0.029119  [  800/ 1536]\n",
      "loss: 0.022901  [  900/ 1536]\n",
      "loss: 0.000123  [ 1000/ 1536]\n",
      "loss: 0.000003  [ 1100/ 1536]\n",
      "loss: 0.129912  [ 1200/ 1536]\n",
      "loss: 0.451709  [ 1300/ 1536]\n",
      "loss: 0.136533  [ 1400/ 1536]\n",
      "loss: 0.020974  [ 1500/ 1536]\n",
      "loss: 0.027268  [    0/ 1536]\n",
      "loss: 0.039483  [  100/ 1536]\n",
      "loss: 0.100790  [  200/ 1536]\n",
      "loss: 0.044816  [  300/ 1536]\n",
      "loss: 0.035770  [  400/ 1536]\n",
      "loss: 0.069446  [  500/ 1536]\n",
      "loss: 0.065327  [  600/ 1536]\n",
      "loss: 0.001007  [  700/ 1536]\n",
      "loss: 0.089368  [  800/ 1536]\n",
      "loss: 0.000751  [  900/ 1536]\n",
      "loss: 0.000952  [ 1000/ 1536]\n",
      "loss: 0.037258  [ 1100/ 1536]\n",
      "loss: 0.006851  [ 1200/ 1536]\n",
      "loss: 0.004499  [ 1300/ 1536]\n",
      "loss: 0.000011  [ 1400/ 1536]\n",
      "loss: 0.028165  [ 1500/ 1536]\n",
      "loss: 0.077314  [    0/ 1536]\n",
      "loss: 0.067242  [  100/ 1536]\n",
      "loss: 0.003566  [  200/ 1536]\n",
      "loss: 0.000253  [  300/ 1536]\n",
      "loss: 0.016775  [  400/ 1536]\n",
      "loss: 0.101758  [  500/ 1536]\n",
      "loss: 0.006105  [  600/ 1536]\n",
      "loss: 0.005632  [  700/ 1536]\n",
      "loss: 0.119716  [  800/ 1536]\n",
      "loss: 0.029505  [  900/ 1536]\n",
      "loss: 0.129197  [ 1000/ 1536]\n",
      "loss: 0.017596  [ 1100/ 1536]\n",
      "loss: 0.028677  [ 1200/ 1536]\n",
      "loss: 0.000877  [ 1300/ 1536]\n",
      "loss: 0.002960  [ 1400/ 1536]\n",
      "loss: 0.005586  [ 1500/ 1536]\n",
      "loss: 0.004845  [    0/ 1536]\n",
      "loss: 0.112560  [  100/ 1536]\n",
      "loss: 0.036302  [  200/ 1536]\n",
      "loss: 0.054064  [  300/ 1536]\n",
      "loss: 0.072481  [  400/ 1536]\n",
      "loss: 0.000944  [  500/ 1536]\n",
      "loss: 0.082155  [  600/ 1536]\n",
      "loss: 0.003697  [  700/ 1536]\n",
      "loss: 0.000544  [  800/ 1536]\n",
      "loss: 0.110581  [  900/ 1536]\n",
      "loss: 0.008518  [ 1000/ 1536]\n",
      "loss: 0.107335  [ 1100/ 1536]\n",
      "loss: 0.107713  [ 1200/ 1536]\n",
      "loss: 0.105108  [ 1300/ 1536]\n",
      "loss: 0.026645  [ 1400/ 1536]\n",
      "loss: 0.002551  [ 1500/ 1536]\n",
      "loss: 0.000977  [    0/ 1536]\n",
      "loss: 0.023106  [  100/ 1536]\n",
      "loss: 0.045236  [  200/ 1536]\n",
      "loss: 0.037453  [  300/ 1536]\n",
      "loss: 0.051985  [  400/ 1536]\n",
      "loss: 0.008225  [  500/ 1536]\n",
      "loss: 0.017942  [  600/ 1536]\n",
      "loss: 0.055062  [  700/ 1536]\n",
      "loss: 0.024395  [  800/ 1536]\n",
      "loss: 0.032620  [  900/ 1536]\n",
      "loss: 0.028269  [ 1000/ 1536]\n",
      "loss: 0.155827  [ 1100/ 1536]\n",
      "loss: 0.019888  [ 1200/ 1536]\n",
      "loss: 0.006143  [ 1300/ 1536]\n",
      "loss: 0.045683  [ 1400/ 1536]\n",
      "loss: 0.004477  [ 1500/ 1536]\n",
      "loss: 0.067932  [    0/ 1536]\n",
      "loss: 0.047168  [  100/ 1536]\n",
      "loss: 0.098880  [  200/ 1536]\n",
      "loss: 0.002493  [  300/ 1536]\n",
      "loss: 0.042786  [  400/ 1536]\n",
      "loss: 0.103349  [  500/ 1536]\n",
      "loss: 0.018195  [  600/ 1536]\n",
      "loss: 0.167902  [  700/ 1536]\n",
      "loss: 0.011673  [  800/ 1536]\n",
      "loss: 0.001160  [  900/ 1536]\n",
      "loss: 0.089064  [ 1000/ 1536]\n",
      "loss: 0.002644  [ 1100/ 1536]\n",
      "loss: 0.143620  [ 1200/ 1536]\n",
      "loss: 0.056647  [ 1300/ 1536]\n",
      "loss: 0.003366  [ 1400/ 1536]\n",
      "loss: 0.093035  [ 1500/ 1536]\n",
      "loss: 0.000749  [    0/ 1536]\n",
      "loss: 0.115086  [  100/ 1536]\n",
      "loss: 0.001152  [  200/ 1536]\n",
      "loss: 0.066917  [  300/ 1536]\n",
      "loss: 0.000694  [  400/ 1536]\n",
      "loss: 0.006925  [  500/ 1536]\n",
      "loss: 0.210298  [  600/ 1536]\n",
      "loss: 0.008630  [  700/ 1536]\n",
      "loss: 0.010983  [  800/ 1536]\n",
      "loss: 0.114809  [  900/ 1536]\n",
      "loss: 0.001516  [ 1000/ 1536]\n",
      "loss: 0.000872  [ 1100/ 1536]\n",
      "loss: 0.001629  [ 1200/ 1536]\n",
      "loss: 0.055012  [ 1300/ 1536]\n",
      "loss: 0.090346  [ 1400/ 1536]\n",
      "loss: 0.103632  [ 1500/ 1536]\n",
      "loss: 0.115516  [    0/ 1536]\n",
      "loss: 0.006427  [  100/ 1536]\n",
      "loss: 0.103080  [  200/ 1536]\n",
      "loss: 0.068622  [  300/ 1536]\n",
      "loss: 0.019813  [  400/ 1536]\n",
      "loss: 0.047070  [  500/ 1536]\n",
      "loss: 0.154597  [  600/ 1536]\n",
      "loss: 0.045762  [  700/ 1536]\n",
      "loss: 0.042485  [  800/ 1536]\n",
      "loss: 0.065471  [  900/ 1536]\n",
      "loss: 0.069899  [ 1000/ 1536]\n",
      "loss: 0.007853  [ 1100/ 1536]\n",
      "loss: 0.100762  [ 1200/ 1536]\n",
      "loss: 0.002690  [ 1300/ 1536]\n",
      "loss: 0.010062  [ 1400/ 1536]\n",
      "loss: 0.024948  [ 1500/ 1536]\n",
      "loss: 0.059974  [    0/ 1536]\n",
      "loss: 0.029650  [  100/ 1536]\n",
      "loss: 0.000322  [  200/ 1536]\n",
      "loss: 0.016954  [  300/ 1536]\n",
      "loss: 0.008679  [  400/ 1536]\n",
      "loss: 0.000471  [  500/ 1536]\n",
      "loss: 0.021608  [  600/ 1536]\n",
      "loss: 0.617564  [  700/ 1536]\n",
      "loss: 0.127389  [  800/ 1536]\n",
      "loss: 0.014032  [  900/ 1536]\n",
      "loss: 0.053266  [ 1000/ 1536]\n",
      "loss: 0.012280  [ 1100/ 1536]\n",
      "loss: 0.071677  [ 1200/ 1536]\n",
      "loss: 0.019907  [ 1300/ 1536]\n",
      "loss: 0.014891  [ 1400/ 1536]\n",
      "loss: 0.043770  [ 1500/ 1536]\n",
      "loss: 0.136867  [    0/ 1536]\n",
      "loss: 0.010285  [  100/ 1536]\n",
      "loss: 0.005008  [  200/ 1536]\n",
      "loss: 0.026840  [  300/ 1536]\n",
      "loss: 0.062239  [  400/ 1536]\n",
      "loss: 0.104491  [  500/ 1536]\n",
      "loss: 0.003077  [  600/ 1536]\n",
      "loss: 0.001161  [  700/ 1536]\n",
      "loss: 0.010764  [  800/ 1536]\n",
      "loss: 0.045245  [  900/ 1536]\n",
      "loss: 0.038103  [ 1000/ 1536]\n",
      "loss: 0.033804  [ 1100/ 1536]\n",
      "loss: 0.042439  [ 1200/ 1536]\n",
      "loss: 0.000051  [ 1300/ 1536]\n",
      "loss: 0.084130  [ 1400/ 1536]\n",
      "loss: 0.018824  [ 1500/ 1536]\n",
      "loss: 0.011378  [    0/ 1536]\n",
      "loss: 0.009942  [  100/ 1536]\n",
      "loss: 0.031809  [  200/ 1536]\n",
      "loss: 0.000308  [  300/ 1536]\n",
      "loss: 0.060786  [  400/ 1536]\n",
      "loss: 0.035229  [  500/ 1536]\n",
      "loss: 0.033316  [  600/ 1536]\n",
      "loss: 0.085906  [  700/ 1536]\n",
      "loss: 0.025335  [  800/ 1536]\n",
      "loss: 0.042481  [  900/ 1536]\n",
      "loss: 0.004271  [ 1000/ 1536]\n",
      "loss: 0.001843  [ 1100/ 1536]\n",
      "loss: 0.000027  [ 1200/ 1536]\n",
      "loss: 0.653062  [ 1300/ 1536]\n",
      "loss: 0.024855  [ 1400/ 1536]\n",
      "loss: 0.005344  [ 1500/ 1536]\n",
      "loss: 0.026431  [    0/ 1536]\n",
      "loss: 0.015460  [  100/ 1536]\n",
      "loss: 0.008286  [  200/ 1536]\n",
      "loss: 0.001550  [  300/ 1536]\n",
      "loss: 0.002147  [  400/ 1536]\n",
      "loss: 0.171138  [  500/ 1536]\n",
      "loss: 0.066190  [  600/ 1536]\n",
      "loss: 0.011677  [  700/ 1536]\n",
      "loss: 0.013386  [  800/ 1536]\n",
      "loss: 0.008992  [  900/ 1536]\n",
      "loss: 0.011557  [ 1000/ 1536]\n",
      "loss: 0.021575  [ 1100/ 1536]\n",
      "loss: 0.065760  [ 1200/ 1536]\n",
      "loss: 0.009258  [ 1300/ 1536]\n",
      "loss: 0.013825  [ 1400/ 1536]\n",
      "loss: 0.049137  [ 1500/ 1536]\n",
      "loss: 0.037422  [    0/ 1536]\n",
      "loss: 0.034762  [  100/ 1536]\n",
      "loss: 0.023099  [  200/ 1536]\n",
      "loss: 0.009982  [  300/ 1536]\n",
      "loss: 0.000820  [  400/ 1536]\n",
      "loss: 0.017023  [  500/ 1536]\n",
      "loss: 0.072348  [  600/ 1536]\n",
      "loss: 0.006889  [  700/ 1536]\n",
      "loss: 0.109914  [  800/ 1536]\n",
      "loss: 0.030116  [  900/ 1536]\n",
      "loss: 0.002333  [ 1000/ 1536]\n",
      "loss: 0.265152  [ 1100/ 1536]\n",
      "loss: 0.003011  [ 1200/ 1536]\n",
      "loss: 0.035384  [ 1300/ 1536]\n",
      "loss: 0.016250  [ 1400/ 1536]\n",
      "loss: 0.000001  [ 1500/ 1536]\n",
      "loss: 0.000692  [    0/ 1536]\n",
      "loss: 0.000318  [  100/ 1536]\n",
      "loss: 0.140146  [  200/ 1536]\n",
      "loss: 0.005428  [  300/ 1536]\n",
      "loss: 0.012959  [  400/ 1536]\n",
      "loss: 0.172017  [  500/ 1536]\n",
      "loss: 0.195147  [  600/ 1536]\n",
      "loss: 0.020049  [  700/ 1536]\n",
      "loss: 0.011498  [  800/ 1536]\n",
      "loss: 0.113158  [  900/ 1536]\n",
      "loss: 0.009592  [ 1000/ 1536]\n",
      "loss: 0.009944  [ 1100/ 1536]\n",
      "loss: 0.011841  [ 1200/ 1536]\n",
      "loss: 0.033305  [ 1300/ 1536]\n",
      "loss: 0.065615  [ 1400/ 1536]\n",
      "loss: 0.019388  [ 1500/ 1536]\n",
      "loss: 0.000000  [    0/ 1536]\n",
      "loss: 0.192380  [  100/ 1536]\n",
      "loss: 0.001487  [  200/ 1536]\n",
      "loss: 0.258547  [  300/ 1536]\n",
      "loss: 0.018774  [  400/ 1536]\n",
      "loss: 0.044300  [  500/ 1536]\n",
      "loss: 0.000957  [  600/ 1536]\n",
      "loss: 0.036864  [  700/ 1536]\n",
      "loss: 0.048379  [  800/ 1536]\n",
      "loss: 0.011391  [  900/ 1536]\n",
      "loss: 0.047248  [ 1000/ 1536]\n",
      "loss: 0.115900  [ 1100/ 1536]\n",
      "loss: 0.018017  [ 1200/ 1536]\n",
      "loss: 0.019626  [ 1300/ 1536]\n",
      "loss: 0.028704  [ 1400/ 1536]\n",
      "loss: 0.030870  [ 1500/ 1536]\n",
      "loss: 0.089522  [    0/ 1536]\n",
      "loss: 0.019648  [  100/ 1536]\n",
      "loss: 0.005132  [  200/ 1536]\n",
      "loss: 0.041539  [  300/ 1536]\n",
      "loss: 0.006026  [  400/ 1536]\n",
      "loss: 0.176445  [  500/ 1536]\n",
      "loss: 0.066276  [  600/ 1536]\n",
      "loss: 0.043714  [  700/ 1536]\n",
      "loss: 0.107591  [  800/ 1536]\n",
      "loss: 0.000413  [  900/ 1536]\n",
      "loss: 0.004582  [ 1000/ 1536]\n",
      "loss: 0.048268  [ 1100/ 1536]\n",
      "loss: 0.114122  [ 1200/ 1536]\n",
      "loss: 0.011499  [ 1300/ 1536]\n",
      "loss: 0.036636  [ 1400/ 1536]\n",
      "loss: 0.188513  [ 1500/ 1536]\n",
      "loss: 0.030062  [    0/ 1536]\n",
      "loss: 0.047126  [  100/ 1536]\n",
      "loss: 0.003711  [  200/ 1536]\n",
      "loss: 0.098637  [  300/ 1536]\n",
      "loss: 0.000249  [  400/ 1536]\n",
      "loss: 0.151471  [  500/ 1536]\n",
      "loss: 0.035245  [  600/ 1536]\n",
      "loss: 0.063487  [  700/ 1536]\n",
      "loss: 0.007927  [  800/ 1536]\n",
      "loss: 0.044546  [  900/ 1536]\n",
      "loss: 0.127058  [ 1000/ 1536]\n",
      "loss: 0.000000  [ 1100/ 1536]\n",
      "loss: 0.047721  [ 1200/ 1536]\n",
      "loss: 0.006233  [ 1300/ 1536]\n",
      "loss: 0.044595  [ 1400/ 1536]\n",
      "loss: 0.009172  [ 1500/ 1536]\n",
      "loss: 0.011755  [    0/ 1536]\n",
      "loss: 0.025794  [  100/ 1536]\n",
      "loss: 0.018327  [  200/ 1536]\n",
      "loss: 0.040335  [  300/ 1536]\n",
      "loss: 0.086151  [  400/ 1536]\n",
      "loss: 0.009464  [  500/ 1536]\n",
      "loss: 0.002697  [  600/ 1536]\n",
      "loss: 0.002003  [  700/ 1536]\n",
      "loss: 0.003707  [  800/ 1536]\n",
      "loss: 0.026003  [  900/ 1536]\n",
      "loss: 0.008079  [ 1000/ 1536]\n",
      "loss: 0.277748  [ 1100/ 1536]\n",
      "loss: 0.100390  [ 1200/ 1536]\n",
      "loss: 0.004585  [ 1300/ 1536]\n",
      "loss: 0.023036  [ 1400/ 1536]\n",
      "loss: 0.095718  [ 1500/ 1536]\n",
      "loss: 0.032200  [    0/ 1536]\n",
      "loss: 0.066981  [  100/ 1536]\n",
      "loss: 0.035626  [  200/ 1536]\n",
      "loss: 0.003089  [  300/ 1536]\n",
      "loss: 0.048941  [  400/ 1536]\n",
      "loss: 0.057518  [  500/ 1536]\n",
      "loss: 0.000000  [  600/ 1536]\n",
      "loss: 0.000465  [  700/ 1536]\n",
      "loss: 0.003549  [  800/ 1536]\n",
      "loss: 0.004730  [  900/ 1536]\n",
      "loss: 0.000053  [ 1000/ 1536]\n",
      "loss: 0.000071  [ 1100/ 1536]\n",
      "loss: 0.001034  [ 1200/ 1536]\n",
      "loss: 0.000514  [ 1300/ 1536]\n",
      "loss: 0.000845  [ 1400/ 1536]\n",
      "loss: 0.093734  [ 1500/ 1536]\n",
      "loss: 0.001431  [    0/ 1536]\n",
      "loss: 0.005059  [  100/ 1536]\n",
      "loss: 0.031312  [  200/ 1536]\n",
      "loss: 0.100635  [  300/ 1536]\n",
      "loss: 0.001829  [  400/ 1536]\n",
      "loss: 0.050441  [  500/ 1536]\n",
      "loss: 0.003877  [  600/ 1536]\n",
      "loss: 0.012168  [  700/ 1536]\n",
      "loss: 0.172527  [  800/ 1536]\n",
      "loss: 0.000713  [  900/ 1536]\n",
      "loss: 0.045139  [ 1000/ 1536]\n",
      "loss: 0.004512  [ 1100/ 1536]\n",
      "loss: 0.085083  [ 1200/ 1536]\n",
      "loss: 0.003353  [ 1300/ 1536]\n",
      "loss: 0.004313  [ 1400/ 1536]\n",
      "loss: 0.000934  [ 1500/ 1536]\n",
      "loss: 0.045984  [    0/ 1536]\n",
      "loss: 0.001015  [  100/ 1536]\n",
      "loss: 0.000765  [  200/ 1536]\n",
      "loss: 0.220415  [  300/ 1536]\n",
      "loss: 0.011922  [  400/ 1536]\n",
      "loss: 0.057663  [  500/ 1536]\n",
      "loss: 0.001918  [  600/ 1536]\n",
      "loss: 0.030712  [  700/ 1536]\n",
      "loss: 0.025759  [  800/ 1536]\n",
      "loss: 0.002160  [  900/ 1536]\n",
      "loss: 0.002015  [ 1000/ 1536]\n",
      "loss: 0.099880  [ 1100/ 1536]\n",
      "loss: 0.044369  [ 1200/ 1536]\n",
      "loss: 0.083050  [ 1300/ 1536]\n",
      "loss: 0.111964  [ 1400/ 1536]\n",
      "loss: 0.003835  [ 1500/ 1536]\n",
      "loss: 0.007745  [    0/ 1536]\n",
      "loss: 0.024366  [  100/ 1536]\n",
      "loss: 0.002036  [  200/ 1536]\n",
      "loss: 0.004950  [  300/ 1536]\n",
      "loss: 0.048775  [  400/ 1536]\n",
      "loss: 0.000296  [  500/ 1536]\n",
      "loss: 0.004457  [  600/ 1536]\n",
      "loss: 0.000030  [  700/ 1536]\n",
      "loss: 0.024322  [  800/ 1536]\n",
      "loss: 0.022230  [  900/ 1536]\n",
      "loss: 0.006713  [ 1000/ 1536]\n",
      "loss: 0.075413  [ 1100/ 1536]\n",
      "loss: 0.000622  [ 1200/ 1536]\n",
      "loss: 0.011685  [ 1300/ 1536]\n",
      "loss: 0.105419  [ 1400/ 1536]\n",
      "loss: 0.018417  [ 1500/ 1536]\n",
      "loss: 0.003559  [    0/ 1536]\n",
      "loss: 0.003332  [  100/ 1536]\n",
      "loss: 0.162570  [  200/ 1536]\n",
      "loss: 0.045151  [  300/ 1536]\n",
      "loss: 0.000147  [  400/ 1536]\n",
      "loss: 0.000406  [  500/ 1536]\n",
      "loss: 0.002009  [  600/ 1536]\n",
      "loss: 0.013845  [  700/ 1536]\n",
      "loss: 0.048135  [  800/ 1536]\n",
      "loss: 0.032801  [  900/ 1536]\n",
      "loss: 0.000252  [ 1000/ 1536]\n",
      "loss: 0.010735  [ 1100/ 1536]\n",
      "loss: 0.028203  [ 1200/ 1536]\n",
      "loss: 0.679706  [ 1300/ 1536]\n",
      "loss: 0.041536  [ 1400/ 1536]\n",
      "loss: 0.061299  [ 1500/ 1536]\n",
      "loss: 0.000008  [    0/ 1536]\n",
      "loss: 0.060242  [  100/ 1536]\n",
      "loss: 0.073275  [  200/ 1536]\n",
      "loss: 0.034732  [  300/ 1536]\n",
      "loss: 0.047282  [  400/ 1536]\n",
      "loss: 0.021122  [  500/ 1536]\n",
      "loss: 0.000410  [  600/ 1536]\n",
      "loss: 0.130357  [  700/ 1536]\n",
      "loss: 0.237800  [  800/ 1536]\n",
      "loss: 0.003371  [  900/ 1536]\n",
      "loss: 0.070645  [ 1000/ 1536]\n",
      "loss: 0.074560  [ 1100/ 1536]\n",
      "loss: 0.002577  [ 1200/ 1536]\n",
      "loss: 0.040020  [ 1300/ 1536]\n",
      "loss: 0.165712  [ 1400/ 1536]\n",
      "loss: 0.006751  [ 1500/ 1536]\n",
      "loss: 0.002176  [    0/ 1536]\n",
      "loss: 0.001184  [  100/ 1536]\n",
      "loss: 0.141242  [  200/ 1536]\n",
      "loss: 0.000303  [  300/ 1536]\n",
      "loss: 0.002623  [  400/ 1536]\n",
      "loss: 0.053701  [  500/ 1536]\n",
      "loss: 0.000337  [  600/ 1536]\n",
      "loss: 0.000116  [  700/ 1536]\n",
      "loss: 0.000332  [  800/ 1536]\n",
      "loss: 0.027901  [  900/ 1536]\n",
      "loss: 0.158701  [ 1000/ 1536]\n",
      "loss: 0.260149  [ 1100/ 1536]\n",
      "loss: 0.013332  [ 1200/ 1536]\n",
      "loss: 0.030552  [ 1300/ 1536]\n",
      "loss: 0.027894  [ 1400/ 1536]\n",
      "loss: 0.014779  [ 1500/ 1536]\n",
      "loss: 0.026866  [    0/ 1536]\n",
      "loss: 0.000720  [  100/ 1536]\n",
      "loss: 0.000126  [  200/ 1536]\n",
      "loss: 0.099475  [  300/ 1536]\n",
      "loss: 0.001538  [  400/ 1536]\n",
      "loss: 0.061740  [  500/ 1536]\n",
      "loss: 0.013358  [  600/ 1536]\n",
      "loss: 0.044027  [  700/ 1536]\n",
      "loss: 0.000000  [  800/ 1536]\n",
      "loss: 0.004035  [  900/ 1536]\n",
      "loss: 0.000948  [ 1000/ 1536]\n",
      "loss: 0.115032  [ 1100/ 1536]\n",
      "loss: 0.004625  [ 1200/ 1536]\n",
      "loss: 0.002235  [ 1300/ 1536]\n",
      "loss: 0.131921  [ 1400/ 1536]\n",
      "loss: 0.018993  [ 1500/ 1536]\n",
      "loss: 0.036520  [    0/ 1536]\n",
      "loss: 0.429931  [  100/ 1536]\n",
      "loss: 0.000836  [  200/ 1536]\n",
      "loss: 0.002185  [  300/ 1536]\n",
      "loss: 0.080409  [  400/ 1536]\n",
      "loss: 0.012456  [  500/ 1536]\n",
      "loss: 0.053730  [  600/ 1536]\n",
      "loss: 0.020840  [  700/ 1536]\n",
      "loss: 0.000196  [  800/ 1536]\n",
      "loss: 0.057683  [  900/ 1536]\n",
      "loss: 0.027051  [ 1000/ 1536]\n",
      "loss: 0.053815  [ 1100/ 1536]\n",
      "loss: 0.057235  [ 1200/ 1536]\n",
      "loss: 0.005856  [ 1300/ 1536]\n",
      "loss: 0.004851  [ 1400/ 1536]\n",
      "loss: 0.051477  [ 1500/ 1536]\n",
      "loss: 0.221801  [    0/ 1536]\n",
      "loss: 0.109549  [  100/ 1536]\n",
      "loss: 0.078126  [  200/ 1536]\n",
      "loss: 0.180875  [  300/ 1536]\n",
      "loss: 0.002481  [  400/ 1536]\n",
      "loss: 0.039430  [  500/ 1536]\n",
      "loss: 0.006141  [  600/ 1536]\n",
      "loss: 0.005979  [  700/ 1536]\n",
      "loss: 0.033767  [  800/ 1536]\n",
      "loss: 0.030411  [  900/ 1536]\n",
      "loss: 0.001192  [ 1000/ 1536]\n",
      "loss: 0.003798  [ 1100/ 1536]\n",
      "loss: 0.012844  [ 1200/ 1536]\n",
      "loss: 0.001000  [ 1300/ 1536]\n",
      "loss: 0.017351  [ 1400/ 1536]\n",
      "loss: 0.029977  [ 1500/ 1536]\n",
      "loss: 0.007235  [    0/ 1536]\n",
      "loss: 0.311000  [  100/ 1536]\n",
      "loss: 0.000720  [  200/ 1536]\n",
      "loss: 0.000003  [  300/ 1536]\n",
      "loss: 0.053442  [  400/ 1536]\n",
      "loss: 0.009214  [  500/ 1536]\n",
      "loss: 0.003787  [  600/ 1536]\n",
      "loss: 0.152542  [  700/ 1536]\n",
      "loss: 0.113893  [  800/ 1536]\n",
      "loss: 0.012544  [  900/ 1536]\n",
      "loss: 0.113532  [ 1000/ 1536]\n",
      "loss: 0.016042  [ 1100/ 1536]\n",
      "loss: 0.003979  [ 1200/ 1536]\n",
      "loss: 0.030374  [ 1300/ 1536]\n",
      "loss: 0.107636  [ 1400/ 1536]\n",
      "loss: 0.191326  [ 1500/ 1536]\n",
      "loss: 0.013233  [    0/ 1536]\n",
      "loss: 0.044838  [  100/ 1536]\n",
      "loss: 0.003344  [  200/ 1536]\n",
      "loss: 0.006958  [  300/ 1536]\n",
      "loss: 0.013184  [  400/ 1536]\n",
      "loss: 0.047053  [  500/ 1536]\n",
      "loss: 0.010936  [  600/ 1536]\n",
      "loss: 0.033914  [  700/ 1536]\n",
      "loss: 0.001272  [  800/ 1536]\n",
      "loss: 0.352986  [  900/ 1536]\n",
      "loss: 0.043360  [ 1000/ 1536]\n",
      "loss: 0.107342  [ 1100/ 1536]\n",
      "loss: 0.025810  [ 1200/ 1536]\n",
      "loss: 0.004219  [ 1300/ 1536]\n",
      "loss: 0.000203  [ 1400/ 1536]\n",
      "loss: 0.159846  [ 1500/ 1536]\n",
      "loss: 0.000625  [    0/ 1536]\n",
      "loss: 0.000130  [  100/ 1536]\n",
      "loss: 0.064125  [  200/ 1536]\n",
      "loss: 0.013444  [  300/ 1536]\n",
      "loss: 0.007735  [  400/ 1536]\n",
      "loss: 0.032396  [  500/ 1536]\n",
      "loss: 0.004315  [  600/ 1536]\n",
      "loss: 0.140663  [  700/ 1536]\n",
      "loss: 0.011920  [  800/ 1536]\n",
      "loss: 0.001031  [  900/ 1536]\n",
      "loss: 0.013165  [ 1000/ 1536]\n",
      "loss: 0.005351  [ 1100/ 1536]\n",
      "loss: 0.018869  [ 1200/ 1536]\n",
      "loss: 0.034986  [ 1300/ 1536]\n",
      "loss: 0.000165  [ 1400/ 1536]\n",
      "loss: 0.002240  [ 1500/ 1536]\n",
      "loss: 0.028889  [    0/ 1536]\n",
      "loss: 0.010397  [  100/ 1536]\n",
      "loss: 0.219016  [  200/ 1536]\n",
      "loss: 0.012772  [  300/ 1536]\n",
      "loss: 0.004141  [  400/ 1536]\n",
      "loss: 0.000843  [  500/ 1536]\n",
      "loss: 0.000603  [  600/ 1536]\n",
      "loss: 0.023021  [  700/ 1536]\n",
      "loss: 0.057341  [  800/ 1536]\n",
      "loss: 0.007898  [  900/ 1536]\n",
      "loss: 0.024390  [ 1000/ 1536]\n",
      "loss: 0.014835  [ 1100/ 1536]\n",
      "loss: 0.016882  [ 1200/ 1536]\n",
      "loss: 0.102153  [ 1300/ 1536]\n",
      "loss: 0.018849  [ 1400/ 1536]\n",
      "loss: 0.013053  [ 1500/ 1536]\n",
      "loss: 0.009416  [    0/ 1536]\n",
      "loss: 0.003071  [  100/ 1536]\n",
      "loss: 0.278142  [  200/ 1536]\n",
      "loss: 0.008222  [  300/ 1536]\n",
      "loss: 0.005728  [  400/ 1536]\n",
      "loss: 0.160215  [  500/ 1536]\n",
      "loss: 0.143597  [  600/ 1536]\n",
      "loss: 0.000093  [  700/ 1536]\n",
      "loss: 0.099797  [  800/ 1536]\n",
      "loss: 0.057249  [  900/ 1536]\n",
      "loss: 0.000583  [ 1000/ 1536]\n",
      "loss: 0.084054  [ 1100/ 1536]\n",
      "loss: 0.028849  [ 1200/ 1536]\n",
      "loss: 0.008241  [ 1300/ 1536]\n",
      "loss: 0.003655  [ 1400/ 1536]\n",
      "loss: 0.008424  [ 1500/ 1536]\n",
      "loss: 0.179277  [    0/ 1536]\n",
      "loss: 0.008109  [  100/ 1536]\n",
      "loss: 0.135249  [  200/ 1536]\n",
      "loss: 0.004449  [  300/ 1536]\n",
      "loss: 0.009797  [  400/ 1536]\n",
      "loss: 0.005462  [  500/ 1536]\n",
      "loss: 0.120508  [  600/ 1536]\n",
      "loss: 0.121970  [  700/ 1536]\n",
      "loss: 0.019217  [  800/ 1536]\n",
      "loss: 0.025968  [  900/ 1536]\n",
      "loss: 0.006504  [ 1000/ 1536]\n",
      "loss: 0.027507  [ 1100/ 1536]\n",
      "loss: 0.026101  [ 1200/ 1536]\n",
      "loss: 0.003363  [ 1300/ 1536]\n",
      "loss: 0.194214  [ 1400/ 1536]\n",
      "loss: 0.000072  [ 1500/ 1536]\n",
      "loss: 0.000231  [    0/ 1536]\n",
      "loss: 0.001876  [  100/ 1536]\n",
      "loss: 0.080635  [  200/ 1536]\n",
      "loss: 0.002798  [  300/ 1536]\n",
      "loss: 0.015925  [  400/ 1536]\n",
      "loss: 0.188397  [  500/ 1536]\n",
      "loss: 0.040394  [  600/ 1536]\n",
      "loss: 0.127029  [  700/ 1536]\n",
      "loss: 0.000010  [  800/ 1536]\n",
      "loss: 0.005658  [  900/ 1536]\n",
      "loss: 0.067228  [ 1000/ 1536]\n",
      "loss: 0.046959  [ 1100/ 1536]\n",
      "loss: 0.006969  [ 1200/ 1536]\n",
      "loss: 0.048254  [ 1300/ 1536]\n",
      "loss: 0.018434  [ 1400/ 1536]\n",
      "loss: 0.125423  [ 1500/ 1536]\n",
      "loss: 0.000707  [    0/ 1536]\n",
      "loss: 0.006078  [  100/ 1536]\n",
      "loss: 0.009919  [  200/ 1536]\n",
      "loss: 0.000171  [  300/ 1536]\n",
      "loss: 0.004250  [  400/ 1536]\n",
      "loss: 0.162431  [  500/ 1536]\n",
      "loss: 0.012620  [  600/ 1536]\n",
      "loss: 0.023535  [  700/ 1536]\n",
      "loss: 0.000200  [  800/ 1536]\n",
      "loss: 0.010238  [  900/ 1536]\n",
      "loss: 0.010960  [ 1000/ 1536]\n",
      "loss: 0.000036  [ 1100/ 1536]\n",
      "loss: 0.002030  [ 1200/ 1536]\n",
      "loss: 0.073232  [ 1300/ 1536]\n",
      "loss: 0.018738  [ 1400/ 1536]\n",
      "loss: 0.096125  [ 1500/ 1536]\n",
      "loss: 0.056116  [    0/ 1536]\n",
      "loss: 0.001151  [  100/ 1536]\n",
      "loss: 0.030514  [  200/ 1536]\n",
      "loss: 0.032433  [  300/ 1536]\n",
      "loss: 0.121649  [  400/ 1536]\n",
      "loss: 0.003606  [  500/ 1536]\n",
      "loss: 0.019671  [  600/ 1536]\n",
      "loss: 0.001680  [  700/ 1536]\n",
      "loss: 0.007775  [  800/ 1536]\n",
      "loss: 0.034436  [  900/ 1536]\n",
      "loss: 0.006075  [ 1000/ 1536]\n",
      "loss: 0.037441  [ 1100/ 1536]\n",
      "loss: 0.008198  [ 1200/ 1536]\n",
      "loss: 0.006323  [ 1300/ 1536]\n",
      "loss: 0.082611  [ 1400/ 1536]\n",
      "loss: 0.001067  [ 1500/ 1536]\n",
      "loss: 0.068177  [    0/ 1536]\n",
      "loss: 0.281928  [  100/ 1536]\n",
      "loss: 0.040798  [  200/ 1536]\n",
      "loss: 0.043306  [  300/ 1536]\n",
      "loss: 0.021013  [  400/ 1536]\n",
      "loss: 0.080222  [  500/ 1536]\n",
      "loss: 0.017027  [  600/ 1536]\n",
      "loss: 0.002170  [  700/ 1536]\n",
      "loss: 0.000856  [  800/ 1536]\n",
      "loss: 0.002623  [  900/ 1536]\n",
      "loss: 0.017352  [ 1000/ 1536]\n",
      "loss: 0.178521  [ 1100/ 1536]\n",
      "loss: 0.035890  [ 1200/ 1536]\n",
      "loss: 0.003815  [ 1300/ 1536]\n",
      "loss: 0.000015  [ 1400/ 1536]\n",
      "loss: 0.007320  [ 1500/ 1536]\n",
      "loss: 0.055494  [    0/ 1536]\n",
      "loss: 0.123410  [  100/ 1536]\n",
      "loss: 0.000075  [  200/ 1536]\n",
      "loss: 0.001564  [  300/ 1536]\n",
      "loss: 0.002849  [  400/ 1536]\n",
      "loss: 0.005565  [  500/ 1536]\n",
      "loss: 0.009188  [  600/ 1536]\n",
      "loss: 0.006351  [  700/ 1536]\n",
      "loss: 0.049819  [  800/ 1536]\n",
      "loss: 0.005483  [  900/ 1536]\n",
      "loss: 0.029070  [ 1000/ 1536]\n",
      "loss: 0.055316  [ 1100/ 1536]\n",
      "loss: 0.001744  [ 1200/ 1536]\n",
      "loss: 0.005375  [ 1300/ 1536]\n",
      "loss: 0.008970  [ 1400/ 1536]\n",
      "loss: 0.017146  [ 1500/ 1536]\n",
      "loss: 0.049440  [    0/ 1536]\n",
      "loss: 0.030459  [  100/ 1536]\n",
      "loss: 0.002185  [  200/ 1536]\n",
      "loss: 0.135740  [  300/ 1536]\n",
      "loss: 0.017987  [  400/ 1536]\n",
      "loss: 0.004674  [  500/ 1536]\n",
      "loss: 0.334462  [  600/ 1536]\n",
      "loss: 0.036430  [  700/ 1536]\n",
      "loss: 0.076961  [  800/ 1536]\n",
      "loss: 0.001077  [  900/ 1536]\n",
      "loss: 0.072508  [ 1000/ 1536]\n",
      "loss: 0.028713  [ 1100/ 1536]\n",
      "loss: 0.006219  [ 1200/ 1536]\n",
      "loss: 0.006931  [ 1300/ 1536]\n",
      "loss: 0.218675  [ 1400/ 1536]\n",
      "loss: 0.005791  [ 1500/ 1536]\n",
      "loss: 0.011143  [    0/ 1536]\n",
      "loss: 0.076536  [  100/ 1536]\n",
      "loss: 0.053910  [  200/ 1536]\n",
      "loss: 0.012372  [  300/ 1536]\n",
      "loss: 0.027469  [  400/ 1536]\n",
      "loss: 0.015483  [  500/ 1536]\n",
      "loss: 0.030898  [  600/ 1536]\n",
      "loss: 0.025761  [  700/ 1536]\n",
      "loss: 0.034225  [  800/ 1536]\n",
      "loss: 0.098600  [  900/ 1536]\n",
      "loss: 0.029185  [ 1000/ 1536]\n",
      "loss: 0.059295  [ 1100/ 1536]\n",
      "loss: 0.060123  [ 1200/ 1536]\n",
      "loss: 0.007450  [ 1300/ 1536]\n",
      "loss: 0.014475  [ 1400/ 1536]\n",
      "loss: 0.000460  [ 1500/ 1536]\n",
      "loss: 0.075426  [    0/ 1536]\n",
      "loss: 0.017164  [  100/ 1536]\n",
      "loss: 0.000008  [  200/ 1536]\n",
      "loss: 0.091936  [  300/ 1536]\n",
      "loss: 0.017541  [  400/ 1536]\n",
      "loss: 0.081486  [  500/ 1536]\n",
      "loss: 0.030885  [  600/ 1536]\n",
      "loss: 0.028861  [  700/ 1536]\n",
      "loss: 0.004197  [  800/ 1536]\n",
      "loss: 0.036623  [  900/ 1536]\n",
      "loss: 0.007222  [ 1000/ 1536]\n",
      "loss: 0.003042  [ 1100/ 1536]\n",
      "loss: 0.041857  [ 1200/ 1536]\n",
      "loss: 0.006607  [ 1300/ 1536]\n",
      "loss: 0.001846  [ 1400/ 1536]\n",
      "loss: 0.026112  [ 1500/ 1536]\n",
      "loss: 0.021827  [    0/ 1536]\n",
      "loss: 0.014223  [  100/ 1536]\n",
      "loss: 0.003962  [  200/ 1536]\n",
      "loss: 0.000168  [  300/ 1536]\n",
      "loss: 0.008371  [  400/ 1536]\n",
      "loss: 0.010959  [  500/ 1536]\n",
      "loss: 0.041179  [  600/ 1536]\n",
      "loss: 0.001509  [  700/ 1536]\n",
      "loss: 0.023155  [  800/ 1536]\n",
      "loss: 0.000109  [  900/ 1536]\n",
      "loss: 0.027206  [ 1000/ 1536]\n",
      "loss: 0.000000  [ 1100/ 1536]\n",
      "loss: 0.033363  [ 1200/ 1536]\n",
      "loss: 0.018522  [ 1300/ 1536]\n",
      "loss: 0.060279  [ 1400/ 1536]\n",
      "loss: 0.005511  [ 1500/ 1536]\n",
      "loss: 0.188641  [    0/ 1536]\n",
      "loss: 0.122419  [  100/ 1536]\n",
      "loss: 0.127392  [  200/ 1536]\n",
      "loss: 0.000517  [  300/ 1536]\n",
      "loss: 0.000235  [  400/ 1536]\n",
      "loss: 0.010747  [  500/ 1536]\n",
      "loss: 0.031301  [  600/ 1536]\n",
      "loss: 0.034299  [  700/ 1536]\n",
      "loss: 0.005840  [  800/ 1536]\n",
      "loss: 0.026554  [  900/ 1536]\n",
      "loss: 0.004509  [ 1000/ 1536]\n",
      "loss: 0.272311  [ 1100/ 1536]\n",
      "loss: 0.049062  [ 1200/ 1536]\n",
      "loss: 0.002003  [ 1300/ 1536]\n",
      "loss: 0.013648  [ 1400/ 1536]\n",
      "loss: 0.100691  [ 1500/ 1536]\n",
      "loss: 0.059134  [    0/ 1536]\n",
      "loss: 0.008911  [  100/ 1536]\n",
      "loss: 0.017961  [  200/ 1536]\n",
      "loss: 0.054774  [  300/ 1536]\n",
      "loss: 0.053653  [  400/ 1536]\n",
      "loss: 0.018399  [  500/ 1536]\n",
      "loss: 0.048263  [  600/ 1536]\n",
      "loss: 0.050676  [  700/ 1536]\n",
      "loss: 0.001387  [  800/ 1536]\n",
      "loss: 0.043190  [  900/ 1536]\n",
      "loss: 0.005099  [ 1000/ 1536]\n",
      "loss: 0.002644  [ 1100/ 1536]\n",
      "loss: 0.004649  [ 1200/ 1536]\n",
      "loss: 0.018945  [ 1300/ 1536]\n",
      "loss: 0.030894  [ 1400/ 1536]\n",
      "loss: 0.096589  [ 1500/ 1536]\n",
      "loss: 0.005136  [    0/ 1536]\n",
      "loss: 0.000724  [  100/ 1536]\n",
      "loss: 0.057737  [  200/ 1536]\n",
      "loss: 0.001905  [  300/ 1536]\n",
      "loss: 0.000018  [  400/ 1536]\n",
      "loss: 0.000026  [  500/ 1536]\n",
      "loss: 0.061964  [  600/ 1536]\n",
      "loss: 0.000048  [  700/ 1536]\n",
      "loss: 0.162848  [  800/ 1536]\n",
      "loss: 0.004265  [  900/ 1536]\n",
      "loss: 0.009275  [ 1000/ 1536]\n",
      "loss: 0.009366  [ 1100/ 1536]\n",
      "loss: 0.021514  [ 1200/ 1536]\n",
      "loss: 0.103159  [ 1300/ 1536]\n",
      "loss: 0.003034  [ 1400/ 1536]\n",
      "loss: 0.001375  [ 1500/ 1536]\n",
      "loss: 0.012064  [    0/ 1536]\n",
      "loss: 0.050411  [  100/ 1536]\n",
      "loss: 0.000100  [  200/ 1536]\n",
      "loss: 0.015032  [  300/ 1536]\n",
      "loss: 0.075806  [  400/ 1536]\n",
      "loss: 0.007144  [  500/ 1536]\n",
      "loss: 0.000327  [  600/ 1536]\n",
      "loss: 0.216630  [  700/ 1536]\n",
      "loss: 0.000582  [  800/ 1536]\n",
      "loss: 0.017856  [  900/ 1536]\n",
      "loss: 0.005456  [ 1000/ 1536]\n",
      "loss: 0.003366  [ 1100/ 1536]\n",
      "loss: 0.056924  [ 1200/ 1536]\n",
      "loss: 0.105962  [ 1300/ 1536]\n",
      "loss: 0.003142  [ 1400/ 1536]\n",
      "loss: 0.080614  [ 1500/ 1536]\n",
      "loss: 0.003933  [    0/ 1536]\n",
      "loss: 0.100354  [  100/ 1536]\n",
      "loss: 0.000555  [  200/ 1536]\n",
      "loss: 0.019797  [  300/ 1536]\n",
      "loss: 0.033369  [  400/ 1536]\n",
      "loss: 0.000316  [  500/ 1536]\n",
      "loss: 0.038195  [  600/ 1536]\n",
      "loss: 0.145173  [  700/ 1536]\n",
      "loss: 0.047365  [  800/ 1536]\n",
      "loss: 0.120383  [  900/ 1536]\n",
      "loss: 0.046388  [ 1000/ 1536]\n",
      "loss: 0.001363  [ 1100/ 1536]\n",
      "loss: 0.068977  [ 1200/ 1536]\n",
      "loss: 0.000812  [ 1300/ 1536]\n",
      "loss: 0.000907  [ 1400/ 1536]\n",
      "loss: 0.004363  [ 1500/ 1536]\n",
      "loss: 0.005387  [    0/ 1536]\n",
      "loss: 0.000572  [  100/ 1536]\n",
      "loss: 0.000830  [  200/ 1536]\n",
      "loss: 0.020980  [  300/ 1536]\n",
      "loss: 0.040631  [  400/ 1536]\n",
      "loss: 0.005156  [  500/ 1536]\n",
      "loss: 0.037011  [  600/ 1536]\n",
      "loss: 0.021536  [  700/ 1536]\n",
      "loss: 0.042916  [  800/ 1536]\n",
      "loss: 0.000683  [  900/ 1536]\n",
      "loss: 0.012280  [ 1000/ 1536]\n",
      "loss: 0.003318  [ 1100/ 1536]\n",
      "loss: 0.010314  [ 1200/ 1536]\n",
      "loss: 0.110083  [ 1300/ 1536]\n",
      "loss: 0.156357  [ 1400/ 1536]\n",
      "loss: 0.039999  [ 1500/ 1536]\n",
      "loss: 0.034065  [    0/ 1536]\n",
      "loss: 0.029642  [  100/ 1536]\n",
      "loss: 0.000410  [  200/ 1536]\n",
      "loss: 0.013476  [  300/ 1536]\n",
      "loss: 0.026943  [  400/ 1536]\n",
      "loss: 0.094796  [  500/ 1536]\n",
      "loss: 0.008497  [  600/ 1536]\n",
      "loss: 0.000001  [  700/ 1536]\n",
      "loss: 0.025729  [  800/ 1536]\n",
      "loss: 0.029722  [  900/ 1536]\n",
      "loss: 0.001259  [ 1000/ 1536]\n",
      "loss: 0.017258  [ 1100/ 1536]\n",
      "loss: 0.002166  [ 1200/ 1536]\n",
      "loss: 0.000010  [ 1300/ 1536]\n",
      "loss: 0.028999  [ 1400/ 1536]\n",
      "loss: 0.065619  [ 1500/ 1536]\n",
      "loss: 0.001059  [    0/ 1536]\n",
      "loss: 0.075479  [  100/ 1536]\n",
      "loss: 0.037475  [  200/ 1536]\n",
      "loss: 0.068810  [  300/ 1536]\n",
      "loss: 0.005424  [  400/ 1536]\n",
      "loss: 0.019960  [  500/ 1536]\n",
      "loss: 0.000000  [  600/ 1536]\n",
      "loss: 0.139037  [  700/ 1536]\n",
      "loss: 0.075978  [  800/ 1536]\n",
      "loss: 0.036395  [  900/ 1536]\n",
      "loss: 0.063393  [ 1000/ 1536]\n",
      "loss: 0.007153  [ 1100/ 1536]\n",
      "loss: 0.004348  [ 1200/ 1536]\n",
      "loss: 0.145263  [ 1300/ 1536]\n",
      "loss: 0.041659  [ 1400/ 1536]\n",
      "loss: 0.006792  [ 1500/ 1536]\n",
      "loss: 0.004930  [    0/ 1536]\n",
      "loss: 0.000774  [  100/ 1536]\n",
      "loss: 0.029282  [  200/ 1536]\n",
      "loss: 0.077312  [  300/ 1536]\n",
      "loss: 0.000381  [  400/ 1536]\n",
      "loss: 0.115196  [  500/ 1536]\n",
      "loss: 0.013080  [  600/ 1536]\n",
      "loss: 0.125298  [  700/ 1536]\n",
      "loss: 0.024781  [  800/ 1536]\n",
      "loss: 0.008955  [  900/ 1536]\n",
      "loss: 0.027407  [ 1000/ 1536]\n",
      "loss: 0.001938  [ 1100/ 1536]\n",
      "loss: 0.061519  [ 1200/ 1536]\n",
      "loss: 0.120614  [ 1300/ 1536]\n",
      "loss: 0.046314  [ 1400/ 1536]\n",
      "loss: 0.013386  [ 1500/ 1536]\n",
      "loss: 0.012948  [    0/ 1536]\n",
      "loss: 0.035585  [  100/ 1536]\n",
      "loss: 0.011282  [  200/ 1536]\n",
      "loss: 0.007519  [  300/ 1536]\n",
      "loss: 0.400287  [  400/ 1536]\n",
      "loss: 0.026380  [  500/ 1536]\n",
      "loss: 0.023743  [  600/ 1536]\n",
      "loss: 0.015128  [  700/ 1536]\n",
      "loss: 0.015335  [  800/ 1536]\n",
      "loss: 0.000623  [  900/ 1536]\n",
      "loss: 0.137908  [ 1000/ 1536]\n",
      "loss: 0.431148  [ 1100/ 1536]\n",
      "loss: 0.212254  [ 1200/ 1536]\n",
      "loss: 0.139210  [ 1300/ 1536]\n",
      "loss: 0.205656  [ 1400/ 1536]\n",
      "loss: 0.000044  [ 1500/ 1536]\n",
      "loss: 0.233913  [    0/ 1536]\n",
      "loss: 0.145852  [  100/ 1536]\n",
      "loss: 0.032616  [  200/ 1536]\n",
      "loss: 0.048800  [  300/ 1536]\n",
      "loss: 0.148391  [  400/ 1536]\n",
      "loss: 0.008534  [  500/ 1536]\n",
      "loss: 0.015292  [  600/ 1536]\n",
      "loss: 0.000385  [  700/ 1536]\n",
      "loss: 0.012258  [  800/ 1536]\n",
      "loss: 0.000538  [  900/ 1536]\n",
      "loss: 0.005989  [ 1000/ 1536]\n",
      "loss: 0.001206  [ 1100/ 1536]\n",
      "loss: 0.090617  [ 1200/ 1536]\n",
      "loss: 0.006250  [ 1300/ 1536]\n",
      "loss: 0.034270  [ 1400/ 1536]\n",
      "loss: 0.065013  [ 1500/ 1536]\n",
      "loss: 0.074661  [    0/ 1536]\n",
      "loss: 0.215045  [  100/ 1536]\n",
      "loss: 0.002159  [  200/ 1536]\n",
      "loss: 0.014607  [  300/ 1536]\n",
      "loss: 0.000128  [  400/ 1536]\n",
      "loss: 0.009885  [  500/ 1536]\n",
      "loss: 0.001820  [  600/ 1536]\n",
      "loss: 0.046557  [  700/ 1536]\n",
      "loss: 0.141195  [  800/ 1536]\n",
      "loss: 0.000919  [  900/ 1536]\n",
      "loss: 0.000016  [ 1000/ 1536]\n",
      "loss: 0.010484  [ 1100/ 1536]\n",
      "loss: 0.001023  [ 1200/ 1536]\n",
      "loss: 0.024427  [ 1300/ 1536]\n",
      "loss: 0.068522  [ 1400/ 1536]\n",
      "loss: 0.004361  [ 1500/ 1536]\n",
      "loss: 0.001869  [    0/ 1536]\n",
      "loss: 0.002565  [  100/ 1536]\n",
      "loss: 0.001098  [  200/ 1536]\n",
      "loss: 0.007832  [  300/ 1536]\n",
      "loss: 0.052145  [  400/ 1536]\n",
      "loss: 0.153718  [  500/ 1536]\n",
      "loss: 0.124442  [  600/ 1536]\n",
      "loss: 0.000184  [  700/ 1536]\n",
      "loss: 0.004996  [  800/ 1536]\n",
      "loss: 0.049323  [  900/ 1536]\n",
      "loss: 0.039841  [ 1000/ 1536]\n",
      "loss: 0.011052  [ 1100/ 1536]\n",
      "loss: 0.117792  [ 1200/ 1536]\n",
      "loss: 0.000135  [ 1300/ 1536]\n",
      "loss: 0.057935  [ 1400/ 1536]\n",
      "loss: 0.065392  [ 1500/ 1536]\n",
      "loss: 0.004055  [    0/ 1536]\n",
      "loss: 0.051762  [  100/ 1536]\n",
      "loss: 0.052495  [  200/ 1536]\n",
      "loss: 0.002046  [  300/ 1536]\n",
      "loss: 0.109506  [  400/ 1536]\n",
      "loss: 0.089674  [  500/ 1536]\n",
      "loss: 0.013900  [  600/ 1536]\n",
      "loss: 0.017435  [  700/ 1536]\n",
      "loss: 0.109098  [  800/ 1536]\n",
      "loss: 0.026102  [  900/ 1536]\n",
      "loss: 0.066619  [ 1000/ 1536]\n",
      "loss: 0.105524  [ 1100/ 1536]\n",
      "loss: 0.027498  [ 1200/ 1536]\n",
      "loss: 0.000000  [ 1300/ 1536]\n",
      "loss: 0.000200  [ 1400/ 1536]\n",
      "loss: 0.006895  [ 1500/ 1536]\n",
      "loss: 0.006552  [    0/ 1536]\n",
      "loss: 0.014200  [  100/ 1536]\n",
      "loss: 0.107767  [  200/ 1536]\n",
      "loss: 0.031295  [  300/ 1536]\n",
      "loss: 0.080809  [  400/ 1536]\n",
      "loss: 0.018124  [  500/ 1536]\n",
      "loss: 0.017914  [  600/ 1536]\n",
      "loss: 0.004097  [  700/ 1536]\n",
      "loss: 0.000001  [  800/ 1536]\n",
      "loss: 0.001426  [  900/ 1536]\n",
      "loss: 0.004733  [ 1000/ 1536]\n",
      "loss: 0.070017  [ 1100/ 1536]\n",
      "loss: 0.041327  [ 1200/ 1536]\n",
      "loss: 0.005800  [ 1300/ 1536]\n",
      "loss: 0.003073  [ 1400/ 1536]\n",
      "loss: 0.008109  [ 1500/ 1536]\n",
      "loss: 0.005616  [    0/ 1536]\n",
      "loss: 0.008492  [  100/ 1536]\n",
      "loss: 0.004066  [  200/ 1536]\n",
      "loss: 0.037959  [  300/ 1536]\n",
      "loss: 0.032371  [  400/ 1536]\n",
      "loss: 0.039585  [  500/ 1536]\n",
      "loss: 0.005461  [  600/ 1536]\n",
      "loss: 0.000040  [  700/ 1536]\n",
      "loss: 0.005340  [  800/ 1536]\n",
      "loss: 0.055021  [  900/ 1536]\n",
      "loss: 0.000722  [ 1000/ 1536]\n",
      "loss: 0.049128  [ 1100/ 1536]\n",
      "loss: 0.104920  [ 1200/ 1536]\n",
      "loss: 0.010119  [ 1300/ 1536]\n",
      "loss: 0.002498  [ 1400/ 1536]\n",
      "loss: 0.007456  [ 1500/ 1536]\n",
      "loss: 0.006483  [    0/ 1536]\n",
      "loss: 0.005410  [  100/ 1536]\n",
      "loss: 0.023529  [  200/ 1536]\n",
      "loss: 0.006089  [  300/ 1536]\n",
      "loss: 0.098623  [  400/ 1536]\n",
      "loss: 0.035347  [  500/ 1536]\n",
      "loss: 0.000774  [  600/ 1536]\n",
      "loss: 0.018554  [  700/ 1536]\n",
      "loss: 0.000417  [  800/ 1536]\n",
      "loss: 0.070761  [  900/ 1536]\n",
      "loss: 0.012853  [ 1000/ 1536]\n",
      "loss: 0.000329  [ 1100/ 1536]\n",
      "loss: 0.006692  [ 1200/ 1536]\n",
      "loss: 0.009240  [ 1300/ 1536]\n",
      "loss: 0.075556  [ 1400/ 1536]\n",
      "loss: 0.030536  [ 1500/ 1536]\n",
      "loss: 0.000933  [    0/ 1536]\n",
      "loss: 0.003459  [  100/ 1536]\n",
      "loss: 0.010623  [  200/ 1536]\n",
      "loss: 0.003127  [  300/ 1536]\n",
      "loss: 0.000411  [  400/ 1536]\n",
      "loss: 0.004249  [  500/ 1536]\n",
      "loss: 0.000341  [  600/ 1536]\n",
      "loss: 0.032469  [  700/ 1536]\n",
      "loss: 0.285129  [  800/ 1536]\n",
      "loss: 0.029242  [  900/ 1536]\n",
      "loss: 0.037636  [ 1000/ 1536]\n",
      "loss: 0.003004  [ 1100/ 1536]\n",
      "loss: 0.170523  [ 1200/ 1536]\n",
      "loss: 0.001393  [ 1300/ 1536]\n",
      "loss: 0.014814  [ 1400/ 1536]\n",
      "loss: 0.000545  [ 1500/ 1536]\n",
      "loss: 0.005270  [    0/ 1536]\n",
      "loss: 0.090847  [  100/ 1536]\n",
      "loss: 0.020672  [  200/ 1536]\n",
      "loss: 0.016811  [  300/ 1536]\n",
      "loss: 0.005150  [  400/ 1536]\n",
      "loss: 0.095617  [  500/ 1536]\n",
      "loss: 0.000978  [  600/ 1536]\n",
      "loss: 0.062856  [  700/ 1536]\n",
      "loss: 0.072430  [  800/ 1536]\n",
      "loss: 0.001068  [  900/ 1536]\n",
      "loss: 0.024452  [ 1000/ 1536]\n",
      "loss: 0.147981  [ 1100/ 1536]\n",
      "loss: 0.000002  [ 1200/ 1536]\n",
      "loss: 0.023576  [ 1300/ 1536]\n",
      "loss: 0.055820  [ 1400/ 1536]\n",
      "loss: 0.010500  [ 1500/ 1536]\n",
      "loss: 0.066647  [    0/ 1536]\n",
      "loss: 0.028837  [  100/ 1536]\n",
      "loss: 0.050451  [  200/ 1536]\n",
      "loss: 0.043035  [  300/ 1536]\n",
      "loss: 0.000478  [  400/ 1536]\n",
      "loss: 0.008714  [  500/ 1536]\n",
      "loss: 0.002812  [  600/ 1536]\n",
      "loss: 0.024898  [  700/ 1536]\n",
      "loss: 0.048539  [  800/ 1536]\n",
      "loss: 0.001326  [  900/ 1536]\n",
      "loss: 0.032964  [ 1000/ 1536]\n",
      "loss: 0.011666  [ 1100/ 1536]\n",
      "loss: 0.118403  [ 1200/ 1536]\n",
      "loss: 0.036436  [ 1300/ 1536]\n",
      "loss: 0.023377  [ 1400/ 1536]\n",
      "loss: 0.002007  [ 1500/ 1536]\n",
      "loss: 0.000047  [    0/ 1536]\n",
      "loss: 0.004925  [  100/ 1536]\n",
      "loss: 0.010612  [  200/ 1536]\n",
      "loss: 0.000690  [  300/ 1536]\n",
      "loss: 0.005760  [  400/ 1536]\n",
      "loss: 0.072025  [  500/ 1536]\n",
      "loss: 0.087915  [  600/ 1536]\n",
      "loss: 0.002813  [  700/ 1536]\n",
      "loss: 0.025752  [  800/ 1536]\n",
      "loss: 0.005446  [  900/ 1536]\n",
      "loss: 0.002084  [ 1000/ 1536]\n",
      "loss: 0.008697  [ 1100/ 1536]\n",
      "loss: 0.017146  [ 1200/ 1536]\n",
      "loss: 0.031491  [ 1300/ 1536]\n",
      "loss: 0.071654  [ 1400/ 1536]\n",
      "loss: 0.080618  [ 1500/ 1536]\n",
      "loss: 0.033739  [    0/ 1536]\n",
      "loss: 0.138757  [  100/ 1536]\n",
      "loss: 0.000000  [  200/ 1536]\n",
      "loss: 0.105898  [  300/ 1536]\n",
      "loss: 0.026172  [  400/ 1536]\n",
      "loss: 0.638884  [  500/ 1536]\n",
      "loss: 0.027150  [  600/ 1536]\n",
      "loss: 0.012386  [  700/ 1536]\n",
      "loss: 0.172400  [  800/ 1536]\n",
      "loss: 0.014682  [  900/ 1536]\n",
      "loss: 0.006506  [ 1000/ 1536]\n",
      "loss: 0.015281  [ 1100/ 1536]\n",
      "loss: 0.002820  [ 1200/ 1536]\n",
      "loss: 0.011592  [ 1300/ 1536]\n",
      "loss: 0.004714  [ 1400/ 1536]\n",
      "loss: 0.140737  [ 1500/ 1536]\n",
      "loss: 0.072146  [    0/ 1536]\n",
      "loss: 0.131080  [  100/ 1536]\n",
      "loss: 0.026273  [  200/ 1536]\n",
      "loss: 0.006496  [  300/ 1536]\n",
      "loss: 0.011945  [  400/ 1536]\n",
      "loss: 0.023010  [  500/ 1536]\n",
      "loss: 0.085635  [  600/ 1536]\n",
      "loss: 0.003431  [  700/ 1536]\n",
      "loss: 0.001777  [  800/ 1536]\n",
      "loss: 0.200502  [  900/ 1536]\n",
      "loss: 0.007852  [ 1000/ 1536]\n",
      "loss: 0.068718  [ 1100/ 1536]\n",
      "loss: 0.011897  [ 1200/ 1536]\n",
      "loss: 0.000024  [ 1300/ 1536]\n",
      "loss: 0.045067  [ 1400/ 1536]\n",
      "loss: 0.001410  [ 1500/ 1536]\n",
      "loss: 0.024477  [    0/ 1536]\n",
      "loss: 0.042994  [  100/ 1536]\n",
      "loss: 0.028304  [  200/ 1536]\n",
      "loss: 0.033422  [  300/ 1536]\n",
      "loss: 0.036639  [  400/ 1536]\n",
      "loss: 0.012205  [  500/ 1536]\n",
      "loss: 0.037247  [  600/ 1536]\n",
      "loss: 0.006435  [  700/ 1536]\n",
      "loss: 0.021441  [  800/ 1536]\n",
      "loss: 0.016301  [  900/ 1536]\n",
      "loss: 0.004004  [ 1000/ 1536]\n",
      "loss: 0.018216  [ 1100/ 1536]\n",
      "loss: 0.106563  [ 1200/ 1536]\n",
      "loss: 0.045645  [ 1300/ 1536]\n",
      "loss: 0.041107  [ 1400/ 1536]\n",
      "loss: 0.130990  [ 1500/ 1536]\n",
      "loss: 0.358607  [    0/ 1536]\n",
      "loss: 0.188302  [  100/ 1536]\n",
      "loss: 0.115566  [  200/ 1536]\n",
      "loss: 0.002168  [  300/ 1536]\n",
      "loss: 0.000911  [  400/ 1536]\n",
      "loss: 0.016791  [  500/ 1536]\n",
      "loss: 0.067849  [  600/ 1536]\n",
      "loss: 0.115980  [  700/ 1536]\n",
      "loss: 0.018088  [  800/ 1536]\n",
      "loss: 0.000485  [  900/ 1536]\n",
      "loss: 0.008108  [ 1000/ 1536]\n",
      "loss: 0.170504  [ 1100/ 1536]\n",
      "loss: 0.000559  [ 1200/ 1536]\n",
      "loss: 0.068330  [ 1300/ 1536]\n",
      "loss: 0.022059  [ 1400/ 1536]\n",
      "loss: 0.008137  [ 1500/ 1536]\n",
      "loss: 0.002173  [    0/ 1536]\n",
      "loss: 0.038455  [  100/ 1536]\n",
      "loss: 0.008694  [  200/ 1536]\n",
      "loss: 0.186494  [  300/ 1536]\n",
      "loss: 0.004936  [  400/ 1536]\n",
      "loss: 0.005407  [  500/ 1536]\n",
      "loss: 0.039762  [  600/ 1536]\n",
      "loss: 0.041264  [  700/ 1536]\n",
      "loss: 0.008788  [  800/ 1536]\n",
      "loss: 0.009403  [  900/ 1536]\n",
      "loss: 0.000040  [ 1000/ 1536]\n",
      "loss: 0.014636  [ 1100/ 1536]\n",
      "loss: 0.018532  [ 1200/ 1536]\n",
      "loss: 0.097482  [ 1300/ 1536]\n",
      "loss: 0.070463  [ 1400/ 1536]\n",
      "loss: 0.000909  [ 1500/ 1536]\n",
      "loss: 0.002823  [    0/ 1536]\n",
      "loss: 0.003465  [  100/ 1536]\n",
      "loss: 0.042459  [  200/ 1536]\n",
      "loss: 0.013709  [  300/ 1536]\n",
      "loss: 0.020723  [  400/ 1536]\n",
      "loss: 0.000003  [  500/ 1536]\n",
      "loss: 0.004740  [  600/ 1536]\n",
      "loss: 0.005470  [  700/ 1536]\n",
      "loss: 0.051981  [  800/ 1536]\n",
      "loss: 0.000404  [  900/ 1536]\n",
      "loss: 0.008624  [ 1000/ 1536]\n",
      "loss: 0.002580  [ 1100/ 1536]\n",
      "loss: 0.009481  [ 1200/ 1536]\n",
      "loss: 0.004058  [ 1300/ 1536]\n",
      "loss: 0.018242  [ 1400/ 1536]\n",
      "loss: 0.003249  [ 1500/ 1536]\n",
      "loss: 0.064131  [    0/ 1536]\n",
      "loss: 0.001586  [  100/ 1536]\n",
      "loss: 0.022047  [  200/ 1536]\n",
      "loss: 0.031156  [  300/ 1536]\n",
      "loss: 0.020606  [  400/ 1536]\n",
      "loss: 0.014150  [  500/ 1536]\n",
      "loss: 0.003442  [  600/ 1536]\n",
      "loss: 0.060087  [  700/ 1536]\n",
      "loss: 0.044749  [  800/ 1536]\n",
      "loss: 0.030296  [  900/ 1536]\n",
      "loss: 0.035490  [ 1000/ 1536]\n",
      "loss: 0.000824  [ 1100/ 1536]\n",
      "loss: 0.019108  [ 1200/ 1536]\n",
      "loss: 0.038507  [ 1300/ 1536]\n",
      "loss: 0.006958  [ 1400/ 1536]\n",
      "loss: 0.028593  [ 1500/ 1536]\n",
      "loss: 0.034813  [    0/ 1536]\n",
      "loss: 0.000415  [  100/ 1536]\n",
      "loss: 0.082726  [  200/ 1536]\n",
      "loss: 0.011636  [  300/ 1536]\n",
      "loss: 0.011296  [  400/ 1536]\n",
      "loss: 0.024802  [  500/ 1536]\n",
      "loss: 0.040084  [  600/ 1536]\n",
      "loss: 0.010036  [  700/ 1536]\n",
      "loss: 0.010384  [  800/ 1536]\n",
      "loss: 0.170638  [  900/ 1536]\n",
      "loss: 0.010507  [ 1000/ 1536]\n",
      "loss: 0.000947  [ 1100/ 1536]\n",
      "loss: 0.014932  [ 1200/ 1536]\n",
      "loss: 0.028040  [ 1300/ 1536]\n",
      "loss: 0.213211  [ 1400/ 1536]\n",
      "loss: 0.004420  [ 1500/ 1536]\n",
      "loss: 0.010432  [    0/ 1536]\n",
      "loss: 0.014947  [  100/ 1536]\n",
      "loss: 0.129592  [  200/ 1536]\n",
      "loss: 0.003364  [  300/ 1536]\n",
      "loss: 0.078679  [  400/ 1536]\n",
      "loss: 0.004457  [  500/ 1536]\n",
      "loss: 0.226542  [  600/ 1536]\n",
      "loss: 0.001078  [  700/ 1536]\n",
      "loss: 0.037562  [  800/ 1536]\n",
      "loss: 0.012078  [  900/ 1536]\n",
      "loss: 0.258609  [ 1000/ 1536]\n",
      "loss: 0.000062  [ 1100/ 1536]\n",
      "loss: 0.049316  [ 1200/ 1536]\n",
      "loss: 0.000065  [ 1300/ 1536]\n",
      "loss: 0.016089  [ 1400/ 1536]\n",
      "loss: 0.005549  [ 1500/ 1536]\n",
      "loss: 0.000391  [    0/ 1536]\n",
      "loss: 0.005357  [  100/ 1536]\n",
      "loss: 0.022056  [  200/ 1536]\n",
      "loss: 0.136488  [  300/ 1536]\n",
      "loss: 0.000989  [  400/ 1536]\n",
      "loss: 0.011667  [  500/ 1536]\n",
      "loss: 0.009547  [  600/ 1536]\n",
      "loss: 0.041925  [  700/ 1536]\n",
      "loss: 0.036796  [  800/ 1536]\n",
      "loss: 0.002264  [  900/ 1536]\n",
      "loss: 0.001838  [ 1000/ 1536]\n",
      "loss: 0.046619  [ 1100/ 1536]\n",
      "loss: 0.026524  [ 1200/ 1536]\n",
      "loss: 0.003626  [ 1300/ 1536]\n",
      "loss: 0.005168  [ 1400/ 1536]\n",
      "loss: 0.008540  [ 1500/ 1536]\n",
      "loss: 0.118225  [    0/ 1536]\n",
      "loss: 0.011560  [  100/ 1536]\n",
      "loss: 0.039593  [  200/ 1536]\n",
      "loss: 0.226503  [  300/ 1536]\n",
      "loss: 0.005997  [  400/ 1536]\n",
      "loss: 0.175035  [  500/ 1536]\n",
      "loss: 0.016608  [  600/ 1536]\n",
      "loss: 0.062820  [  700/ 1536]\n",
      "loss: 0.040953  [  800/ 1536]\n",
      "loss: 0.005825  [  900/ 1536]\n",
      "loss: 0.122351  [ 1000/ 1536]\n",
      "loss: 0.000200  [ 1100/ 1536]\n",
      "loss: 0.003040  [ 1200/ 1536]\n",
      "loss: 0.128041  [ 1300/ 1536]\n",
      "loss: 0.157979  [ 1400/ 1536]\n",
      "loss: 0.068409  [ 1500/ 1536]\n",
      "loss: 0.034621  [    0/ 1536]\n",
      "loss: 0.027088  [  100/ 1536]\n",
      "loss: 0.088207  [  200/ 1536]\n",
      "loss: 0.090398  [  300/ 1536]\n",
      "loss: 0.187716  [  400/ 1536]\n",
      "loss: 0.094122  [  500/ 1536]\n",
      "loss: 0.226827  [  600/ 1536]\n",
      "loss: 0.090628  [  700/ 1536]\n",
      "loss: 0.000009  [  800/ 1536]\n",
      "loss: 0.001538  [  900/ 1536]\n",
      "loss: 0.013307  [ 1000/ 1536]\n",
      "loss: 0.020553  [ 1100/ 1536]\n",
      "loss: 0.003488  [ 1200/ 1536]\n",
      "loss: 0.016462  [ 1300/ 1536]\n",
      "loss: 0.044286  [ 1400/ 1536]\n",
      "loss: 0.119904  [ 1500/ 1536]\n",
      "loss: 0.022787  [    0/ 1536]\n",
      "loss: 0.001936  [  100/ 1536]\n",
      "loss: 0.004672  [  200/ 1536]\n",
      "loss: 0.028991  [  300/ 1536]\n",
      "loss: 0.012886  [  400/ 1536]\n",
      "loss: 0.020211  [  500/ 1536]\n",
      "loss: 0.016513  [  600/ 1536]\n",
      "loss: 0.002556  [  700/ 1536]\n",
      "loss: 0.014708  [  800/ 1536]\n",
      "loss: 0.139860  [  900/ 1536]\n",
      "loss: 0.006234  [ 1000/ 1536]\n",
      "loss: 0.000348  [ 1100/ 1536]\n",
      "loss: 0.000495  [ 1200/ 1536]\n",
      "loss: 0.003922  [ 1300/ 1536]\n",
      "loss: 0.005590  [ 1400/ 1536]\n",
      "loss: 0.157967  [ 1500/ 1536]\n",
      "loss: 0.001401  [    0/ 1536]\n",
      "loss: 0.004309  [  100/ 1536]\n",
      "loss: 0.000026  [  200/ 1536]\n",
      "loss: 0.080437  [  300/ 1536]\n",
      "loss: 0.095742  [  400/ 1536]\n",
      "loss: 0.048050  [  500/ 1536]\n",
      "loss: 0.039755  [  600/ 1536]\n",
      "loss: 0.003019  [  700/ 1536]\n",
      "loss: 0.009132  [  800/ 1536]\n",
      "loss: 0.076616  [  900/ 1536]\n",
      "loss: 0.127855  [ 1000/ 1536]\n",
      "loss: 0.063558  [ 1100/ 1536]\n",
      "loss: 0.000261  [ 1200/ 1536]\n",
      "loss: 0.028998  [ 1300/ 1536]\n",
      "loss: 0.006250  [ 1400/ 1536]\n",
      "loss: 0.013858  [ 1500/ 1536]\n",
      "loss: 0.002967  [    0/ 1536]\n",
      "loss: 0.044109  [  100/ 1536]\n",
      "loss: 0.244925  [  200/ 1536]\n",
      "loss: 0.001657  [  300/ 1536]\n",
      "loss: 0.063250  [  400/ 1536]\n",
      "loss: 0.103760  [  500/ 1536]\n",
      "loss: 0.036715  [  600/ 1536]\n",
      "loss: 0.150962  [  700/ 1536]\n",
      "loss: 0.004314  [  800/ 1536]\n",
      "loss: 0.010718  [  900/ 1536]\n",
      "loss: 0.019900  [ 1000/ 1536]\n",
      "loss: 0.048646  [ 1100/ 1536]\n",
      "loss: 0.043697  [ 1200/ 1536]\n",
      "loss: 0.079491  [ 1300/ 1536]\n",
      "loss: 0.016807  [ 1400/ 1536]\n",
      "loss: 0.000319  [ 1500/ 1536]\n",
      "loss: 0.032320  [    0/ 1536]\n",
      "loss: 0.013978  [  100/ 1536]\n",
      "loss: 0.025333  [  200/ 1536]\n",
      "loss: 0.006290  [  300/ 1536]\n",
      "loss: 0.156212  [  400/ 1536]\n",
      "loss: 0.000043  [  500/ 1536]\n",
      "loss: 0.001028  [  600/ 1536]\n",
      "loss: 0.086237  [  700/ 1536]\n",
      "loss: 0.013457  [  800/ 1536]\n",
      "loss: 0.001157  [  900/ 1536]\n",
      "loss: 0.001056  [ 1000/ 1536]\n",
      "loss: 0.028650  [ 1100/ 1536]\n",
      "loss: 0.031667  [ 1200/ 1536]\n",
      "loss: 0.001632  [ 1300/ 1536]\n",
      "loss: 0.005142  [ 1400/ 1536]\n",
      "loss: 0.000429  [ 1500/ 1536]\n",
      "loss: 0.055001  [    0/ 1536]\n",
      "loss: 0.016112  [  100/ 1536]\n",
      "loss: 0.011408  [  200/ 1536]\n",
      "loss: 0.160611  [  300/ 1536]\n",
      "loss: 0.109881  [  400/ 1536]\n",
      "loss: 0.003194  [  500/ 1536]\n",
      "loss: 0.001597  [  600/ 1536]\n",
      "loss: 0.000627  [  700/ 1536]\n",
      "loss: 0.122434  [  800/ 1536]\n",
      "loss: 0.072631  [  900/ 1536]\n",
      "loss: 0.002361  [ 1000/ 1536]\n",
      "loss: 0.004375  [ 1100/ 1536]\n",
      "loss: 0.006774  [ 1200/ 1536]\n",
      "loss: 0.000615  [ 1300/ 1536]\n",
      "loss: 0.167577  [ 1400/ 1536]\n",
      "loss: 0.025727  [ 1500/ 1536]\n",
      "loss: 0.393468  [    0/ 1536]\n",
      "loss: 0.000197  [  100/ 1536]\n",
      "loss: 0.097165  [  200/ 1536]\n",
      "loss: 0.008430  [  300/ 1536]\n",
      "loss: 0.295039  [  400/ 1536]\n",
      "loss: 0.084202  [  500/ 1536]\n",
      "loss: 0.002527  [  600/ 1536]\n",
      "loss: 0.104010  [  700/ 1536]\n",
      "loss: 0.019990  [  800/ 1536]\n",
      "loss: 0.033581  [  900/ 1536]\n",
      "loss: 0.000093  [ 1000/ 1536]\n",
      "loss: 0.014117  [ 1100/ 1536]\n",
      "loss: 0.039516  [ 1200/ 1536]\n",
      "loss: 0.000069  [ 1300/ 1536]\n",
      "loss: 0.004573  [ 1400/ 1536]\n",
      "loss: 0.021487  [ 1500/ 1536]\n",
      "loss: 0.000470  [    0/ 1536]\n",
      "loss: 0.000837  [  100/ 1536]\n",
      "loss: 0.003710  [  200/ 1536]\n",
      "loss: 0.124967  [  300/ 1536]\n",
      "loss: 0.065293  [  400/ 1536]\n",
      "loss: 0.000432  [  500/ 1536]\n",
      "loss: 0.141183  [  600/ 1536]\n",
      "loss: 0.110964  [  700/ 1536]\n",
      "loss: 0.012612  [  800/ 1536]\n",
      "loss: 0.000009  [  900/ 1536]\n",
      "loss: 0.038212  [ 1000/ 1536]\n",
      "loss: 0.001007  [ 1100/ 1536]\n",
      "loss: 0.002454  [ 1200/ 1536]\n",
      "loss: 0.047946  [ 1300/ 1536]\n",
      "loss: 0.004406  [ 1400/ 1536]\n",
      "loss: 0.116048  [ 1500/ 1536]\n",
      "loss: 0.000163  [    0/ 1536]\n",
      "loss: 0.028920  [  100/ 1536]\n",
      "loss: 0.013445  [  200/ 1536]\n",
      "loss: 0.025441  [  300/ 1536]\n",
      "loss: 0.103221  [  400/ 1536]\n",
      "loss: 0.013850  [  500/ 1536]\n",
      "loss: 0.013737  [  600/ 1536]\n",
      "loss: 0.081070  [  700/ 1536]\n",
      "loss: 0.041015  [  800/ 1536]\n",
      "loss: 0.206919  [  900/ 1536]\n",
      "loss: 0.007497  [ 1000/ 1536]\n",
      "loss: 0.111417  [ 1100/ 1536]\n",
      "loss: 0.019026  [ 1200/ 1536]\n",
      "loss: 0.011786  [ 1300/ 1536]\n",
      "loss: 0.323911  [ 1400/ 1536]\n",
      "loss: 0.012403  [ 1500/ 1536]\n",
      "loss: 0.002296  [    0/ 1536]\n",
      "loss: 0.024041  [  100/ 1536]\n",
      "loss: 0.272640  [  200/ 1536]\n",
      "loss: 0.004535  [  300/ 1536]\n",
      "loss: 0.005991  [  400/ 1536]\n",
      "loss: 0.036067  [  500/ 1536]\n",
      "loss: 0.000024  [  600/ 1536]\n",
      "loss: 0.017254  [  700/ 1536]\n",
      "loss: 0.000106  [  800/ 1536]\n",
      "loss: 0.042497  [  900/ 1536]\n",
      "loss: 0.001988  [ 1000/ 1536]\n",
      "loss: 0.107729  [ 1100/ 1536]\n",
      "loss: 0.093232  [ 1200/ 1536]\n",
      "loss: 0.011457  [ 1300/ 1536]\n",
      "loss: 0.004159  [ 1400/ 1536]\n",
      "loss: 0.044107  [ 1500/ 1536]\n",
      "loss: 0.000037  [    0/ 1536]\n",
      "loss: 0.015638  [  100/ 1536]\n",
      "loss: 0.210303  [  200/ 1536]\n",
      "loss: 0.002029  [  300/ 1536]\n",
      "loss: 0.003448  [  400/ 1536]\n",
      "loss: 0.000882  [  500/ 1536]\n",
      "loss: 0.125671  [  600/ 1536]\n",
      "loss: 0.015880  [  700/ 1536]\n",
      "loss: 0.123370  [  800/ 1536]\n",
      "loss: 0.002265  [  900/ 1536]\n",
      "loss: 0.001316  [ 1000/ 1536]\n",
      "loss: 0.051289  [ 1100/ 1536]\n",
      "loss: 0.020175  [ 1200/ 1536]\n",
      "loss: 0.005371  [ 1300/ 1536]\n",
      "loss: 0.021598  [ 1400/ 1536]\n",
      "loss: 0.000110  [ 1500/ 1536]\n",
      "loss: 0.011374  [    0/ 1536]\n",
      "loss: 0.046891  [  100/ 1536]\n",
      "loss: 0.005039  [  200/ 1536]\n",
      "loss: 0.015481  [  300/ 1536]\n",
      "loss: 0.012855  [  400/ 1536]\n",
      "loss: 0.037382  [  500/ 1536]\n",
      "loss: 0.012844  [  600/ 1536]\n",
      "loss: 0.001241  [  700/ 1536]\n",
      "loss: 0.005477  [  800/ 1536]\n",
      "loss: 0.005209  [  900/ 1536]\n",
      "loss: 0.041351  [ 1000/ 1536]\n",
      "loss: 0.026051  [ 1100/ 1536]\n",
      "loss: 0.031989  [ 1200/ 1536]\n",
      "loss: 0.180165  [ 1300/ 1536]\n",
      "loss: 0.003788  [ 1400/ 1536]\n",
      "loss: 0.026101  [ 1500/ 1536]\n",
      "loss: 0.136968  [    0/ 1536]\n",
      "loss: 0.017867  [  100/ 1536]\n",
      "loss: 0.154951  [  200/ 1536]\n",
      "loss: 0.014629  [  300/ 1536]\n",
      "loss: 0.021570  [  400/ 1536]\n",
      "loss: 0.037680  [  500/ 1536]\n",
      "loss: 0.098042  [  600/ 1536]\n",
      "loss: 0.000333  [  700/ 1536]\n",
      "loss: 0.108448  [  800/ 1536]\n",
      "loss: 0.003105  [  900/ 1536]\n",
      "loss: 0.075635  [ 1000/ 1536]\n",
      "loss: 0.029472  [ 1100/ 1536]\n",
      "loss: 0.102397  [ 1200/ 1536]\n",
      "loss: 0.013412  [ 1300/ 1536]\n",
      "loss: 0.001616  [ 1400/ 1536]\n",
      "loss: 0.000494  [ 1500/ 1536]\n",
      "loss: 0.038674  [    0/ 1536]\n",
      "loss: 0.031448  [  100/ 1536]\n",
      "loss: 0.232882  [  200/ 1536]\n",
      "loss: 0.042101  [  300/ 1536]\n",
      "loss: 0.031377  [  400/ 1536]\n",
      "loss: 0.045395  [  500/ 1536]\n",
      "loss: 0.022813  [  600/ 1536]\n",
      "loss: 0.027617  [  700/ 1536]\n",
      "loss: 0.047219  [  800/ 1536]\n",
      "loss: 0.004164  [  900/ 1536]\n",
      "loss: 0.008356  [ 1000/ 1536]\n",
      "loss: 0.195261  [ 1100/ 1536]\n",
      "loss: 0.000631  [ 1200/ 1536]\n",
      "loss: 0.051540  [ 1300/ 1536]\n",
      "loss: 0.011439  [ 1400/ 1536]\n",
      "loss: 0.121086  [ 1500/ 1536]\n",
      "loss: 0.077694  [    0/ 1536]\n",
      "loss: 0.024950  [  100/ 1536]\n",
      "loss: 0.032691  [  200/ 1536]\n",
      "loss: 0.068925  [  300/ 1536]\n",
      "loss: 0.013700  [  400/ 1536]\n",
      "loss: 0.032568  [  500/ 1536]\n",
      "loss: 0.000005  [  600/ 1536]\n",
      "loss: 0.036467  [  700/ 1536]\n",
      "loss: 0.034817  [  800/ 1536]\n",
      "loss: 0.006974  [  900/ 1536]\n",
      "loss: 0.000017  [ 1000/ 1536]\n",
      "loss: 0.013077  [ 1100/ 1536]\n",
      "loss: 0.008689  [ 1200/ 1536]\n",
      "loss: 0.013965  [ 1300/ 1536]\n",
      "loss: 0.001276  [ 1400/ 1536]\n",
      "loss: 0.039689  [ 1500/ 1536]\n",
      "loss: 0.082496  [    0/ 1536]\n",
      "loss: 0.002608  [  100/ 1536]\n",
      "loss: 0.006969  [  200/ 1536]\n",
      "loss: 0.116980  [  300/ 1536]\n",
      "loss: 0.009238  [  400/ 1536]\n",
      "loss: 0.000976  [  500/ 1536]\n",
      "loss: 0.016791  [  600/ 1536]\n",
      "loss: 0.000112  [  700/ 1536]\n",
      "loss: 0.011117  [  800/ 1536]\n",
      "loss: 0.403456  [  900/ 1536]\n",
      "loss: 0.167290  [ 1000/ 1536]\n",
      "loss: 0.001448  [ 1100/ 1536]\n",
      "loss: 0.014523  [ 1200/ 1536]\n",
      "loss: 0.018597  [ 1300/ 1536]\n",
      "loss: 0.000014  [ 1400/ 1536]\n",
      "loss: 0.115058  [ 1500/ 1536]\n",
      "loss: 0.137012  [    0/ 1536]\n",
      "loss: 0.058012  [  100/ 1536]\n",
      "loss: 0.151982  [  200/ 1536]\n",
      "loss: 0.005984  [  300/ 1536]\n",
      "loss: 0.084482  [  400/ 1536]\n",
      "loss: 0.010403  [  500/ 1536]\n",
      "loss: 0.194193  [  600/ 1536]\n",
      "loss: 0.020388  [  700/ 1536]\n",
      "loss: 0.148361  [  800/ 1536]\n",
      "loss: 0.089134  [  900/ 1536]\n",
      "loss: 0.661905  [ 1000/ 1536]\n",
      "loss: 0.014657  [ 1100/ 1536]\n",
      "loss: 0.084683  [ 1200/ 1536]\n",
      "loss: 0.063150  [ 1300/ 1536]\n",
      "loss: 0.038935  [ 1400/ 1536]\n",
      "loss: 0.118410  [ 1500/ 1536]\n",
      "loss: 0.015838  [    0/ 1536]\n",
      "loss: 0.033461  [  100/ 1536]\n",
      "loss: 0.095349  [  200/ 1536]\n",
      "loss: 0.016627  [  300/ 1536]\n",
      "loss: 0.015393  [  400/ 1536]\n",
      "loss: 0.001312  [  500/ 1536]\n",
      "loss: 0.006323  [  600/ 1536]\n",
      "loss: 0.002914  [  700/ 1536]\n",
      "loss: 0.012460  [  800/ 1536]\n",
      "loss: 0.058836  [  900/ 1536]\n",
      "loss: 0.003491  [ 1000/ 1536]\n",
      "loss: 0.001156  [ 1100/ 1536]\n",
      "loss: 0.157422  [ 1200/ 1536]\n",
      "loss: 0.005313  [ 1300/ 1536]\n",
      "loss: 0.004782  [ 1400/ 1536]\n",
      "loss: 0.054555  [ 1500/ 1536]\n",
      "loss: 0.085079  [    0/ 1536]\n",
      "loss: 0.004435  [  100/ 1536]\n",
      "loss: 0.004723  [  200/ 1536]\n",
      "loss: 0.107316  [  300/ 1536]\n",
      "loss: 0.013520  [  400/ 1536]\n",
      "loss: 0.015430  [  500/ 1536]\n",
      "loss: 0.197073  [  600/ 1536]\n",
      "loss: 0.008819  [  700/ 1536]\n",
      "loss: 0.003198  [  800/ 1536]\n",
      "loss: 0.013101  [  900/ 1536]\n",
      "loss: 0.041684  [ 1000/ 1536]\n",
      "loss: 0.082759  [ 1100/ 1536]\n",
      "loss: 0.005823  [ 1200/ 1536]\n",
      "loss: 0.011232  [ 1300/ 1536]\n",
      "loss: 0.188722  [ 1400/ 1536]\n",
      "loss: 0.147904  [ 1500/ 1536]\n",
      "loss: 0.014478  [    0/ 1536]\n",
      "loss: 0.004443  [  100/ 1536]\n",
      "loss: 0.044090  [  200/ 1536]\n",
      "loss: 0.003619  [  300/ 1536]\n",
      "loss: 0.018680  [  400/ 1536]\n",
      "loss: 0.023544  [  500/ 1536]\n",
      "loss: 0.001187  [  600/ 1536]\n",
      "loss: 0.003009  [  700/ 1536]\n",
      "loss: 0.013590  [  800/ 1536]\n",
      "loss: 0.023926  [  900/ 1536]\n",
      "loss: 0.018056  [ 1000/ 1536]\n",
      "loss: 0.004579  [ 1100/ 1536]\n",
      "loss: 0.038186  [ 1200/ 1536]\n",
      "loss: 0.001617  [ 1300/ 1536]\n",
      "loss: 0.045981  [ 1400/ 1536]\n",
      "loss: 0.000176  [ 1500/ 1536]\n",
      "loss: 0.078313  [    0/ 1536]\n",
      "loss: 0.001320  [  100/ 1536]\n",
      "loss: 0.051863  [  200/ 1536]\n",
      "loss: 0.023148  [  300/ 1536]\n",
      "loss: 0.000148  [  400/ 1536]\n",
      "loss: 0.005843  [  500/ 1536]\n",
      "loss: 0.071904  [  600/ 1536]\n",
      "loss: 0.003687  [  700/ 1536]\n",
      "loss: 0.081848  [  800/ 1536]\n",
      "loss: 0.022009  [  900/ 1536]\n",
      "loss: 0.021222  [ 1000/ 1536]\n",
      "loss: 0.060504  [ 1100/ 1536]\n",
      "loss: 0.115846  [ 1200/ 1536]\n",
      "loss: 0.000068  [ 1300/ 1536]\n",
      "loss: 0.008267  [ 1400/ 1536]\n",
      "loss: 0.013751  [ 1500/ 1536]\n",
      "loss: 0.033623  [    0/ 1536]\n",
      "loss: 0.000004  [  100/ 1536]\n",
      "loss: 0.016295  [  200/ 1536]\n",
      "loss: 0.000224  [  300/ 1536]\n",
      "loss: 0.099688  [  400/ 1536]\n",
      "loss: 0.046484  [  500/ 1536]\n",
      "loss: 0.036198  [  600/ 1536]\n",
      "loss: 0.000562  [  700/ 1536]\n",
      "loss: 0.000050  [  800/ 1536]\n",
      "loss: 0.061495  [  900/ 1536]\n",
      "loss: 0.052628  [ 1000/ 1536]\n",
      "loss: 0.040673  [ 1100/ 1536]\n",
      "loss: 0.025956  [ 1200/ 1536]\n",
      "loss: 0.000414  [ 1300/ 1536]\n",
      "loss: 0.014165  [ 1400/ 1536]\n",
      "loss: 0.004749  [ 1500/ 1536]\n",
      "loss: 0.011031  [    0/ 1536]\n",
      "loss: 0.064635  [  100/ 1536]\n",
      "loss: 0.007069  [  200/ 1536]\n",
      "loss: 0.000301  [  300/ 1536]\n",
      "loss: 0.002946  [  400/ 1536]\n",
      "loss: 0.010452  [  500/ 1536]\n",
      "loss: 0.095564  [  600/ 1536]\n",
      "loss: 0.010883  [  700/ 1536]\n",
      "loss: 0.024930  [  800/ 1536]\n",
      "loss: 0.007250  [  900/ 1536]\n",
      "loss: 0.000014  [ 1000/ 1536]\n",
      "loss: 0.070830  [ 1100/ 1536]\n",
      "loss: 0.166420  [ 1200/ 1536]\n",
      "loss: 0.118259  [ 1300/ 1536]\n",
      "loss: 0.013144  [ 1400/ 1536]\n",
      "loss: 0.004442  [ 1500/ 1536]\n",
      "loss: 0.018387  [    0/ 1536]\n",
      "loss: 0.003451  [  100/ 1536]\n",
      "loss: 0.000290  [  200/ 1536]\n",
      "loss: 0.009537  [  300/ 1536]\n",
      "loss: 0.000018  [  400/ 1536]\n",
      "loss: 0.000163  [  500/ 1536]\n",
      "loss: 0.001708  [  600/ 1536]\n",
      "loss: 0.000001  [  700/ 1536]\n",
      "loss: 0.030479  [  800/ 1536]\n",
      "loss: 0.003443  [  900/ 1536]\n",
      "loss: 0.000884  [ 1000/ 1536]\n",
      "loss: 0.005731  [ 1100/ 1536]\n",
      "loss: 0.062077  [ 1200/ 1536]\n",
      "loss: 0.000027  [ 1300/ 1536]\n",
      "loss: 0.116768  [ 1400/ 1536]\n",
      "loss: 0.088814  [ 1500/ 1536]\n",
      "loss: 0.000040  [    0/ 1536]\n",
      "loss: 0.036438  [  100/ 1536]\n",
      "loss: 0.015414  [  200/ 1536]\n",
      "loss: 0.000440  [  300/ 1536]\n",
      "loss: 0.032834  [  400/ 1536]\n",
      "loss: 0.003487  [  500/ 1536]\n",
      "loss: 0.010080  [  600/ 1536]\n",
      "loss: 0.013786  [  700/ 1536]\n",
      "loss: 0.022139  [  800/ 1536]\n",
      "loss: 0.038534  [  900/ 1536]\n",
      "loss: 0.028707  [ 1000/ 1536]\n",
      "loss: 0.014700  [ 1100/ 1536]\n",
      "loss: 0.024905  [ 1200/ 1536]\n",
      "loss: 0.453237  [ 1300/ 1536]\n",
      "loss: 0.007934  [ 1400/ 1536]\n",
      "loss: 0.006637  [ 1500/ 1536]\n",
      "loss: 0.000111  [    0/ 1536]\n",
      "loss: 0.110611  [  100/ 1536]\n",
      "loss: 0.074736  [  200/ 1536]\n",
      "loss: 0.030437  [  300/ 1536]\n",
      "loss: 0.001301  [  400/ 1536]\n",
      "loss: 0.211525  [  500/ 1536]\n",
      "loss: 0.000576  [  600/ 1536]\n",
      "loss: 0.027468  [  700/ 1536]\n",
      "loss: 0.008362  [  800/ 1536]\n",
      "loss: 0.059396  [  900/ 1536]\n",
      "loss: 0.012388  [ 1000/ 1536]\n",
      "loss: 0.096783  [ 1100/ 1536]\n",
      "loss: 0.004576  [ 1200/ 1536]\n",
      "loss: 0.001049  [ 1300/ 1536]\n",
      "loss: 0.059711  [ 1400/ 1536]\n",
      "loss: 0.056035  [ 1500/ 1536]\n",
      "loss: 0.002278  [    0/ 1536]\n",
      "loss: 0.058981  [  100/ 1536]\n",
      "loss: 0.005184  [  200/ 1536]\n",
      "loss: 0.448360  [  300/ 1536]\n",
      "loss: 0.125302  [  400/ 1536]\n",
      "loss: 0.035940  [  500/ 1536]\n",
      "loss: 0.000239  [  600/ 1536]\n",
      "loss: 0.033001  [  700/ 1536]\n",
      "loss: 0.054573  [  800/ 1536]\n",
      "loss: 0.002711  [  900/ 1536]\n",
      "loss: 0.048019  [ 1000/ 1536]\n",
      "loss: 0.153908  [ 1100/ 1536]\n",
      "loss: 0.000930  [ 1200/ 1536]\n",
      "loss: 0.000477  [ 1300/ 1536]\n",
      "loss: 0.070068  [ 1400/ 1536]\n",
      "loss: 0.006714  [ 1500/ 1536]\n",
      "loss: 0.112110  [    0/ 1536]\n",
      "loss: 0.210009  [  100/ 1536]\n",
      "loss: 0.101642  [  200/ 1536]\n",
      "loss: 0.019154  [  300/ 1536]\n",
      "loss: 0.097592  [  400/ 1536]\n",
      "loss: 0.000359  [  500/ 1536]\n",
      "loss: 0.215969  [  600/ 1536]\n",
      "loss: 0.032634  [  700/ 1536]\n",
      "loss: 0.005264  [  800/ 1536]\n",
      "loss: 0.129284  [  900/ 1536]\n",
      "loss: 0.005095  [ 1000/ 1536]\n",
      "loss: 0.047336  [ 1100/ 1536]\n",
      "loss: 0.000079  [ 1200/ 1536]\n",
      "loss: 0.015791  [ 1300/ 1536]\n",
      "loss: 0.050831  [ 1400/ 1536]\n",
      "loss: 0.002141  [ 1500/ 1536]\n",
      "loss: 0.005491  [    0/ 1536]\n",
      "loss: 0.027278  [  100/ 1536]\n",
      "loss: 0.028665  [  200/ 1536]\n",
      "loss: 0.000238  [  300/ 1536]\n",
      "loss: 0.032723  [  400/ 1536]\n",
      "loss: 0.001483  [  500/ 1536]\n",
      "loss: 0.005557  [  600/ 1536]\n",
      "loss: 0.113707  [  700/ 1536]\n",
      "loss: 0.103007  [  800/ 1536]\n",
      "loss: 0.033252  [  900/ 1536]\n",
      "loss: 0.004432  [ 1000/ 1536]\n",
      "loss: 0.003602  [ 1100/ 1536]\n",
      "loss: 0.014959  [ 1200/ 1536]\n",
      "loss: 0.019252  [ 1300/ 1536]\n",
      "loss: 0.002452  [ 1400/ 1536]\n",
      "loss: 0.020459  [ 1500/ 1536]\n",
      "loss: 0.005208  [    0/ 1536]\n",
      "loss: 0.108520  [  100/ 1536]\n",
      "loss: 0.238654  [  200/ 1536]\n",
      "loss: 0.007813  [  300/ 1536]\n",
      "loss: 0.006131  [  400/ 1536]\n",
      "loss: 0.001646  [  500/ 1536]\n",
      "loss: 0.001389  [  600/ 1536]\n",
      "loss: 0.051785  [  700/ 1536]\n",
      "loss: 0.003964  [  800/ 1536]\n",
      "loss: 0.041298  [  900/ 1536]\n",
      "loss: 0.005190  [ 1000/ 1536]\n",
      "loss: 0.034733  [ 1100/ 1536]\n",
      "loss: 0.000311  [ 1200/ 1536]\n",
      "loss: 0.039241  [ 1300/ 1536]\n",
      "loss: 0.000769  [ 1400/ 1536]\n",
      "loss: 0.002113  [ 1500/ 1536]\n",
      "loss: 0.065921  [    0/ 1536]\n",
      "loss: 0.149764  [  100/ 1536]\n",
      "loss: 0.236971  [  200/ 1536]\n",
      "loss: 0.001051  [  300/ 1536]\n",
      "loss: 0.077029  [  400/ 1536]\n",
      "loss: 0.035328  [  500/ 1536]\n",
      "loss: 0.095033  [  600/ 1536]\n",
      "loss: 0.000333  [  700/ 1536]\n",
      "loss: 0.016151  [  800/ 1536]\n",
      "loss: 0.227764  [  900/ 1536]\n",
      "loss: 0.142544  [ 1000/ 1536]\n",
      "loss: 0.088675  [ 1100/ 1536]\n",
      "loss: 0.014482  [ 1200/ 1536]\n",
      "loss: 0.056238  [ 1300/ 1536]\n",
      "loss: 0.000234  [ 1400/ 1536]\n",
      "loss: 0.041867  [ 1500/ 1536]\n",
      "loss: 0.043250  [    0/ 1536]\n",
      "loss: 0.029898  [  100/ 1536]\n",
      "loss: 0.001740  [  200/ 1536]\n",
      "loss: 0.005070  [  300/ 1536]\n",
      "loss: 0.062707  [  400/ 1536]\n",
      "loss: 0.019876  [  500/ 1536]\n",
      "loss: 0.012534  [  600/ 1536]\n",
      "loss: 0.146142  [  700/ 1536]\n",
      "loss: 0.047365  [  800/ 1536]\n",
      "loss: 0.017623  [  900/ 1536]\n",
      "loss: 0.118558  [ 1000/ 1536]\n",
      "loss: 0.005619  [ 1100/ 1536]\n",
      "loss: 0.007570  [ 1200/ 1536]\n",
      "loss: 0.141588  [ 1300/ 1536]\n",
      "loss: 0.005310  [ 1400/ 1536]\n",
      "loss: 0.108888  [ 1500/ 1536]\n",
      "loss: 0.001520  [    0/ 1536]\n",
      "loss: 0.060184  [  100/ 1536]\n",
      "loss: 0.030516  [  200/ 1536]\n",
      "loss: 0.039315  [  300/ 1536]\n",
      "loss: 0.001297  [  400/ 1536]\n",
      "loss: 0.083705  [  500/ 1536]\n",
      "loss: 0.022217  [  600/ 1536]\n",
      "loss: 0.003057  [  700/ 1536]\n",
      "loss: 0.051786  [  800/ 1536]\n",
      "loss: 0.004441  [  900/ 1536]\n",
      "loss: 0.009953  [ 1000/ 1536]\n",
      "loss: 0.018245  [ 1100/ 1536]\n",
      "loss: 0.045523  [ 1200/ 1536]\n",
      "loss: 0.107831  [ 1300/ 1536]\n",
      "loss: 0.045966  [ 1400/ 1536]\n",
      "loss: 0.064757  [ 1500/ 1536]\n",
      "loss: 0.000700  [    0/ 1536]\n",
      "loss: 0.242789  [  100/ 1536]\n",
      "loss: 0.037111  [  200/ 1536]\n",
      "loss: 0.172328  [  300/ 1536]\n",
      "loss: 0.001546  [  400/ 1536]\n",
      "loss: 0.000212  [  500/ 1536]\n",
      "loss: 0.007114  [  600/ 1536]\n",
      "loss: 0.002202  [  700/ 1536]\n",
      "loss: 0.032700  [  800/ 1536]\n",
      "loss: 0.091612  [  900/ 1536]\n",
      "loss: 0.002769  [ 1000/ 1536]\n",
      "loss: 0.036481  [ 1100/ 1536]\n",
      "loss: 0.010395  [ 1200/ 1536]\n",
      "loss: 0.040399  [ 1300/ 1536]\n",
      "loss: 0.000002  [ 1400/ 1536]\n",
      "loss: 0.014397  [ 1500/ 1536]\n",
      "loss: 0.125139  [    0/ 1536]\n",
      "loss: 0.017267  [  100/ 1536]\n",
      "loss: 0.002591  [  200/ 1536]\n",
      "loss: 0.000119  [  300/ 1536]\n",
      "loss: 0.005506  [  400/ 1536]\n",
      "loss: 0.058265  [  500/ 1536]\n",
      "loss: 0.010852  [  600/ 1536]\n",
      "loss: 0.008828  [  700/ 1536]\n",
      "loss: 0.008948  [  800/ 1536]\n",
      "loss: 0.050912  [  900/ 1536]\n",
      "loss: 0.022863  [ 1000/ 1536]\n",
      "loss: 0.051010  [ 1100/ 1536]\n",
      "loss: 0.322095  [ 1200/ 1536]\n",
      "loss: 0.113840  [ 1300/ 1536]\n",
      "loss: 0.046514  [ 1400/ 1536]\n",
      "loss: 0.113563  [ 1500/ 1536]\n",
      "loss: 0.000566  [    0/ 1536]\n",
      "loss: 0.000130  [  100/ 1536]\n",
      "loss: 0.242516  [  200/ 1536]\n",
      "loss: 0.071486  [  300/ 1536]\n",
      "loss: 0.076042  [  400/ 1536]\n",
      "loss: 0.002456  [  500/ 1536]\n",
      "loss: 0.003031  [  600/ 1536]\n",
      "loss: 0.025490  [  700/ 1536]\n",
      "loss: 0.018126  [  800/ 1536]\n",
      "loss: 0.001299  [  900/ 1536]\n",
      "loss: 0.005214  [ 1000/ 1536]\n",
      "loss: 0.048503  [ 1100/ 1536]\n",
      "loss: 0.022934  [ 1200/ 1536]\n",
      "loss: 0.061941  [ 1300/ 1536]\n",
      "loss: 0.189391  [ 1400/ 1536]\n",
      "loss: 0.016124  [ 1500/ 1536]\n",
      "loss: 0.181577  [    0/ 1536]\n",
      "loss: 0.005627  [  100/ 1536]\n",
      "loss: 0.131577  [  200/ 1536]\n",
      "loss: 0.005738  [  300/ 1536]\n",
      "loss: 0.211306  [  400/ 1536]\n",
      "loss: 0.000998  [  500/ 1536]\n",
      "loss: 0.164643  [  600/ 1536]\n",
      "loss: 0.088553  [  700/ 1536]\n",
      "loss: 0.009278  [  800/ 1536]\n",
      "loss: 0.116462  [  900/ 1536]\n",
      "loss: 0.030752  [ 1000/ 1536]\n",
      "loss: 0.056464  [ 1100/ 1536]\n",
      "loss: 0.045616  [ 1200/ 1536]\n",
      "loss: 0.025501  [ 1300/ 1536]\n",
      "loss: 0.007532  [ 1400/ 1536]\n",
      "loss: 0.000739  [ 1500/ 1536]\n",
      "loss: 0.009070  [    0/ 1536]\n",
      "loss: 0.000635  [  100/ 1536]\n",
      "loss: 0.002234  [  200/ 1536]\n",
      "loss: 0.016766  [  300/ 1536]\n",
      "loss: 0.099421  [  400/ 1536]\n",
      "loss: 0.000172  [  500/ 1536]\n",
      "loss: 0.000991  [  600/ 1536]\n",
      "loss: 0.045503  [  700/ 1536]\n",
      "loss: 0.003250  [  800/ 1536]\n",
      "loss: 0.069739  [  900/ 1536]\n",
      "loss: 0.046113  [ 1000/ 1536]\n",
      "loss: 0.000603  [ 1100/ 1536]\n",
      "loss: 0.002890  [ 1200/ 1536]\n",
      "loss: 0.038142  [ 1300/ 1536]\n",
      "loss: 0.028193  [ 1400/ 1536]\n",
      "loss: 0.009546  [ 1500/ 1536]\n",
      "loss: 0.066520  [    0/ 1536]\n",
      "loss: 0.012408  [  100/ 1536]\n",
      "loss: 0.005487  [  200/ 1536]\n",
      "loss: 0.039589  [  300/ 1536]\n",
      "loss: 0.005104  [  400/ 1536]\n",
      "loss: 0.010782  [  500/ 1536]\n",
      "loss: 0.001207  [  600/ 1536]\n",
      "loss: 0.002121  [  700/ 1536]\n",
      "loss: 0.331226  [  800/ 1536]\n",
      "loss: 0.014455  [  900/ 1536]\n",
      "loss: 0.246486  [ 1000/ 1536]\n",
      "loss: 0.022196  [ 1100/ 1536]\n",
      "loss: 0.029694  [ 1200/ 1536]\n",
      "loss: 0.013176  [ 1300/ 1536]\n",
      "loss: 0.022047  [ 1400/ 1536]\n",
      "loss: 0.001516  [ 1500/ 1536]\n",
      "loss: 0.103492  [    0/ 1536]\n",
      "loss: 0.018454  [  100/ 1536]\n",
      "loss: 0.074577  [  200/ 1536]\n",
      "loss: 0.030738  [  300/ 1536]\n",
      "loss: 0.007001  [  400/ 1536]\n",
      "loss: 0.000375  [  500/ 1536]\n",
      "loss: 0.043204  [  600/ 1536]\n",
      "loss: 0.000479  [  700/ 1536]\n",
      "loss: 0.032581  [  800/ 1536]\n",
      "loss: 0.026314  [  900/ 1536]\n",
      "loss: 0.040939  [ 1000/ 1536]\n",
      "loss: 0.041574  [ 1100/ 1536]\n",
      "loss: 0.081206  [ 1200/ 1536]\n",
      "loss: 0.007986  [ 1300/ 1536]\n",
      "loss: 0.013946  [ 1400/ 1536]\n",
      "loss: 0.019080  [ 1500/ 1536]\n",
      "loss: 0.009195  [    0/ 1536]\n",
      "loss: 0.213707  [  100/ 1536]\n",
      "loss: 0.059625  [  200/ 1536]\n",
      "loss: 0.007645  [  300/ 1536]\n",
      "loss: 0.020731  [  400/ 1536]\n",
      "loss: 0.022057  [  500/ 1536]\n",
      "loss: 0.024473  [  600/ 1536]\n",
      "loss: 0.039614  [  700/ 1536]\n",
      "loss: 0.008954  [  800/ 1536]\n",
      "loss: 0.051958  [  900/ 1536]\n",
      "loss: 0.038476  [ 1000/ 1536]\n",
      "loss: 0.008127  [ 1100/ 1536]\n",
      "loss: 0.002178  [ 1200/ 1536]\n",
      "loss: 0.006243  [ 1300/ 1536]\n",
      "loss: 0.298353  [ 1400/ 1536]\n",
      "loss: 0.037792  [ 1500/ 1536]\n",
      "loss: 0.088496  [    0/ 1536]\n",
      "loss: 0.058359  [  100/ 1536]\n",
      "loss: 0.000103  [  200/ 1536]\n",
      "loss: 0.086700  [  300/ 1536]\n",
      "loss: 0.195302  [  400/ 1536]\n",
      "loss: 0.000504  [  500/ 1536]\n",
      "loss: 0.003526  [  600/ 1536]\n",
      "loss: 0.230644  [  700/ 1536]\n",
      "loss: 0.002560  [  800/ 1536]\n",
      "loss: 0.022991  [  900/ 1536]\n",
      "loss: 0.003902  [ 1000/ 1536]\n",
      "loss: 0.040702  [ 1100/ 1536]\n",
      "loss: 0.038589  [ 1200/ 1536]\n",
      "loss: 0.004597  [ 1300/ 1536]\n",
      "loss: 0.000013  [ 1400/ 1536]\n",
      "loss: 0.002217  [ 1500/ 1536]\n",
      "loss: 0.018520  [    0/ 1536]\n",
      "loss: 0.007108  [  100/ 1536]\n",
      "loss: 0.035552  [  200/ 1536]\n",
      "loss: 0.012072  [  300/ 1536]\n",
      "loss: 0.017591  [  400/ 1536]\n",
      "loss: 0.002081  [  500/ 1536]\n",
      "loss: 0.039777  [  600/ 1536]\n",
      "loss: 0.005844  [  700/ 1536]\n",
      "loss: 0.006670  [  800/ 1536]\n",
      "loss: 0.261875  [  900/ 1536]\n",
      "loss: 0.019512  [ 1000/ 1536]\n",
      "loss: 0.004407  [ 1100/ 1536]\n",
      "loss: 0.192211  [ 1200/ 1536]\n",
      "loss: 0.061733  [ 1300/ 1536]\n",
      "loss: 0.000677  [ 1400/ 1536]\n",
      "loss: 0.000206  [ 1500/ 1536]\n",
      "loss: 0.306533  [    0/ 1536]\n",
      "loss: 0.000120  [  100/ 1536]\n",
      "loss: 0.009059  [  200/ 1536]\n",
      "loss: 0.087672  [  300/ 1536]\n",
      "loss: 0.004780  [  400/ 1536]\n",
      "loss: 0.014777  [  500/ 1536]\n",
      "loss: 0.234938  [  600/ 1536]\n",
      "loss: 0.041096  [  700/ 1536]\n",
      "loss: 0.023483  [  800/ 1536]\n",
      "loss: 0.117890  [  900/ 1536]\n",
      "loss: 0.003132  [ 1000/ 1536]\n",
      "loss: 0.001280  [ 1100/ 1536]\n",
      "loss: 0.014954  [ 1200/ 1536]\n",
      "loss: 0.168190  [ 1300/ 1536]\n",
      "loss: 0.017982  [ 1400/ 1536]\n",
      "loss: 0.003108  [ 1500/ 1536]\n",
      "loss: 0.002553  [    0/ 1536]\n",
      "loss: 0.000148  [  100/ 1536]\n",
      "loss: 0.002200  [  200/ 1536]\n",
      "loss: 0.004429  [  300/ 1536]\n",
      "loss: 0.149289  [  400/ 1536]\n",
      "loss: 0.005177  [  500/ 1536]\n",
      "loss: 0.079399  [  600/ 1536]\n",
      "loss: 0.007498  [  700/ 1536]\n",
      "loss: 0.011434  [  800/ 1536]\n",
      "loss: 0.016779  [  900/ 1536]\n",
      "loss: 0.056392  [ 1000/ 1536]\n",
      "loss: 0.139457  [ 1100/ 1536]\n",
      "loss: 0.019091  [ 1200/ 1536]\n",
      "loss: 0.033286  [ 1300/ 1536]\n",
      "loss: 0.005194  [ 1400/ 1536]\n",
      "loss: 0.035689  [ 1500/ 1536]\n",
      "loss: 0.006823  [    0/ 1536]\n",
      "loss: 0.006101  [  100/ 1536]\n",
      "loss: 0.006628  [  200/ 1536]\n",
      "loss: 0.042389  [  300/ 1536]\n",
      "loss: 0.054493  [  400/ 1536]\n",
      "loss: 0.002494  [  500/ 1536]\n",
      "loss: 0.018501  [  600/ 1536]\n",
      "loss: 0.005446  [  700/ 1536]\n",
      "loss: 0.000366  [  800/ 1536]\n",
      "loss: 0.003571  [  900/ 1536]\n",
      "loss: 0.002081  [ 1000/ 1536]\n",
      "loss: 0.038857  [ 1100/ 1536]\n",
      "loss: 0.074755  [ 1200/ 1536]\n",
      "loss: 0.000765  [ 1300/ 1536]\n",
      "loss: 0.207932  [ 1400/ 1536]\n",
      "loss: 0.007500  [ 1500/ 1536]\n",
      "loss: 0.237454  [    0/ 1536]\n",
      "loss: 0.060837  [  100/ 1536]\n",
      "loss: 0.000000  [  200/ 1536]\n",
      "loss: 0.011868  [  300/ 1536]\n",
      "loss: 0.020925  [  400/ 1536]\n",
      "loss: 0.027677  [  500/ 1536]\n",
      "loss: 0.018189  [  600/ 1536]\n",
      "loss: 0.038905  [  700/ 1536]\n",
      "loss: 0.035251  [  800/ 1536]\n",
      "loss: 0.122300  [  900/ 1536]\n",
      "loss: 0.000373  [ 1000/ 1536]\n",
      "loss: 0.061243  [ 1100/ 1536]\n",
      "loss: 0.092713  [ 1200/ 1536]\n",
      "loss: 0.065271  [ 1300/ 1536]\n",
      "loss: 0.133101  [ 1400/ 1536]\n",
      "loss: 0.103550  [ 1500/ 1536]\n",
      "loss: 0.008885  [    0/ 1536]\n",
      "loss: 0.034257  [  100/ 1536]\n",
      "loss: 0.000955  [  200/ 1536]\n",
      "loss: 0.019091  [  300/ 1536]\n",
      "loss: 0.007798  [  400/ 1536]\n",
      "loss: 0.226788  [  500/ 1536]\n",
      "loss: 0.012319  [  600/ 1536]\n",
      "loss: 0.003876  [  700/ 1536]\n",
      "loss: 0.071979  [  800/ 1536]\n",
      "loss: 0.000402  [  900/ 1536]\n",
      "loss: 0.057328  [ 1000/ 1536]\n",
      "loss: 0.014147  [ 1100/ 1536]\n",
      "loss: 0.091172  [ 1200/ 1536]\n",
      "loss: 0.032429  [ 1300/ 1536]\n",
      "loss: 0.010574  [ 1400/ 1536]\n",
      "loss: 0.001305  [ 1500/ 1536]\n",
      "loss: 0.023244  [    0/ 1536]\n",
      "loss: 0.065770  [  100/ 1536]\n",
      "loss: 0.000048  [  200/ 1536]\n",
      "loss: 0.006794  [  300/ 1536]\n",
      "loss: 0.000820  [  400/ 1536]\n",
      "loss: 0.019028  [  500/ 1536]\n",
      "loss: 0.198347  [  600/ 1536]\n",
      "loss: 0.000304  [  700/ 1536]\n",
      "loss: 0.094171  [  800/ 1536]\n",
      "loss: 0.015134  [  900/ 1536]\n",
      "loss: 0.000802  [ 1000/ 1536]\n",
      "loss: 0.002807  [ 1100/ 1536]\n",
      "loss: 0.029288  [ 1200/ 1536]\n",
      "loss: 0.011211  [ 1300/ 1536]\n",
      "loss: 0.004973  [ 1400/ 1536]\n",
      "loss: 0.066676  [ 1500/ 1536]\n",
      "loss: 0.108171  [    0/ 1536]\n",
      "loss: 0.193654  [  100/ 1536]\n",
      "loss: 0.001026  [  200/ 1536]\n",
      "loss: 0.048070  [  300/ 1536]\n",
      "loss: 0.011684  [  400/ 1536]\n",
      "loss: 0.217672  [  500/ 1536]\n",
      "loss: 0.177344  [  600/ 1536]\n",
      "loss: 0.019347  [  700/ 1536]\n",
      "loss: 0.004685  [  800/ 1536]\n",
      "loss: 0.051789  [  900/ 1536]\n",
      "loss: 0.019182  [ 1000/ 1536]\n",
      "loss: 0.034558  [ 1100/ 1536]\n",
      "loss: 0.012960  [ 1200/ 1536]\n",
      "loss: 0.089082  [ 1300/ 1536]\n",
      "loss: 0.000661  [ 1400/ 1536]\n",
      "loss: 0.011759  [ 1500/ 1536]\n",
      "loss: 0.052148  [    0/ 1536]\n",
      "loss: 0.014727  [  100/ 1536]\n",
      "loss: 0.009743  [  200/ 1536]\n",
      "loss: 0.030065  [  300/ 1536]\n",
      "loss: 0.002396  [  400/ 1536]\n",
      "loss: 0.003271  [  500/ 1536]\n",
      "loss: 0.001230  [  600/ 1536]\n",
      "loss: 0.066071  [  700/ 1536]\n",
      "loss: 0.190931  [  800/ 1536]\n",
      "loss: 0.001287  [  900/ 1536]\n",
      "loss: 0.009932  [ 1000/ 1536]\n",
      "loss: 0.070294  [ 1100/ 1536]\n",
      "loss: 0.186620  [ 1200/ 1536]\n",
      "loss: 0.030229  [ 1300/ 1536]\n",
      "loss: 0.006816  [ 1400/ 1536]\n",
      "loss: 0.085769  [ 1500/ 1536]\n",
      "loss: 0.006454  [    0/ 1536]\n",
      "loss: 0.082527  [  100/ 1536]\n",
      "loss: 0.051565  [  200/ 1536]\n",
      "loss: 0.114374  [  300/ 1536]\n",
      "loss: 0.001159  [  400/ 1536]\n",
      "loss: 0.143863  [  500/ 1536]\n",
      "loss: 0.015565  [  600/ 1536]\n",
      "loss: 0.046992  [  700/ 1536]\n",
      "loss: 0.018380  [  800/ 1536]\n",
      "loss: 0.036144  [  900/ 1536]\n",
      "loss: 0.098408  [ 1000/ 1536]\n",
      "loss: 0.075624  [ 1100/ 1536]\n",
      "loss: 0.044561  [ 1200/ 1536]\n",
      "loss: 0.000623  [ 1300/ 1536]\n",
      "loss: 0.246075  [ 1400/ 1536]\n",
      "loss: 0.018334  [ 1500/ 1536]\n",
      "loss: 0.002157  [    0/ 1536]\n",
      "loss: 0.000292  [  100/ 1536]\n",
      "loss: 0.000995  [  200/ 1536]\n",
      "loss: 0.002151  [  300/ 1536]\n",
      "loss: 0.064541  [  400/ 1536]\n",
      "loss: 0.130200  [  500/ 1536]\n",
      "loss: 0.004071  [  600/ 1536]\n",
      "loss: 0.000135  [  700/ 1536]\n",
      "loss: 0.011392  [  800/ 1536]\n",
      "loss: 0.017703  [  900/ 1536]\n",
      "loss: 0.042329  [ 1000/ 1536]\n",
      "loss: 0.141237  [ 1100/ 1536]\n",
      "loss: 0.061521  [ 1200/ 1536]\n",
      "loss: 0.072362  [ 1300/ 1536]\n",
      "loss: 0.058673  [ 1400/ 1536]\n",
      "loss: 0.049778  [ 1500/ 1536]\n",
      "loss: 0.019082  [    0/ 1536]\n",
      "loss: 0.010980  [  100/ 1536]\n",
      "loss: 0.003160  [  200/ 1536]\n",
      "loss: 0.264108  [  300/ 1536]\n",
      "loss: 0.007023  [  400/ 1536]\n",
      "loss: 0.007897  [  500/ 1536]\n",
      "loss: 0.002143  [  600/ 1536]\n",
      "loss: 0.007130  [  700/ 1536]\n",
      "loss: 0.058051  [  800/ 1536]\n",
      "loss: 0.027588  [  900/ 1536]\n",
      "loss: 0.024887  [ 1000/ 1536]\n",
      "loss: 0.024066  [ 1100/ 1536]\n",
      "loss: 0.018381  [ 1200/ 1536]\n",
      "loss: 0.033967  [ 1300/ 1536]\n",
      "loss: 0.110916  [ 1400/ 1536]\n",
      "loss: 0.016321  [ 1500/ 1536]\n",
      "loss: 0.009066  [    0/ 1536]\n",
      "loss: 0.004707  [  100/ 1536]\n",
      "loss: 0.005539  [  200/ 1536]\n",
      "loss: 0.002467  [  300/ 1536]\n",
      "loss: 0.000161  [  400/ 1536]\n",
      "loss: 0.000033  [  500/ 1536]\n",
      "loss: 0.000345  [  600/ 1536]\n",
      "loss: 0.006627  [  700/ 1536]\n",
      "loss: 0.003479  [  800/ 1536]\n",
      "loss: 0.001626  [  900/ 1536]\n",
      "loss: 0.075902  [ 1000/ 1536]\n",
      "loss: 0.047433  [ 1100/ 1536]\n",
      "loss: 0.052012  [ 1200/ 1536]\n",
      "loss: 0.000913  [ 1300/ 1536]\n",
      "loss: 0.042475  [ 1400/ 1536]\n",
      "loss: 0.015681  [ 1500/ 1536]\n",
      "loss: 0.014239  [    0/ 1536]\n",
      "loss: 0.105672  [  100/ 1536]\n",
      "loss: 0.004289  [  200/ 1536]\n",
      "loss: 0.021163  [  300/ 1536]\n",
      "loss: 0.029377  [  400/ 1536]\n",
      "loss: 0.132311  [  500/ 1536]\n",
      "loss: 0.012753  [  600/ 1536]\n",
      "loss: 0.166051  [  700/ 1536]\n",
      "loss: 0.049746  [  800/ 1536]\n",
      "loss: 0.043013  [  900/ 1536]\n",
      "loss: 0.063725  [ 1000/ 1536]\n",
      "loss: 0.075912  [ 1100/ 1536]\n",
      "loss: 0.007233  [ 1200/ 1536]\n",
      "loss: 0.029988  [ 1300/ 1536]\n",
      "loss: 0.005558  [ 1400/ 1536]\n",
      "loss: 0.095654  [ 1500/ 1536]\n",
      "loss: 0.032748  [    0/ 1536]\n",
      "loss: 0.059395  [  100/ 1536]\n",
      "loss: 0.000783  [  200/ 1536]\n",
      "loss: 0.001172  [  300/ 1536]\n",
      "loss: 0.007550  [  400/ 1536]\n",
      "loss: 0.027078  [  500/ 1536]\n",
      "loss: 0.000039  [  600/ 1536]\n",
      "loss: 0.024138  [  700/ 1536]\n",
      "loss: 0.004927  [  800/ 1536]\n",
      "loss: 0.035475  [  900/ 1536]\n",
      "loss: 0.000667  [ 1000/ 1536]\n",
      "loss: 0.060607  [ 1100/ 1536]\n",
      "loss: 0.000207  [ 1200/ 1536]\n",
      "loss: 0.001306  [ 1300/ 1536]\n",
      "loss: 0.131205  [ 1400/ 1536]\n",
      "loss: 0.028329  [ 1500/ 1536]\n",
      "loss: 0.006518  [    0/ 1536]\n",
      "loss: 0.032839  [  100/ 1536]\n",
      "loss: 0.050388  [  200/ 1536]\n",
      "loss: 0.001463  [  300/ 1536]\n",
      "loss: 0.016300  [  400/ 1536]\n",
      "loss: 0.046978  [  500/ 1536]\n",
      "loss: 0.004808  [  600/ 1536]\n",
      "loss: 0.000161  [  700/ 1536]\n",
      "loss: 0.000917  [  800/ 1536]\n",
      "loss: 0.000008  [  900/ 1536]\n",
      "loss: 0.008832  [ 1000/ 1536]\n",
      "loss: 0.028655  [ 1100/ 1536]\n",
      "loss: 0.004087  [ 1200/ 1536]\n",
      "loss: 0.000075  [ 1300/ 1536]\n",
      "loss: 0.061858  [ 1400/ 1536]\n",
      "loss: 0.108083  [ 1500/ 1536]\n",
      "loss: 0.000461  [    0/ 1536]\n",
      "loss: 0.000328  [  100/ 1536]\n",
      "loss: 0.007420  [  200/ 1536]\n",
      "loss: 0.000839  [  300/ 1536]\n",
      "loss: 0.001023  [  400/ 1536]\n",
      "loss: 0.027308  [  500/ 1536]\n",
      "loss: 0.049470  [  600/ 1536]\n",
      "loss: 0.002140  [  700/ 1536]\n",
      "loss: 0.018414  [  800/ 1536]\n",
      "loss: 0.031348  [  900/ 1536]\n",
      "loss: 0.072480  [ 1000/ 1536]\n",
      "loss: 0.031539  [ 1100/ 1536]\n",
      "loss: 0.003853  [ 1200/ 1536]\n",
      "loss: 0.009040  [ 1300/ 1536]\n",
      "loss: 0.026422  [ 1400/ 1536]\n",
      "loss: 0.060449  [ 1500/ 1536]\n",
      "loss: 0.081098  [    0/ 1536]\n",
      "loss: 0.067581  [  100/ 1536]\n",
      "loss: 0.150675  [  200/ 1536]\n",
      "loss: 0.061503  [  300/ 1536]\n",
      "loss: 0.042721  [  400/ 1536]\n",
      "loss: 0.000381  [  500/ 1536]\n",
      "loss: 0.035838  [  600/ 1536]\n",
      "loss: 0.000008  [  700/ 1536]\n",
      "loss: 0.001039  [  800/ 1536]\n",
      "loss: 0.413445  [  900/ 1536]\n",
      "loss: 0.029279  [ 1000/ 1536]\n",
      "loss: 0.040942  [ 1100/ 1536]\n",
      "loss: 0.027457  [ 1200/ 1536]\n",
      "loss: 0.015812  [ 1300/ 1536]\n",
      "loss: 0.017829  [ 1400/ 1536]\n",
      "loss: 0.110844  [ 1500/ 1536]\n",
      "loss: 0.002030  [    0/ 1536]\n",
      "loss: 0.001313  [  100/ 1536]\n",
      "loss: 0.061722  [  200/ 1536]\n",
      "loss: 0.083934  [  300/ 1536]\n",
      "loss: 0.001249  [  400/ 1536]\n",
      "loss: 0.236715  [  500/ 1536]\n",
      "loss: 0.032165  [  600/ 1536]\n",
      "loss: 0.100975  [  700/ 1536]\n",
      "loss: 0.042594  [  800/ 1536]\n",
      "loss: 0.005317  [  900/ 1536]\n",
      "loss: 0.065065  [ 1000/ 1536]\n",
      "loss: 0.005139  [ 1100/ 1536]\n",
      "loss: 0.105417  [ 1200/ 1536]\n",
      "loss: 0.006999  [ 1300/ 1536]\n",
      "loss: 0.102107  [ 1400/ 1536]\n",
      "loss: 0.114640  [ 1500/ 1536]\n",
      "loss: 0.017162  [    0/ 1536]\n",
      "loss: 0.056951  [  100/ 1536]\n",
      "loss: 0.026886  [  200/ 1536]\n",
      "loss: 0.011420  [  300/ 1536]\n",
      "loss: 0.084786  [  400/ 1536]\n",
      "loss: 0.017890  [  500/ 1536]\n",
      "loss: 0.005119  [  600/ 1536]\n",
      "loss: 0.092174  [  700/ 1536]\n",
      "loss: 0.019451  [  800/ 1536]\n",
      "loss: 0.001327  [  900/ 1536]\n",
      "loss: 0.015560  [ 1000/ 1536]\n",
      "loss: 0.000609  [ 1100/ 1536]\n",
      "loss: 0.001516  [ 1200/ 1536]\n",
      "loss: 0.063705  [ 1300/ 1536]\n",
      "loss: 0.000425  [ 1400/ 1536]\n",
      "loss: 0.007098  [ 1500/ 1536]\n",
      "loss: 0.005736  [    0/ 1536]\n",
      "loss: 0.003442  [  100/ 1536]\n",
      "loss: 0.000059  [  200/ 1536]\n",
      "loss: 0.042871  [  300/ 1536]\n",
      "loss: 0.000401  [  400/ 1536]\n",
      "loss: 0.072510  [  500/ 1536]\n",
      "loss: 0.000020  [  600/ 1536]\n",
      "loss: 0.007229  [  700/ 1536]\n",
      "loss: 0.071272  [  800/ 1536]\n",
      "loss: 0.021567  [  900/ 1536]\n",
      "loss: 0.107694  [ 1000/ 1536]\n",
      "loss: 0.029781  [ 1100/ 1536]\n",
      "loss: 0.040453  [ 1200/ 1536]\n",
      "loss: 0.001941  [ 1300/ 1536]\n",
      "loss: 0.000298  [ 1400/ 1536]\n",
      "loss: 0.042114  [ 1500/ 1536]\n",
      "loss: 0.013824  [    0/ 1536]\n",
      "loss: 0.048136  [  100/ 1536]\n",
      "loss: 0.003424  [  200/ 1536]\n",
      "loss: 0.041101  [  300/ 1536]\n",
      "loss: 0.028780  [  400/ 1536]\n",
      "loss: 0.006276  [  500/ 1536]\n",
      "loss: 0.060334  [  600/ 1536]\n",
      "loss: 0.010275  [  700/ 1536]\n",
      "loss: 0.059541  [  800/ 1536]\n",
      "loss: 0.007459  [  900/ 1536]\n",
      "loss: 0.019031  [ 1000/ 1536]\n",
      "loss: 0.002491  [ 1100/ 1536]\n",
      "loss: 0.106644  [ 1200/ 1536]\n",
      "loss: 0.006626  [ 1300/ 1536]\n",
      "loss: 0.354215  [ 1400/ 1536]\n",
      "loss: 0.000215  [ 1500/ 1536]\n",
      "loss: 0.000973  [    0/ 1536]\n",
      "loss: 0.004903  [  100/ 1536]\n",
      "loss: 0.006758  [  200/ 1536]\n",
      "loss: 0.032179  [  300/ 1536]\n",
      "loss: 0.018409  [  400/ 1536]\n",
      "loss: 0.040667  [  500/ 1536]\n",
      "loss: 0.115233  [  600/ 1536]\n",
      "loss: 0.032058  [  700/ 1536]\n",
      "loss: 0.130987  [  800/ 1536]\n",
      "loss: 0.106292  [  900/ 1536]\n",
      "loss: 0.013400  [ 1000/ 1536]\n",
      "loss: 0.002875  [ 1100/ 1536]\n",
      "loss: 0.049574  [ 1200/ 1536]\n",
      "loss: 0.080781  [ 1300/ 1536]\n",
      "loss: 0.000002  [ 1400/ 1536]\n",
      "loss: 0.250032  [ 1500/ 1536]\n",
      "loss: 0.224878  [    0/ 1536]\n",
      "loss: 0.000168  [  100/ 1536]\n",
      "loss: 0.016653  [  200/ 1536]\n",
      "loss: 0.140511  [  300/ 1536]\n",
      "loss: 0.084966  [  400/ 1536]\n",
      "loss: 0.097253  [  500/ 1536]\n",
      "loss: 0.024576  [  600/ 1536]\n",
      "loss: 0.002557  [  700/ 1536]\n",
      "loss: 0.001500  [  800/ 1536]\n",
      "loss: 0.006384  [  900/ 1536]\n",
      "loss: 0.012184  [ 1000/ 1536]\n",
      "loss: 0.003628  [ 1100/ 1536]\n",
      "loss: 0.005688  [ 1200/ 1536]\n",
      "loss: 0.024131  [ 1300/ 1536]\n",
      "loss: 0.002108  [ 1400/ 1536]\n",
      "loss: 0.013992  [ 1500/ 1536]\n",
      "loss: 0.076986  [    0/ 1536]\n",
      "loss: 0.007915  [  100/ 1536]\n",
      "loss: 0.063730  [  200/ 1536]\n",
      "loss: 0.364602  [  300/ 1536]\n",
      "loss: 0.030355  [  400/ 1536]\n",
      "loss: 0.002036  [  500/ 1536]\n",
      "loss: 0.000543  [  600/ 1536]\n",
      "loss: 0.153322  [  700/ 1536]\n",
      "loss: 0.007715  [  800/ 1536]\n",
      "loss: 0.001776  [  900/ 1536]\n",
      "loss: 0.151936  [ 1000/ 1536]\n",
      "loss: 0.007705  [ 1100/ 1536]\n",
      "loss: 0.038859  [ 1200/ 1536]\n",
      "loss: 0.102071  [ 1300/ 1536]\n",
      "loss: 0.093563  [ 1400/ 1536]\n",
      "loss: 0.077443  [ 1500/ 1536]\n",
      "loss: 0.007803  [    0/ 1536]\n",
      "loss: 0.002043  [  100/ 1536]\n",
      "loss: 0.016831  [  200/ 1536]\n",
      "loss: 0.002717  [  300/ 1536]\n",
      "loss: 0.013371  [  400/ 1536]\n",
      "loss: 0.102766  [  500/ 1536]\n",
      "loss: 0.020002  [  600/ 1536]\n",
      "loss: 0.118482  [  700/ 1536]\n",
      "loss: 0.069563  [  800/ 1536]\n",
      "loss: 0.003662  [  900/ 1536]\n",
      "loss: 0.044984  [ 1000/ 1536]\n",
      "loss: 0.006465  [ 1100/ 1536]\n",
      "loss: 0.027688  [ 1200/ 1536]\n",
      "loss: 0.077994  [ 1300/ 1536]\n",
      "loss: 0.008120  [ 1400/ 1536]\n",
      "loss: 0.068016  [ 1500/ 1536]\n",
      "loss: 0.013103  [    0/ 1536]\n",
      "loss: 0.000900  [  100/ 1536]\n",
      "loss: 0.018635  [  200/ 1536]\n",
      "loss: 0.032549  [  300/ 1536]\n",
      "loss: 0.015906  [  400/ 1536]\n",
      "loss: 0.000212  [  500/ 1536]\n",
      "loss: 0.002193  [  600/ 1536]\n",
      "loss: 0.012372  [  700/ 1536]\n",
      "loss: 0.005175  [  800/ 1536]\n",
      "loss: 0.042942  [  900/ 1536]\n",
      "loss: 0.000764  [ 1000/ 1536]\n",
      "loss: 0.001678  [ 1100/ 1536]\n",
      "loss: 0.047732  [ 1200/ 1536]\n",
      "loss: 0.059260  [ 1300/ 1536]\n",
      "loss: 0.007565  [ 1400/ 1536]\n",
      "loss: 0.059758  [ 1500/ 1536]\n",
      "loss: 0.029072  [    0/ 1536]\n",
      "loss: 0.005536  [  100/ 1536]\n",
      "loss: 0.042974  [  200/ 1536]\n",
      "loss: 0.027416  [  300/ 1536]\n",
      "loss: 0.063254  [  400/ 1536]\n",
      "loss: 0.019286  [  500/ 1536]\n",
      "loss: 0.009645  [  600/ 1536]\n",
      "loss: 0.065082  [  700/ 1536]\n",
      "loss: 0.004803  [  800/ 1536]\n",
      "loss: 0.000196  [  900/ 1536]\n",
      "loss: 0.450316  [ 1000/ 1536]\n",
      "loss: 0.000168  [ 1100/ 1536]\n",
      "loss: 0.009687  [ 1200/ 1536]\n",
      "loss: 0.037228  [ 1300/ 1536]\n",
      "loss: 0.014469  [ 1400/ 1536]\n",
      "loss: 0.044000  [ 1500/ 1536]\n",
      "loss: 0.016150  [    0/ 1536]\n",
      "loss: 0.000955  [  100/ 1536]\n",
      "loss: 0.108946  [  200/ 1536]\n",
      "loss: 0.011598  [  300/ 1536]\n",
      "loss: 0.009354  [  400/ 1536]\n",
      "loss: 0.102985  [  500/ 1536]\n",
      "loss: 0.003884  [  600/ 1536]\n",
      "loss: 0.000555  [  700/ 1536]\n",
      "loss: 0.002406  [  800/ 1536]\n",
      "loss: 0.005436  [  900/ 1536]\n",
      "loss: 0.000456  [ 1000/ 1536]\n",
      "loss: 0.005254  [ 1100/ 1536]\n",
      "loss: 0.017478  [ 1200/ 1536]\n",
      "loss: 0.003439  [ 1300/ 1536]\n",
      "loss: 0.073572  [ 1400/ 1536]\n",
      "loss: 0.099782  [ 1500/ 1536]\n",
      "loss: 0.005414  [    0/ 1536]\n",
      "loss: 0.000093  [  100/ 1536]\n",
      "loss: 0.001419  [  200/ 1536]\n",
      "loss: 0.079134  [  300/ 1536]\n",
      "loss: 0.009571  [  400/ 1536]\n",
      "loss: 0.024516  [  500/ 1536]\n",
      "loss: 0.000049  [  600/ 1536]\n",
      "loss: 0.000014  [  700/ 1536]\n",
      "loss: 0.001027  [  800/ 1536]\n",
      "loss: 0.223601  [  900/ 1536]\n",
      "loss: 0.005588  [ 1000/ 1536]\n",
      "loss: 0.106391  [ 1100/ 1536]\n",
      "loss: 0.006945  [ 1200/ 1536]\n",
      "loss: 0.002098  [ 1300/ 1536]\n",
      "loss: 0.026258  [ 1400/ 1536]\n",
      "loss: 0.002879  [ 1500/ 1536]\n",
      "loss: 0.012228  [    0/ 1536]\n",
      "loss: 0.007401  [  100/ 1536]\n",
      "loss: 0.087196  [  200/ 1536]\n",
      "loss: 0.046476  [  300/ 1536]\n",
      "loss: 0.115050  [  400/ 1536]\n",
      "loss: 0.000684  [  500/ 1536]\n",
      "loss: 0.073614  [  600/ 1536]\n",
      "loss: 0.055056  [  700/ 1536]\n",
      "loss: 0.021958  [  800/ 1536]\n",
      "loss: 0.199444  [  900/ 1536]\n",
      "loss: 0.002854  [ 1000/ 1536]\n",
      "loss: 0.002659  [ 1100/ 1536]\n",
      "loss: 0.039901  [ 1200/ 1536]\n",
      "loss: 0.000912  [ 1300/ 1536]\n",
      "loss: 0.008752  [ 1400/ 1536]\n",
      "loss: 0.097463  [ 1500/ 1536]\n",
      "loss: 0.040085  [    0/ 1536]\n",
      "loss: 0.027640  [  100/ 1536]\n",
      "loss: 0.002253  [  200/ 1536]\n",
      "loss: 0.238748  [  300/ 1536]\n",
      "loss: 0.009083  [  400/ 1536]\n",
      "loss: 0.032871  [  500/ 1536]\n",
      "loss: 0.000390  [  600/ 1536]\n",
      "loss: 0.065724  [  700/ 1536]\n",
      "loss: 0.045107  [  800/ 1536]\n",
      "loss: 0.008446  [  900/ 1536]\n",
      "loss: 0.004419  [ 1000/ 1536]\n",
      "loss: 0.008093  [ 1100/ 1536]\n",
      "loss: 0.019301  [ 1200/ 1536]\n",
      "loss: 0.003947  [ 1300/ 1536]\n",
      "loss: 0.004587  [ 1400/ 1536]\n",
      "loss: 0.011554  [ 1500/ 1536]\n",
      "loss: 0.160266  [    0/ 1536]\n",
      "loss: 0.078354  [  100/ 1536]\n",
      "loss: 0.049502  [  200/ 1536]\n",
      "loss: 0.109380  [  300/ 1536]\n",
      "loss: 0.063979  [  400/ 1536]\n",
      "loss: 0.000471  [  500/ 1536]\n",
      "loss: 0.000281  [  600/ 1536]\n",
      "loss: 0.057568  [  700/ 1536]\n",
      "loss: 0.017458  [  800/ 1536]\n",
      "loss: 0.003998  [  900/ 1536]\n",
      "loss: 0.000001  [ 1000/ 1536]\n",
      "loss: 0.001024  [ 1100/ 1536]\n",
      "loss: 0.023678  [ 1200/ 1536]\n",
      "loss: 0.129142  [ 1300/ 1536]\n",
      "loss: 0.049946  [ 1400/ 1536]\n",
      "loss: 0.020456  [ 1500/ 1536]\n",
      "loss: 0.041785  [    0/ 1536]\n",
      "loss: 0.091406  [  100/ 1536]\n",
      "loss: 0.005121  [  200/ 1536]\n",
      "loss: 0.239490  [  300/ 1536]\n",
      "loss: 0.085570  [  400/ 1536]\n",
      "loss: 0.005687  [  500/ 1536]\n",
      "loss: 0.118477  [  600/ 1536]\n",
      "loss: 0.083567  [  700/ 1536]\n",
      "loss: 0.004740  [  800/ 1536]\n",
      "loss: 0.051889  [  900/ 1536]\n",
      "loss: 0.101020  [ 1000/ 1536]\n",
      "loss: 0.045208  [ 1100/ 1536]\n",
      "loss: 0.001035  [ 1200/ 1536]\n",
      "loss: 0.109923  [ 1300/ 1536]\n",
      "loss: 0.222532  [ 1400/ 1536]\n",
      "loss: 0.010904  [ 1500/ 1536]\n",
      "loss: 0.033622  [    0/ 1536]\n",
      "loss: 0.027888  [  100/ 1536]\n",
      "loss: 0.000189  [  200/ 1536]\n",
      "loss: 0.007510  [  300/ 1536]\n",
      "loss: 0.088804  [  400/ 1536]\n",
      "loss: 0.007569  [  500/ 1536]\n",
      "loss: 0.010461  [  600/ 1536]\n",
      "loss: 0.066718  [  700/ 1536]\n",
      "loss: 0.009658  [  800/ 1536]\n",
      "loss: 0.001296  [  900/ 1536]\n",
      "loss: 0.069746  [ 1000/ 1536]\n",
      "loss: 0.011195  [ 1100/ 1536]\n",
      "loss: 0.000801  [ 1200/ 1536]\n",
      "loss: 0.122232  [ 1300/ 1536]\n",
      "loss: 0.024684  [ 1400/ 1536]\n",
      "loss: 0.000142  [ 1500/ 1536]\n",
      "loss: 0.105994  [    0/ 1536]\n",
      "loss: 0.003380  [  100/ 1536]\n",
      "loss: 0.223837  [  200/ 1536]\n",
      "loss: 0.017902  [  300/ 1536]\n",
      "loss: 0.000577  [  400/ 1536]\n",
      "loss: 0.023644  [  500/ 1536]\n",
      "loss: 0.000019  [  600/ 1536]\n",
      "loss: 0.006525  [  700/ 1536]\n",
      "loss: 0.005528  [  800/ 1536]\n",
      "loss: 0.002669  [  900/ 1536]\n",
      "loss: 0.017788  [ 1000/ 1536]\n",
      "loss: 0.036603  [ 1100/ 1536]\n",
      "loss: 0.032341  [ 1200/ 1536]\n",
      "loss: 0.003807  [ 1300/ 1536]\n",
      "loss: 0.044740  [ 1400/ 1536]\n",
      "loss: 0.049644  [ 1500/ 1536]\n",
      "loss: 0.005890  [    0/ 1536]\n",
      "loss: 0.004257  [  100/ 1536]\n",
      "loss: 0.053977  [  200/ 1536]\n",
      "loss: 0.083308  [  300/ 1536]\n",
      "loss: 0.044473  [  400/ 1536]\n",
      "loss: 0.048643  [  500/ 1536]\n",
      "loss: 0.007040  [  600/ 1536]\n",
      "loss: 0.035117  [  700/ 1536]\n",
      "loss: 0.064372  [  800/ 1536]\n",
      "loss: 0.193128  [  900/ 1536]\n",
      "loss: 0.003128  [ 1000/ 1536]\n",
      "loss: 0.042342  [ 1100/ 1536]\n",
      "loss: 0.011022  [ 1200/ 1536]\n",
      "loss: 0.010883  [ 1300/ 1536]\n",
      "loss: 0.002991  [ 1400/ 1536]\n",
      "loss: 0.020939  [ 1500/ 1536]\n",
      "loss: 0.003327  [    0/ 1536]\n",
      "loss: 0.000494  [  100/ 1536]\n",
      "loss: 0.000167  [  200/ 1536]\n",
      "loss: 0.138853  [  300/ 1536]\n",
      "loss: 0.005536  [  400/ 1536]\n",
      "loss: 0.014086  [  500/ 1536]\n",
      "loss: 0.000394  [  600/ 1536]\n",
      "loss: 0.000040  [  700/ 1536]\n",
      "loss: 0.013679  [  800/ 1536]\n",
      "loss: 0.000390  [  900/ 1536]\n",
      "loss: 0.000713  [ 1000/ 1536]\n",
      "loss: 0.007968  [ 1100/ 1536]\n",
      "loss: 0.106083  [ 1200/ 1536]\n",
      "loss: 0.021245  [ 1300/ 1536]\n",
      "loss: 0.006866  [ 1400/ 1536]\n",
      "loss: 0.025528  [ 1500/ 1536]\n",
      "loss: 0.012834  [    0/ 1536]\n",
      "loss: 0.036264  [  100/ 1536]\n",
      "loss: 0.056078  [  200/ 1536]\n",
      "loss: 0.018047  [  300/ 1536]\n",
      "loss: 0.000619  [  400/ 1536]\n",
      "loss: 0.000033  [  500/ 1536]\n",
      "loss: 0.044966  [  600/ 1536]\n",
      "loss: 0.009395  [  700/ 1536]\n",
      "loss: 0.006638  [  800/ 1536]\n",
      "loss: 0.000174  [  900/ 1536]\n",
      "loss: 0.021687  [ 1000/ 1536]\n",
      "loss: 0.083722  [ 1100/ 1536]\n",
      "loss: 0.000673  [ 1200/ 1536]\n",
      "loss: 0.001387  [ 1300/ 1536]\n",
      "loss: 0.000545  [ 1400/ 1536]\n",
      "loss: 0.234539  [ 1500/ 1536]\n",
      "loss: 0.006695  [    0/ 1536]\n",
      "loss: 0.046361  [  100/ 1536]\n",
      "loss: 0.039738  [  200/ 1536]\n",
      "loss: 0.039773  [  300/ 1536]\n",
      "loss: 0.004740  [  400/ 1536]\n",
      "loss: 0.000088  [  500/ 1536]\n",
      "loss: 0.012235  [  600/ 1536]\n",
      "loss: 0.001376  [  700/ 1536]\n",
      "loss: 0.023088  [  800/ 1536]\n",
      "loss: 0.080605  [  900/ 1536]\n",
      "loss: 0.128828  [ 1000/ 1536]\n",
      "loss: 0.073361  [ 1100/ 1536]\n",
      "loss: 0.044411  [ 1200/ 1536]\n",
      "loss: 0.003510  [ 1300/ 1536]\n",
      "loss: 0.006307  [ 1400/ 1536]\n",
      "loss: 0.000194  [ 1500/ 1536]\n",
      "loss: 0.040215  [    0/ 1536]\n",
      "loss: 0.004041  [  100/ 1536]\n",
      "loss: 0.014445  [  200/ 1536]\n",
      "loss: 0.034978  [  300/ 1536]\n",
      "loss: 0.047074  [  400/ 1536]\n",
      "loss: 0.019678  [  500/ 1536]\n",
      "loss: 0.157728  [  600/ 1536]\n",
      "loss: 0.013118  [  700/ 1536]\n",
      "loss: 0.074010  [  800/ 1536]\n",
      "loss: 0.021703  [  900/ 1536]\n",
      "loss: 0.082113  [ 1000/ 1536]\n",
      "loss: 0.054254  [ 1100/ 1536]\n",
      "loss: 0.056899  [ 1200/ 1536]\n",
      "loss: 0.241690  [ 1300/ 1536]\n",
      "loss: 0.036709  [ 1400/ 1536]\n",
      "loss: 0.014758  [ 1500/ 1536]\n",
      "loss: 0.079181  [    0/ 1536]\n",
      "loss: 0.049467  [  100/ 1536]\n",
      "loss: 0.019307  [  200/ 1536]\n",
      "loss: 0.000957  [  300/ 1536]\n",
      "loss: 0.007878  [  400/ 1536]\n",
      "loss: 0.030506  [  500/ 1536]\n",
      "loss: 0.003960  [  600/ 1536]\n",
      "loss: 0.000005  [  700/ 1536]\n",
      "loss: 0.008753  [  800/ 1536]\n",
      "loss: 0.175324  [  900/ 1536]\n",
      "loss: 0.123270  [ 1000/ 1536]\n",
      "loss: 0.005889  [ 1100/ 1536]\n",
      "loss: 0.009444  [ 1200/ 1536]\n",
      "loss: 0.451086  [ 1300/ 1536]\n",
      "loss: 0.021760  [ 1400/ 1536]\n",
      "loss: 0.040844  [ 1500/ 1536]\n",
      "loss: 0.100828  [    0/ 1536]\n",
      "loss: 0.003289  [  100/ 1536]\n",
      "loss: 0.002498  [  200/ 1536]\n",
      "loss: 0.043140  [  300/ 1536]\n",
      "loss: 0.000004  [  400/ 1536]\n",
      "loss: 0.172224  [  500/ 1536]\n",
      "loss: 0.026059  [  600/ 1536]\n",
      "loss: 0.013984  [  700/ 1536]\n",
      "loss: 0.001692  [  800/ 1536]\n",
      "loss: 0.009269  [  900/ 1536]\n",
      "loss: 0.000158  [ 1000/ 1536]\n",
      "loss: 0.000010  [ 1100/ 1536]\n",
      "loss: 0.010079  [ 1200/ 1536]\n",
      "loss: 0.016340  [ 1300/ 1536]\n",
      "loss: 0.002326  [ 1400/ 1536]\n",
      "loss: 0.000058  [ 1500/ 1536]\n",
      "loss: 0.000827  [    0/ 1536]\n",
      "loss: 0.034070  [  100/ 1536]\n",
      "loss: 0.056680  [  200/ 1536]\n",
      "loss: 0.036747  [  300/ 1536]\n",
      "loss: 0.000613  [  400/ 1536]\n",
      "loss: 0.003831  [  500/ 1536]\n",
      "loss: 0.012414  [  600/ 1536]\n",
      "loss: 0.082062  [  700/ 1536]\n",
      "loss: 0.057872  [  800/ 1536]\n",
      "loss: 0.000994  [  900/ 1536]\n",
      "loss: 0.067950  [ 1000/ 1536]\n",
      "loss: 0.007915  [ 1100/ 1536]\n",
      "loss: 0.062842  [ 1200/ 1536]\n",
      "loss: 0.033858  [ 1300/ 1536]\n",
      "loss: 0.008403  [ 1400/ 1536]\n",
      "loss: 0.000360  [ 1500/ 1536]\n",
      "loss: 0.114639  [    0/ 1536]\n",
      "loss: 0.003902  [  100/ 1536]\n",
      "loss: 0.007977  [  200/ 1536]\n",
      "loss: 0.000526  [  300/ 1536]\n",
      "loss: 0.070385  [  400/ 1536]\n",
      "loss: 0.019385  [  500/ 1536]\n",
      "loss: 0.002212  [  600/ 1536]\n",
      "loss: 0.004328  [  700/ 1536]\n",
      "loss: 0.075972  [  800/ 1536]\n",
      "loss: 0.009768  [  900/ 1536]\n",
      "loss: 0.065409  [ 1000/ 1536]\n",
      "loss: 0.014064  [ 1100/ 1536]\n",
      "loss: 0.001609  [ 1200/ 1536]\n",
      "loss: 0.018636  [ 1300/ 1536]\n",
      "loss: 0.197681  [ 1400/ 1536]\n",
      "loss: 0.001961  [ 1500/ 1536]\n",
      "loss: 0.095555  [    0/ 1536]\n",
      "loss: 0.007228  [  100/ 1536]\n",
      "loss: 0.143490  [  200/ 1536]\n",
      "loss: 0.105413  [  300/ 1536]\n",
      "loss: 0.004172  [  400/ 1536]\n",
      "loss: 0.035632  [  500/ 1536]\n",
      "loss: 0.019939  [  600/ 1536]\n",
      "loss: 0.052154  [  700/ 1536]\n",
      "loss: 0.036165  [  800/ 1536]\n",
      "loss: 0.000540  [  900/ 1536]\n",
      "loss: 0.215890  [ 1000/ 1536]\n",
      "loss: 0.008520  [ 1100/ 1536]\n",
      "loss: 0.027458  [ 1200/ 1536]\n",
      "loss: 0.212743  [ 1300/ 1536]\n",
      "loss: 0.052745  [ 1400/ 1536]\n",
      "loss: 0.015757  [ 1500/ 1536]\n",
      "loss: 0.006946  [    0/ 1536]\n",
      "loss: 0.000422  [  100/ 1536]\n",
      "loss: 0.060035  [  200/ 1536]\n",
      "loss: 0.031837  [  300/ 1536]\n",
      "loss: 0.068221  [  400/ 1536]\n",
      "loss: 0.000764  [  500/ 1536]\n",
      "loss: 0.126508  [  600/ 1536]\n",
      "loss: 0.000103  [  700/ 1536]\n",
      "loss: 0.005176  [  800/ 1536]\n",
      "loss: 0.003655  [  900/ 1536]\n",
      "loss: 0.098675  [ 1000/ 1536]\n",
      "loss: 0.011321  [ 1100/ 1536]\n",
      "loss: 0.000236  [ 1200/ 1536]\n",
      "loss: 0.020801  [ 1300/ 1536]\n",
      "loss: 0.066686  [ 1400/ 1536]\n",
      "loss: 0.050151  [ 1500/ 1536]\n",
      "loss: 0.000927  [    0/ 1536]\n",
      "loss: 0.101316  [  100/ 1536]\n",
      "loss: 0.013532  [  200/ 1536]\n",
      "loss: 0.022456  [  300/ 1536]\n",
      "loss: 0.001558  [  400/ 1536]\n",
      "loss: 0.016981  [  500/ 1536]\n",
      "loss: 0.191649  [  600/ 1536]\n",
      "loss: 0.000041  [  700/ 1536]\n",
      "loss: 0.001179  [  800/ 1536]\n",
      "loss: 0.002576  [  900/ 1536]\n",
      "loss: 0.099202  [ 1000/ 1536]\n",
      "loss: 0.040969  [ 1100/ 1536]\n",
      "loss: 0.052711  [ 1200/ 1536]\n",
      "loss: 0.053087  [ 1300/ 1536]\n",
      "loss: 0.081973  [ 1400/ 1536]\n",
      "loss: 0.123244  [ 1500/ 1536]\n",
      "loss: 0.039796  [    0/ 1536]\n",
      "loss: 0.012615  [  100/ 1536]\n",
      "loss: 0.003398  [  200/ 1536]\n",
      "loss: 0.013983  [  300/ 1536]\n",
      "loss: 0.160279  [  400/ 1536]\n",
      "loss: 0.004053  [  500/ 1536]\n",
      "loss: 0.006844  [  600/ 1536]\n",
      "loss: 0.053950  [  700/ 1536]\n",
      "loss: 0.000203  [  800/ 1536]\n",
      "loss: 0.009012  [  900/ 1536]\n",
      "loss: 0.024967  [ 1000/ 1536]\n",
      "loss: 0.210819  [ 1100/ 1536]\n",
      "loss: 0.107065  [ 1200/ 1536]\n",
      "loss: 0.100689  [ 1300/ 1536]\n",
      "loss: 0.002582  [ 1400/ 1536]\n",
      "loss: 0.007167  [ 1500/ 1536]\n",
      "loss: 0.001587  [    0/ 1536]\n",
      "loss: 0.000236  [  100/ 1536]\n",
      "loss: 0.057474  [  200/ 1536]\n",
      "loss: 0.083597  [  300/ 1536]\n",
      "loss: 0.012618  [  400/ 1536]\n",
      "loss: 0.010711  [  500/ 1536]\n",
      "loss: 0.003441  [  600/ 1536]\n",
      "loss: 0.014609  [  700/ 1536]\n",
      "loss: 0.117461  [  800/ 1536]\n",
      "loss: 0.000844  [  900/ 1536]\n",
      "loss: 0.001093  [ 1000/ 1536]\n",
      "loss: 0.103140  [ 1100/ 1536]\n",
      "loss: 0.008411  [ 1200/ 1536]\n",
      "loss: 0.011492  [ 1300/ 1536]\n",
      "loss: 0.004370  [ 1400/ 1536]\n",
      "loss: 0.074280  [ 1500/ 1536]\n",
      "loss: 0.000002  [    0/ 1536]\n",
      "loss: 0.008928  [  100/ 1536]\n",
      "loss: 0.000022  [  200/ 1536]\n",
      "loss: 0.011035  [  300/ 1536]\n",
      "loss: 0.033809  [  400/ 1536]\n",
      "loss: 0.010663  [  500/ 1536]\n",
      "loss: 0.054854  [  600/ 1536]\n",
      "loss: 0.137169  [  700/ 1536]\n",
      "loss: 0.046663  [  800/ 1536]\n",
      "loss: 0.000189  [  900/ 1536]\n",
      "loss: 0.002365  [ 1000/ 1536]\n",
      "loss: 0.005512  [ 1100/ 1536]\n",
      "loss: 0.002209  [ 1200/ 1536]\n",
      "loss: 0.164773  [ 1300/ 1536]\n",
      "loss: 0.002059  [ 1400/ 1536]\n",
      "loss: 0.002072  [ 1500/ 1536]\n",
      "loss: 0.000408  [    0/ 1536]\n",
      "loss: 0.056632  [  100/ 1536]\n",
      "loss: 0.028771  [  200/ 1536]\n",
      "loss: 0.006519  [  300/ 1536]\n",
      "loss: 0.000580  [  400/ 1536]\n",
      "loss: 0.007981  [  500/ 1536]\n",
      "loss: 0.084980  [  600/ 1536]\n",
      "loss: 0.017270  [  700/ 1536]\n",
      "loss: 0.012790  [  800/ 1536]\n",
      "loss: 0.002082  [  900/ 1536]\n",
      "loss: 0.018497  [ 1000/ 1536]\n",
      "loss: 0.015311  [ 1100/ 1536]\n",
      "loss: 0.011599  [ 1200/ 1536]\n",
      "loss: 0.000127  [ 1300/ 1536]\n",
      "loss: 0.000183  [ 1400/ 1536]\n",
      "loss: 0.009379  [ 1500/ 1536]\n",
      "loss: 0.145254  [    0/ 1536]\n",
      "loss: 0.177604  [  100/ 1536]\n",
      "loss: 0.046628  [  200/ 1536]\n",
      "loss: 0.011301  [  300/ 1536]\n",
      "loss: 0.026832  [  400/ 1536]\n",
      "loss: 0.011392  [  500/ 1536]\n",
      "loss: 0.079001  [  600/ 1536]\n",
      "loss: 0.001591  [  700/ 1536]\n",
      "loss: 0.028160  [  800/ 1536]\n",
      "loss: 0.050282  [  900/ 1536]\n",
      "loss: 0.101982  [ 1000/ 1536]\n",
      "loss: 0.079812  [ 1100/ 1536]\n",
      "loss: 0.009785  [ 1200/ 1536]\n",
      "loss: 0.001900  [ 1300/ 1536]\n",
      "loss: 0.031618  [ 1400/ 1536]\n",
      "loss: 0.007486  [ 1500/ 1536]\n",
      "loss: 0.084287  [    0/ 1536]\n",
      "loss: 0.004098  [  100/ 1536]\n",
      "loss: 0.002703  [  200/ 1536]\n",
      "loss: 0.010129  [  300/ 1536]\n",
      "loss: 0.005162  [  400/ 1536]\n",
      "loss: 0.046646  [  500/ 1536]\n",
      "loss: 0.010931  [  600/ 1536]\n",
      "loss: 0.033714  [  700/ 1536]\n",
      "loss: 0.042090  [  800/ 1536]\n",
      "loss: 0.002294  [  900/ 1536]\n",
      "loss: 0.000345  [ 1000/ 1536]\n",
      "loss: 0.001328  [ 1100/ 1536]\n",
      "loss: 0.156169  [ 1200/ 1536]\n",
      "loss: 0.083720  [ 1300/ 1536]\n",
      "loss: 0.030246  [ 1400/ 1536]\n",
      "loss: 0.052053  [ 1500/ 1536]\n",
      "loss: 0.001177  [    0/ 1536]\n",
      "loss: 0.013924  [  100/ 1536]\n",
      "loss: 0.001132  [  200/ 1536]\n",
      "loss: 0.066247  [  300/ 1536]\n",
      "loss: 0.051443  [  400/ 1536]\n",
      "loss: 0.102376  [  500/ 1536]\n",
      "loss: 0.003249  [  600/ 1536]\n",
      "loss: 0.265591  [  700/ 1536]\n",
      "loss: 0.100828  [  800/ 1536]\n",
      "loss: 0.000562  [  900/ 1536]\n",
      "loss: 0.001969  [ 1000/ 1536]\n",
      "loss: 0.002124  [ 1100/ 1536]\n",
      "loss: 0.009014  [ 1200/ 1536]\n",
      "loss: 0.057842  [ 1300/ 1536]\n",
      "loss: 0.047260  [ 1400/ 1536]\n",
      "loss: 0.044541  [ 1500/ 1536]\n",
      "loss: 0.011217  [    0/ 1536]\n",
      "loss: 0.192874  [  100/ 1536]\n",
      "loss: 0.079748  [  200/ 1536]\n",
      "loss: 0.283605  [  300/ 1536]\n",
      "loss: 0.001128  [  400/ 1536]\n",
      "loss: 0.061999  [  500/ 1536]\n",
      "loss: 0.096838  [  600/ 1536]\n",
      "loss: 0.100669  [  700/ 1536]\n",
      "loss: 0.007721  [  800/ 1536]\n",
      "loss: 0.028499  [  900/ 1536]\n",
      "loss: 0.032406  [ 1000/ 1536]\n",
      "loss: 0.026496  [ 1100/ 1536]\n",
      "loss: 0.000683  [ 1200/ 1536]\n",
      "loss: 0.013279  [ 1300/ 1536]\n",
      "loss: 0.017136  [ 1400/ 1536]\n",
      "loss: 0.000847  [ 1500/ 1536]\n",
      "loss: 0.395764  [    0/ 1536]\n",
      "loss: 0.005098  [  100/ 1536]\n",
      "loss: 0.044641  [  200/ 1536]\n",
      "loss: 0.034744  [  300/ 1536]\n",
      "loss: 0.000905  [  400/ 1536]\n",
      "loss: 0.094961  [  500/ 1536]\n",
      "loss: 0.036612  [  600/ 1536]\n",
      "loss: 0.000052  [  700/ 1536]\n",
      "loss: 0.111484  [  800/ 1536]\n",
      "loss: 0.037854  [  900/ 1536]\n",
      "loss: 0.010737  [ 1000/ 1536]\n",
      "loss: 0.075892  [ 1100/ 1536]\n",
      "loss: 0.007109  [ 1200/ 1536]\n",
      "loss: 0.001153  [ 1300/ 1536]\n",
      "loss: 0.152498  [ 1400/ 1536]\n",
      "loss: 0.004919  [ 1500/ 1536]\n",
      "loss: 0.018228  [    0/ 1536]\n",
      "loss: 0.029848  [  100/ 1536]\n",
      "loss: 0.018336  [  200/ 1536]\n",
      "loss: 0.012087  [  300/ 1536]\n",
      "loss: 0.000966  [  400/ 1536]\n",
      "loss: 0.005021  [  500/ 1536]\n",
      "loss: 0.029761  [  600/ 1536]\n",
      "loss: 0.035618  [  700/ 1536]\n",
      "loss: 0.145706  [  800/ 1536]\n",
      "loss: 0.013547  [  900/ 1536]\n",
      "loss: 0.101995  [ 1000/ 1536]\n",
      "loss: 0.000066  [ 1100/ 1536]\n",
      "loss: 0.002286  [ 1200/ 1536]\n",
      "loss: 0.001904  [ 1300/ 1536]\n",
      "loss: 0.243362  [ 1400/ 1536]\n",
      "loss: 0.012868  [ 1500/ 1536]\n",
      "loss: 0.071751  [    0/ 1536]\n",
      "loss: 0.000903  [  100/ 1536]\n",
      "loss: 0.048993  [  200/ 1536]\n",
      "loss: 0.029841  [  300/ 1536]\n",
      "loss: 0.006607  [  400/ 1536]\n",
      "loss: 0.035014  [  500/ 1536]\n",
      "loss: 0.073771  [  600/ 1536]\n",
      "loss: 0.004077  [  700/ 1536]\n",
      "loss: 0.084692  [  800/ 1536]\n",
      "loss: 0.046332  [  900/ 1536]\n",
      "loss: 0.005646  [ 1000/ 1536]\n",
      "loss: 0.041832  [ 1100/ 1536]\n",
      "loss: 0.067145  [ 1200/ 1536]\n",
      "loss: 0.072746  [ 1300/ 1536]\n",
      "loss: 0.111786  [ 1400/ 1536]\n",
      "loss: 0.000000  [ 1500/ 1536]\n",
      "loss: 0.000067  [    0/ 1536]\n",
      "loss: 0.039307  [  100/ 1536]\n",
      "loss: 0.004310  [  200/ 1536]\n",
      "loss: 0.027586  [  300/ 1536]\n",
      "loss: 0.006317  [  400/ 1536]\n",
      "loss: 0.010981  [  500/ 1536]\n",
      "loss: 0.035373  [  600/ 1536]\n",
      "loss: 0.158185  [  700/ 1536]\n",
      "loss: 0.000077  [  800/ 1536]\n",
      "loss: 0.013690  [  900/ 1536]\n",
      "loss: 0.064435  [ 1000/ 1536]\n",
      "loss: 0.018494  [ 1100/ 1536]\n",
      "loss: 0.023202  [ 1200/ 1536]\n",
      "loss: 0.000028  [ 1300/ 1536]\n",
      "loss: 0.004819  [ 1400/ 1536]\n",
      "loss: 0.024506  [ 1500/ 1536]\n",
      "loss: 0.014539  [    0/ 1536]\n",
      "loss: 0.000506  [  100/ 1536]\n",
      "loss: 0.008143  [  200/ 1536]\n",
      "loss: 0.001466  [  300/ 1536]\n",
      "loss: 0.000005  [  400/ 1536]\n",
      "loss: 0.041340  [  500/ 1536]\n",
      "loss: 0.000314  [  600/ 1536]\n",
      "loss: 0.004702  [  700/ 1536]\n",
      "loss: 0.026225  [  800/ 1536]\n",
      "loss: 0.125003  [  900/ 1536]\n",
      "loss: 0.096433  [ 1000/ 1536]\n",
      "loss: 0.083550  [ 1100/ 1536]\n",
      "loss: 0.004053  [ 1200/ 1536]\n",
      "loss: 0.028623  [ 1300/ 1536]\n",
      "loss: 0.112529  [ 1400/ 1536]\n",
      "loss: 0.008466  [ 1500/ 1536]\n",
      "loss: 0.099635  [    0/ 1536]\n",
      "loss: 0.001568  [  100/ 1536]\n",
      "loss: 0.021963  [  200/ 1536]\n",
      "loss: 0.000351  [  300/ 1536]\n",
      "loss: 0.162347  [  400/ 1536]\n",
      "loss: 0.003025  [  500/ 1536]\n",
      "loss: 0.000682  [  600/ 1536]\n",
      "loss: 0.049685  [  700/ 1536]\n",
      "loss: 0.007216  [  800/ 1536]\n",
      "loss: 0.002527  [  900/ 1536]\n",
      "loss: 0.052977  [ 1000/ 1536]\n",
      "loss: 0.035732  [ 1100/ 1536]\n",
      "loss: 0.040068  [ 1200/ 1536]\n",
      "loss: 0.127859  [ 1300/ 1536]\n",
      "loss: 0.007065  [ 1400/ 1536]\n",
      "loss: 0.069888  [ 1500/ 1536]\n",
      "loss: 0.026423  [    0/ 1536]\n",
      "loss: 0.207105  [  100/ 1536]\n",
      "loss: 0.024318  [  200/ 1536]\n",
      "loss: 0.004184  [  300/ 1536]\n",
      "loss: 0.005534  [  400/ 1536]\n",
      "loss: 0.008341  [  500/ 1536]\n",
      "loss: 0.083259  [  600/ 1536]\n",
      "loss: 0.001094  [  700/ 1536]\n",
      "loss: 0.000271  [  800/ 1536]\n",
      "loss: 0.000994  [  900/ 1536]\n",
      "loss: 0.059609  [ 1000/ 1536]\n",
      "loss: 0.102671  [ 1100/ 1536]\n",
      "loss: 0.026882  [ 1200/ 1536]\n",
      "loss: 0.003797  [ 1300/ 1536]\n",
      "loss: 0.002082  [ 1400/ 1536]\n",
      "loss: 0.065413  [ 1500/ 1536]\n",
      "loss: 0.000018  [    0/ 1536]\n",
      "loss: 0.029167  [  100/ 1536]\n",
      "loss: 0.000984  [  200/ 1536]\n",
      "loss: 0.002214  [  300/ 1536]\n",
      "loss: 0.150369  [  400/ 1536]\n",
      "loss: 0.004775  [  500/ 1536]\n",
      "loss: 0.002449  [  600/ 1536]\n",
      "loss: 0.008113  [  700/ 1536]\n",
      "loss: 0.002104  [  800/ 1536]\n",
      "loss: 0.010671  [  900/ 1536]\n",
      "loss: 0.003997  [ 1000/ 1536]\n",
      "loss: 0.210316  [ 1100/ 1536]\n",
      "loss: 0.023694  [ 1200/ 1536]\n",
      "loss: 0.001636  [ 1300/ 1536]\n",
      "loss: 0.011360  [ 1400/ 1536]\n",
      "loss: 0.064177  [ 1500/ 1536]\n",
      "loss: 0.005603  [    0/ 1536]\n",
      "loss: 0.006166  [  100/ 1536]\n",
      "loss: 0.111049  [  200/ 1536]\n",
      "loss: 0.080923  [  300/ 1536]\n",
      "loss: 0.006556  [  400/ 1536]\n",
      "loss: 0.006772  [  500/ 1536]\n",
      "loss: 0.056749  [  600/ 1536]\n",
      "loss: 0.098678  [  700/ 1536]\n",
      "loss: 0.006710  [  800/ 1536]\n",
      "loss: 0.063165  [  900/ 1536]\n",
      "loss: 0.002169  [ 1000/ 1536]\n",
      "loss: 0.012515  [ 1100/ 1536]\n",
      "loss: 0.081035  [ 1200/ 1536]\n",
      "loss: 0.046009  [ 1300/ 1536]\n",
      "loss: 0.001529  [ 1400/ 1536]\n",
      "loss: 0.002858  [ 1500/ 1536]\n",
      "loss: 0.007590  [    0/ 1536]\n",
      "loss: 0.000210  [  100/ 1536]\n",
      "loss: 0.007868  [  200/ 1536]\n",
      "loss: 0.020390  [  300/ 1536]\n",
      "loss: 0.109104  [  400/ 1536]\n",
      "loss: 0.009258  [  500/ 1536]\n",
      "loss: 0.000043  [  600/ 1536]\n",
      "loss: 0.022486  [  700/ 1536]\n",
      "loss: 0.000204  [  800/ 1536]\n",
      "loss: 0.028365  [  900/ 1536]\n",
      "loss: 0.037400  [ 1000/ 1536]\n",
      "loss: 0.006069  [ 1100/ 1536]\n",
      "loss: 0.012737  [ 1200/ 1536]\n",
      "loss: 0.004348  [ 1300/ 1536]\n",
      "loss: 0.000002  [ 1400/ 1536]\n",
      "loss: 0.015666  [ 1500/ 1536]\n",
      "loss: 0.000004  [    0/ 1536]\n",
      "loss: 0.002185  [  100/ 1536]\n",
      "loss: 0.222648  [  200/ 1536]\n",
      "loss: 0.006863  [  300/ 1536]\n",
      "loss: 0.039603  [  400/ 1536]\n",
      "loss: 0.046694  [  500/ 1536]\n",
      "loss: 0.006620  [  600/ 1536]\n",
      "loss: 0.086207  [  700/ 1536]\n",
      "loss: 0.168627  [  800/ 1536]\n",
      "loss: 0.000594  [  900/ 1536]\n",
      "loss: 0.009718  [ 1000/ 1536]\n",
      "loss: 0.014504  [ 1100/ 1536]\n",
      "loss: 0.069385  [ 1200/ 1536]\n",
      "loss: 0.024582  [ 1300/ 1536]\n",
      "loss: 0.007305  [ 1400/ 1536]\n",
      "loss: 0.000161  [ 1500/ 1536]\n",
      "loss: 0.147200  [    0/ 1536]\n",
      "loss: 0.000985  [  100/ 1536]\n",
      "loss: 0.000146  [  200/ 1536]\n",
      "loss: 0.156811  [  300/ 1536]\n",
      "loss: 0.000827  [  400/ 1536]\n",
      "loss: 0.003068  [  500/ 1536]\n",
      "loss: 0.000000  [  600/ 1536]\n",
      "loss: 0.028176  [  700/ 1536]\n",
      "loss: 0.002851  [  800/ 1536]\n",
      "loss: 0.090563  [  900/ 1536]\n",
      "loss: 0.025342  [ 1000/ 1536]\n",
      "loss: 0.027859  [ 1100/ 1536]\n",
      "loss: 0.000713  [ 1200/ 1536]\n",
      "loss: 0.050863  [ 1300/ 1536]\n",
      "loss: 0.017796  [ 1400/ 1536]\n",
      "loss: 0.047041  [ 1500/ 1536]\n",
      "loss: 0.000038  [    0/ 1536]\n",
      "loss: 0.000029  [  100/ 1536]\n",
      "loss: 0.000321  [  200/ 1536]\n",
      "loss: 0.094042  [  300/ 1536]\n",
      "loss: 0.025309  [  400/ 1536]\n",
      "loss: 0.000011  [  500/ 1536]\n",
      "loss: 0.000372  [  600/ 1536]\n",
      "loss: 0.018779  [  700/ 1536]\n",
      "loss: 0.000003  [  800/ 1536]\n",
      "loss: 0.216545  [  900/ 1536]\n",
      "loss: 0.066463  [ 1000/ 1536]\n",
      "loss: 0.006226  [ 1100/ 1536]\n",
      "loss: 0.132935  [ 1200/ 1536]\n",
      "loss: 0.003180  [ 1300/ 1536]\n",
      "loss: 0.031697  [ 1400/ 1536]\n",
      "loss: 0.001326  [ 1500/ 1536]\n",
      "loss: 0.011547  [    0/ 1536]\n",
      "loss: 0.060188  [  100/ 1536]\n",
      "loss: 0.051213  [  200/ 1536]\n",
      "loss: 0.000152  [  300/ 1536]\n",
      "loss: 0.031035  [  400/ 1536]\n",
      "loss: 0.002571  [  500/ 1536]\n",
      "loss: 0.014136  [  600/ 1536]\n",
      "loss: 0.013434  [  700/ 1536]\n",
      "loss: 0.025500  [  800/ 1536]\n",
      "loss: 0.078104  [  900/ 1536]\n",
      "loss: 0.000634  [ 1000/ 1536]\n",
      "loss: 0.016759  [ 1100/ 1536]\n",
      "loss: 0.020549  [ 1200/ 1536]\n",
      "loss: 0.014992  [ 1300/ 1536]\n",
      "loss: 0.009105  [ 1400/ 1536]\n",
      "loss: 0.060581  [ 1500/ 1536]\n",
      "loss: 0.007008  [    0/ 1536]\n",
      "loss: 0.059190  [  100/ 1536]\n",
      "loss: 0.014484  [  200/ 1536]\n",
      "loss: 0.325086  [  300/ 1536]\n",
      "loss: 0.000007  [  400/ 1536]\n",
      "loss: 0.019233  [  500/ 1536]\n",
      "loss: 0.004102  [  600/ 1536]\n",
      "loss: 0.063858  [  700/ 1536]\n",
      "loss: 0.053694  [  800/ 1536]\n",
      "loss: 0.003392  [  900/ 1536]\n",
      "loss: 0.002343  [ 1000/ 1536]\n",
      "loss: 0.000009  [ 1100/ 1536]\n",
      "loss: 0.002565  [ 1200/ 1536]\n",
      "loss: 0.176440  [ 1300/ 1536]\n",
      "loss: 0.001579  [ 1400/ 1536]\n",
      "loss: 0.005055  [ 1500/ 1536]\n",
      "loss: 0.012854  [    0/ 1536]\n",
      "loss: 0.016840  [  100/ 1536]\n",
      "loss: 0.007374  [  200/ 1536]\n",
      "loss: 0.038855  [  300/ 1536]\n",
      "loss: 0.058401  [  400/ 1536]\n",
      "loss: 0.238738  [  500/ 1536]\n",
      "loss: 0.000210  [  600/ 1536]\n",
      "loss: 0.125283  [  700/ 1536]\n",
      "loss: 0.000462  [  800/ 1536]\n",
      "loss: 0.266719  [  900/ 1536]\n",
      "loss: 0.006785  [ 1000/ 1536]\n",
      "loss: 0.025588  [ 1100/ 1536]\n",
      "loss: 0.065977  [ 1200/ 1536]\n",
      "loss: 0.004789  [ 1300/ 1536]\n",
      "loss: 0.050285  [ 1400/ 1536]\n",
      "loss: 0.034234  [ 1500/ 1536]\n",
      "loss: 0.187923  [    0/ 1536]\n",
      "loss: 0.000116  [  100/ 1536]\n",
      "loss: 0.000311  [  200/ 1536]\n",
      "loss: 0.000410  [  300/ 1536]\n",
      "loss: 0.042743  [  400/ 1536]\n",
      "loss: 0.168690  [  500/ 1536]\n",
      "loss: 0.032962  [  600/ 1536]\n",
      "loss: 0.002196  [  700/ 1536]\n",
      "loss: 0.139550  [  800/ 1536]\n",
      "loss: 0.000045  [  900/ 1536]\n",
      "loss: 0.000152  [ 1000/ 1536]\n",
      "loss: 0.020767  [ 1100/ 1536]\n",
      "loss: 0.000001  [ 1200/ 1536]\n",
      "loss: 0.026070  [ 1300/ 1536]\n",
      "loss: 0.055193  [ 1400/ 1536]\n",
      "loss: 0.000859  [ 1500/ 1536]\n",
      "loss: 0.144083  [    0/ 1536]\n",
      "loss: 0.000026  [  100/ 1536]\n",
      "loss: 0.045398  [  200/ 1536]\n",
      "loss: 0.000004  [  300/ 1536]\n",
      "loss: 0.015871  [  400/ 1536]\n",
      "loss: 0.076047  [  500/ 1536]\n",
      "loss: 0.054636  [  600/ 1536]\n",
      "loss: 0.000010  [  700/ 1536]\n",
      "loss: 0.002664  [  800/ 1536]\n",
      "loss: 0.002215  [  900/ 1536]\n",
      "loss: 0.034083  [ 1000/ 1536]\n",
      "loss: 0.131264  [ 1100/ 1536]\n",
      "loss: 0.000435  [ 1200/ 1536]\n",
      "loss: 0.000241  [ 1300/ 1536]\n",
      "loss: 0.005235  [ 1400/ 1536]\n",
      "loss: 0.005126  [ 1500/ 1536]\n",
      "loss: 0.008657  [    0/ 1536]\n",
      "loss: 0.000113  [  100/ 1536]\n",
      "loss: 0.010167  [  200/ 1536]\n",
      "loss: 0.034885  [  300/ 1536]\n",
      "loss: 0.004173  [  400/ 1536]\n",
      "loss: 0.015984  [  500/ 1536]\n",
      "loss: 0.067559  [  600/ 1536]\n",
      "loss: 0.000033  [  700/ 1536]\n",
      "loss: 0.002279  [  800/ 1536]\n",
      "loss: 0.000753  [  900/ 1536]\n",
      "loss: 0.265936  [ 1000/ 1536]\n",
      "loss: 0.026864  [ 1100/ 1536]\n",
      "loss: 0.026072  [ 1200/ 1536]\n",
      "loss: 0.106423  [ 1300/ 1536]\n",
      "loss: 0.018348  [ 1400/ 1536]\n",
      "loss: 0.248748  [ 1500/ 1536]\n",
      "loss: 0.092133  [    0/ 1536]\n",
      "loss: 0.000003  [  100/ 1536]\n",
      "loss: 0.033150  [  200/ 1536]\n",
      "loss: 0.016669  [  300/ 1536]\n",
      "loss: 0.164402  [  400/ 1536]\n",
      "loss: 0.108445  [  500/ 1536]\n",
      "loss: 0.025536  [  600/ 1536]\n",
      "loss: 0.082321  [  700/ 1536]\n",
      "loss: 0.113026  [  800/ 1536]\n",
      "loss: 0.011498  [  900/ 1536]\n",
      "loss: 0.000492  [ 1000/ 1536]\n",
      "loss: 0.004190  [ 1100/ 1536]\n",
      "loss: 0.031503  [ 1200/ 1536]\n",
      "loss: 0.028466  [ 1300/ 1536]\n",
      "loss: 0.035857  [ 1400/ 1536]\n",
      "loss: 0.005944  [ 1500/ 1536]\n",
      "loss: 0.019496  [    0/ 1536]\n",
      "loss: 0.009833  [  100/ 1536]\n",
      "loss: 0.059767  [  200/ 1536]\n",
      "loss: 0.010331  [  300/ 1536]\n",
      "loss: 0.018542  [  400/ 1536]\n",
      "loss: 0.002093  [  500/ 1536]\n",
      "loss: 0.014018  [  600/ 1536]\n",
      "loss: 0.000273  [  700/ 1536]\n",
      "loss: 0.073055  [  800/ 1536]\n",
      "loss: 0.003288  [  900/ 1536]\n",
      "loss: 0.039102  [ 1000/ 1536]\n",
      "loss: 0.020492  [ 1100/ 1536]\n",
      "loss: 0.019925  [ 1200/ 1536]\n",
      "loss: 0.041264  [ 1300/ 1536]\n",
      "loss: 0.004602  [ 1400/ 1536]\n",
      "loss: 0.157757  [ 1500/ 1536]\n",
      "loss: 0.002314  [    0/ 1536]\n",
      "loss: 0.005781  [  100/ 1536]\n",
      "loss: 0.030374  [  200/ 1536]\n",
      "loss: 0.054857  [  300/ 1536]\n",
      "loss: 0.081711  [  400/ 1536]\n",
      "loss: 0.092614  [  500/ 1536]\n",
      "loss: 0.043818  [  600/ 1536]\n",
      "loss: 0.021361  [  700/ 1536]\n",
      "loss: 0.014860  [  800/ 1536]\n",
      "loss: 0.005403  [  900/ 1536]\n",
      "loss: 0.067929  [ 1000/ 1536]\n",
      "loss: 0.006505  [ 1100/ 1536]\n",
      "loss: 0.217611  [ 1200/ 1536]\n",
      "loss: 0.000159  [ 1300/ 1536]\n",
      "loss: 0.032314  [ 1400/ 1536]\n",
      "loss: 0.157824  [ 1500/ 1536]\n",
      "loss: 0.137965  [    0/ 1536]\n",
      "loss: 0.052646  [  100/ 1536]\n",
      "loss: 0.000808  [  200/ 1536]\n",
      "loss: 0.228938  [  300/ 1536]\n",
      "loss: 0.033238  [  400/ 1536]\n",
      "loss: 0.001347  [  500/ 1536]\n",
      "loss: 0.017331  [  600/ 1536]\n",
      "loss: 0.115043  [  700/ 1536]\n",
      "loss: 0.007913  [  800/ 1536]\n",
      "loss: 0.031282  [  900/ 1536]\n",
      "loss: 0.110344  [ 1000/ 1536]\n",
      "loss: 0.023409  [ 1100/ 1536]\n",
      "loss: 0.001885  [ 1200/ 1536]\n",
      "loss: 0.018125  [ 1300/ 1536]\n",
      "loss: 0.071119  [ 1400/ 1536]\n",
      "loss: 0.027557  [ 1500/ 1536]\n",
      "loss: 0.029386  [    0/ 1536]\n",
      "loss: 0.240783  [  100/ 1536]\n",
      "loss: 0.282535  [  200/ 1536]\n",
      "loss: 0.046685  [  300/ 1536]\n",
      "loss: 0.000376  [  400/ 1536]\n",
      "loss: 0.066372  [  500/ 1536]\n",
      "loss: 0.000026  [  600/ 1536]\n",
      "loss: 0.032445  [  700/ 1536]\n",
      "loss: 0.041446  [  800/ 1536]\n",
      "loss: 0.016725  [  900/ 1536]\n",
      "loss: 0.146296  [ 1000/ 1536]\n",
      "loss: 0.003408  [ 1100/ 1536]\n",
      "loss: 0.016463  [ 1200/ 1536]\n",
      "loss: 0.024341  [ 1300/ 1536]\n",
      "loss: 0.071963  [ 1400/ 1536]\n",
      "loss: 0.004966  [ 1500/ 1536]\n",
      "loss: 0.006321  [    0/ 1536]\n",
      "loss: 0.110608  [  100/ 1536]\n",
      "loss: 0.020381  [  200/ 1536]\n",
      "loss: 0.027615  [  300/ 1536]\n",
      "loss: 0.167734  [  400/ 1536]\n",
      "loss: 0.192934  [  500/ 1536]\n",
      "loss: 0.007170  [  600/ 1536]\n",
      "loss: 0.011472  [  700/ 1536]\n",
      "loss: 0.001158  [  800/ 1536]\n",
      "loss: 0.102103  [  900/ 1536]\n",
      "loss: 0.006096  [ 1000/ 1536]\n",
      "loss: 0.058891  [ 1100/ 1536]\n",
      "loss: 0.006855  [ 1200/ 1536]\n",
      "loss: 0.008055  [ 1300/ 1536]\n",
      "loss: 0.051470  [ 1400/ 1536]\n",
      "loss: 0.121481  [ 1500/ 1536]\n",
      "loss: 0.028639  [    0/ 1536]\n",
      "loss: 0.002849  [  100/ 1536]\n",
      "loss: 0.015778  [  200/ 1536]\n",
      "loss: 0.000220  [  300/ 1536]\n",
      "loss: 0.009793  [  400/ 1536]\n",
      "loss: 0.038287  [  500/ 1536]\n",
      "loss: 0.060457  [  600/ 1536]\n",
      "loss: 0.027611  [  700/ 1536]\n",
      "loss: 0.032305  [  800/ 1536]\n",
      "loss: 0.088737  [  900/ 1536]\n",
      "loss: 0.000445  [ 1000/ 1536]\n",
      "loss: 0.027542  [ 1100/ 1536]\n",
      "loss: 0.068597  [ 1200/ 1536]\n",
      "loss: 0.017987  [ 1300/ 1536]\n",
      "loss: 0.025321  [ 1400/ 1536]\n",
      "loss: 0.004152  [ 1500/ 1536]\n",
      "loss: 0.008462  [    0/ 1536]\n",
      "loss: 0.013143  [  100/ 1536]\n",
      "loss: 0.097945  [  200/ 1536]\n",
      "loss: 0.008278  [  300/ 1536]\n",
      "loss: 0.062429  [  400/ 1536]\n",
      "loss: 0.000048  [  500/ 1536]\n",
      "loss: 0.100480  [  600/ 1536]\n",
      "loss: 0.030256  [  700/ 1536]\n",
      "loss: 0.106626  [  800/ 1536]\n",
      "loss: 0.000958  [  900/ 1536]\n",
      "loss: 0.001067  [ 1000/ 1536]\n",
      "loss: 0.209283  [ 1100/ 1536]\n",
      "loss: 0.001565  [ 1200/ 1536]\n",
      "loss: 0.006613  [ 1300/ 1536]\n",
      "loss: 0.029125  [ 1400/ 1536]\n",
      "loss: 0.069378  [ 1500/ 1536]\n",
      "loss: 0.069162  [    0/ 1536]\n",
      "loss: 0.043003  [  100/ 1536]\n",
      "loss: 0.016717  [  200/ 1536]\n",
      "loss: 0.069904  [  300/ 1536]\n",
      "loss: 0.009736  [  400/ 1536]\n",
      "loss: 0.039817  [  500/ 1536]\n",
      "loss: 0.003460  [  600/ 1536]\n",
      "loss: 0.003769  [  700/ 1536]\n",
      "loss: 0.018579  [  800/ 1536]\n",
      "loss: 0.003641  [  900/ 1536]\n",
      "loss: 0.011480  [ 1000/ 1536]\n",
      "loss: 0.040142  [ 1100/ 1536]\n",
      "loss: 0.000354  [ 1200/ 1536]\n",
      "loss: 0.298959  [ 1300/ 1536]\n",
      "loss: 0.004316  [ 1400/ 1536]\n",
      "loss: 0.060562  [ 1500/ 1536]\n",
      "loss: 0.028441  [    0/ 1536]\n",
      "loss: 0.064473  [  100/ 1536]\n",
      "loss: 0.027909  [  200/ 1536]\n",
      "loss: 0.110383  [  300/ 1536]\n",
      "loss: 0.015563  [  400/ 1536]\n",
      "loss: 0.017428  [  500/ 1536]\n",
      "loss: 0.019463  [  600/ 1536]\n",
      "loss: 0.001344  [  700/ 1536]\n",
      "loss: 0.008476  [  800/ 1536]\n",
      "loss: 0.000804  [  900/ 1536]\n",
      "loss: 0.000060  [ 1000/ 1536]\n",
      "loss: 0.110146  [ 1100/ 1536]\n",
      "loss: 0.015159  [ 1200/ 1536]\n",
      "loss: 0.000468  [ 1300/ 1536]\n",
      "loss: 0.048400  [ 1400/ 1536]\n",
      "loss: 0.090300  [ 1500/ 1536]\n",
      "loss: 0.009524  [    0/ 1536]\n",
      "loss: 0.032201  [  100/ 1536]\n",
      "loss: 0.016508  [  200/ 1536]\n",
      "loss: 0.010989  [  300/ 1536]\n",
      "loss: 0.001035  [  400/ 1536]\n",
      "loss: 0.000335  [  500/ 1536]\n",
      "loss: 0.029350  [  600/ 1536]\n",
      "loss: 0.005747  [  700/ 1536]\n",
      "loss: 0.012238  [  800/ 1536]\n",
      "loss: 0.008805  [  900/ 1536]\n",
      "loss: 0.021894  [ 1000/ 1536]\n",
      "loss: 0.012785  [ 1100/ 1536]\n",
      "loss: 0.024698  [ 1200/ 1536]\n",
      "loss: 0.024160  [ 1300/ 1536]\n",
      "loss: 0.030052  [ 1400/ 1536]\n",
      "loss: 0.005203  [ 1500/ 1536]\n",
      "loss: 0.017127  [    0/ 1536]\n",
      "loss: 0.012003  [  100/ 1536]\n",
      "loss: 0.210907  [  200/ 1536]\n",
      "loss: 0.022513  [  300/ 1536]\n",
      "loss: 0.008225  [  400/ 1536]\n",
      "loss: 0.001534  [  500/ 1536]\n",
      "loss: 0.047071  [  600/ 1536]\n",
      "loss: 0.058383  [  700/ 1536]\n",
      "loss: 0.006027  [  800/ 1536]\n",
      "loss: 0.002952  [  900/ 1536]\n",
      "loss: 0.000599  [ 1000/ 1536]\n",
      "loss: 0.004521  [ 1100/ 1536]\n",
      "loss: 0.101907  [ 1200/ 1536]\n",
      "loss: 0.004053  [ 1300/ 1536]\n",
      "loss: 0.000930  [ 1400/ 1536]\n",
      "loss: 0.154614  [ 1500/ 1536]\n",
      "loss: 0.045782  [    0/ 1536]\n",
      "loss: 0.007755  [  100/ 1536]\n",
      "loss: 0.000073  [  200/ 1536]\n",
      "loss: 0.096861  [  300/ 1536]\n",
      "loss: 0.119861  [  400/ 1536]\n",
      "loss: 0.026700  [  500/ 1536]\n",
      "loss: 0.000031  [  600/ 1536]\n",
      "loss: 0.100739  [  700/ 1536]\n",
      "loss: 0.000233  [  800/ 1536]\n",
      "loss: 0.193807  [  900/ 1536]\n",
      "loss: 0.110280  [ 1000/ 1536]\n",
      "loss: 0.009523  [ 1100/ 1536]\n",
      "loss: 0.039744  [ 1200/ 1536]\n",
      "loss: 0.003049  [ 1300/ 1536]\n",
      "loss: 0.136523  [ 1400/ 1536]\n",
      "loss: 0.100513  [ 1500/ 1536]\n",
      "loss: 0.004133  [    0/ 1536]\n",
      "loss: 0.007173  [  100/ 1536]\n",
      "loss: 0.026105  [  200/ 1536]\n",
      "loss: 0.015992  [  300/ 1536]\n",
      "loss: 0.030390  [  400/ 1536]\n",
      "loss: 0.009449  [  500/ 1536]\n",
      "loss: 0.003755  [  600/ 1536]\n",
      "loss: 0.005875  [  700/ 1536]\n",
      "loss: 0.554160  [  800/ 1536]\n",
      "loss: 0.009132  [  900/ 1536]\n",
      "loss: 0.009634  [ 1000/ 1536]\n",
      "loss: 0.025515  [ 1100/ 1536]\n",
      "loss: 0.040163  [ 1200/ 1536]\n",
      "loss: 0.012169  [ 1300/ 1536]\n",
      "loss: 0.034777  [ 1400/ 1536]\n",
      "loss: 0.031247  [ 1500/ 1536]\n",
      "loss: 0.000068  [    0/ 1536]\n",
      "loss: 0.027529  [  100/ 1536]\n",
      "loss: 0.029510  [  200/ 1536]\n",
      "loss: 0.225831  [  300/ 1536]\n",
      "loss: 0.044875  [  400/ 1536]\n",
      "loss: 0.019506  [  500/ 1536]\n",
      "loss: 0.002629  [  600/ 1536]\n",
      "loss: 0.250648  [  700/ 1536]\n",
      "loss: 0.007327  [  800/ 1536]\n",
      "loss: 0.017183  [  900/ 1536]\n",
      "loss: 0.042375  [ 1000/ 1536]\n",
      "loss: 0.003976  [ 1100/ 1536]\n",
      "loss: 0.000579  [ 1200/ 1536]\n",
      "loss: 0.142223  [ 1300/ 1536]\n",
      "loss: 0.004197  [ 1400/ 1536]\n",
      "loss: 0.036322  [ 1500/ 1536]\n",
      "loss: 0.001510  [    0/ 1536]\n",
      "loss: 0.008379  [  100/ 1536]\n",
      "loss: 0.002162  [  200/ 1536]\n",
      "loss: 0.000008  [  300/ 1536]\n",
      "loss: 0.032527  [  400/ 1536]\n",
      "loss: 0.007780  [  500/ 1536]\n",
      "loss: 0.008106  [  600/ 1536]\n",
      "loss: 0.038249  [  700/ 1536]\n",
      "loss: 0.047786  [  800/ 1536]\n",
      "loss: 0.035237  [  900/ 1536]\n",
      "loss: 0.011215  [ 1000/ 1536]\n",
      "loss: 0.006200  [ 1100/ 1536]\n",
      "loss: 0.005567  [ 1200/ 1536]\n",
      "loss: 0.001843  [ 1300/ 1536]\n",
      "loss: 0.040046  [ 1400/ 1536]\n",
      "loss: 0.002661  [ 1500/ 1536]\n",
      "loss: 0.003975  [    0/ 1536]\n",
      "loss: 0.021138  [  100/ 1536]\n",
      "loss: 0.000388  [  200/ 1536]\n",
      "loss: 0.006992  [  300/ 1536]\n",
      "loss: 0.017786  [  400/ 1536]\n",
      "loss: 0.002793  [  500/ 1536]\n",
      "loss: 0.049133  [  600/ 1536]\n",
      "loss: 0.000180  [  700/ 1536]\n",
      "loss: 0.011018  [  800/ 1536]\n",
      "loss: 0.094615  [  900/ 1536]\n",
      "loss: 0.059034  [ 1000/ 1536]\n",
      "loss: 0.013866  [ 1100/ 1536]\n",
      "loss: 0.049072  [ 1200/ 1536]\n",
      "loss: 0.004213  [ 1300/ 1536]\n",
      "loss: 0.014409  [ 1400/ 1536]\n",
      "loss: 0.043941  [ 1500/ 1536]\n",
      "loss: 0.000669  [    0/ 1536]\n",
      "loss: 0.100538  [  100/ 1536]\n",
      "loss: 0.000272  [  200/ 1536]\n",
      "loss: 0.044962  [  300/ 1536]\n",
      "loss: 0.000689  [  400/ 1536]\n",
      "loss: 0.066057  [  500/ 1536]\n",
      "loss: 0.004838  [  600/ 1536]\n",
      "loss: 0.023845  [  700/ 1536]\n",
      "loss: 0.104923  [  800/ 1536]\n",
      "loss: 0.072744  [  900/ 1536]\n",
      "loss: 0.001736  [ 1000/ 1536]\n",
      "loss: 0.012044  [ 1100/ 1536]\n",
      "loss: 0.018493  [ 1200/ 1536]\n",
      "loss: 0.021020  [ 1300/ 1536]\n",
      "loss: 0.000595  [ 1400/ 1536]\n",
      "loss: 0.078559  [ 1500/ 1536]\n",
      "loss: 0.005081  [    0/ 1536]\n",
      "loss: 0.023972  [  100/ 1536]\n",
      "loss: 0.032559  [  200/ 1536]\n",
      "loss: 0.003658  [  300/ 1536]\n",
      "loss: 0.035651  [  400/ 1536]\n",
      "loss: 0.008491  [  500/ 1536]\n",
      "loss: 0.008912  [  600/ 1536]\n",
      "loss: 0.095379  [  700/ 1536]\n",
      "loss: 0.057833  [  800/ 1536]\n",
      "loss: 0.015260  [  900/ 1536]\n",
      "loss: 0.098336  [ 1000/ 1536]\n",
      "loss: 0.002941  [ 1100/ 1536]\n",
      "loss: 0.000699  [ 1200/ 1536]\n",
      "loss: 0.021220  [ 1300/ 1536]\n",
      "loss: 0.000308  [ 1400/ 1536]\n",
      "loss: 0.003782  [ 1500/ 1536]\n",
      "loss: 0.001697  [    0/ 1536]\n",
      "loss: 0.014687  [  100/ 1536]\n",
      "loss: 0.000015  [  200/ 1536]\n",
      "loss: 0.056491  [  300/ 1536]\n",
      "loss: 0.002992  [  400/ 1536]\n",
      "loss: 0.014861  [  500/ 1536]\n",
      "loss: 0.021765  [  600/ 1536]\n",
      "loss: 0.010650  [  700/ 1536]\n",
      "loss: 0.000718  [  800/ 1536]\n",
      "loss: 0.008371  [  900/ 1536]\n",
      "loss: 0.066904  [ 1000/ 1536]\n",
      "loss: 0.000021  [ 1100/ 1536]\n",
      "loss: 0.002005  [ 1200/ 1536]\n",
      "loss: 0.030398  [ 1300/ 1536]\n",
      "loss: 0.016990  [ 1400/ 1536]\n",
      "loss: 0.102410  [ 1500/ 1536]\n",
      "loss: 0.121837  [    0/ 1536]\n",
      "loss: 0.087012  [  100/ 1536]\n",
      "loss: 0.029431  [  200/ 1536]\n",
      "loss: 0.000640  [  300/ 1536]\n",
      "loss: 0.073343  [  400/ 1536]\n",
      "loss: 0.000032  [  500/ 1536]\n",
      "loss: 0.055219  [  600/ 1536]\n",
      "loss: 0.007418  [  700/ 1536]\n",
      "loss: 0.006986  [  800/ 1536]\n",
      "loss: 0.000903  [  900/ 1536]\n",
      "loss: 0.000060  [ 1000/ 1536]\n",
      "loss: 0.011403  [ 1100/ 1536]\n",
      "loss: 0.003381  [ 1200/ 1536]\n",
      "loss: 0.020755  [ 1300/ 1536]\n",
      "loss: 0.152130  [ 1400/ 1536]\n",
      "loss: 0.001842  [ 1500/ 1536]\n",
      "loss: 0.023605  [    0/ 1536]\n",
      "loss: 0.110270  [  100/ 1536]\n",
      "loss: 0.028147  [  200/ 1536]\n",
      "loss: 0.011399  [  300/ 1536]\n",
      "loss: 0.009690  [  400/ 1536]\n",
      "loss: 0.018779  [  500/ 1536]\n",
      "loss: 0.074401  [  600/ 1536]\n",
      "loss: 0.076477  [  700/ 1536]\n",
      "loss: 0.009707  [  800/ 1536]\n",
      "loss: 0.135119  [  900/ 1536]\n",
      "loss: 0.006205  [ 1000/ 1536]\n",
      "loss: 0.003863  [ 1100/ 1536]\n",
      "loss: 0.033303  [ 1200/ 1536]\n",
      "loss: 0.010401  [ 1300/ 1536]\n",
      "loss: 0.089036  [ 1400/ 1536]\n",
      "loss: 0.225850  [ 1500/ 1536]\n",
      "loss: 0.024585  [    0/ 1536]\n",
      "loss: 0.006031  [  100/ 1536]\n",
      "loss: 0.017360  [  200/ 1536]\n",
      "loss: 0.060162  [  300/ 1536]\n",
      "loss: 0.000016  [  400/ 1536]\n",
      "loss: 0.057433  [  500/ 1536]\n",
      "loss: 0.053233  [  600/ 1536]\n",
      "loss: 0.005940  [  700/ 1536]\n",
      "loss: 0.005678  [  800/ 1536]\n",
      "loss: 0.225339  [  900/ 1536]\n",
      "loss: 0.183287  [ 1000/ 1536]\n",
      "loss: 0.008883  [ 1100/ 1536]\n",
      "loss: 0.029457  [ 1200/ 1536]\n",
      "loss: 0.198715  [ 1300/ 1536]\n",
      "loss: 0.055245  [ 1400/ 1536]\n",
      "loss: 0.030250  [ 1500/ 1536]\n",
      "loss: 0.019229  [    0/ 1536]\n",
      "loss: 0.000008  [  100/ 1536]\n",
      "loss: 0.078795  [  200/ 1536]\n",
      "loss: 0.003759  [  300/ 1536]\n",
      "loss: 0.018111  [  400/ 1536]\n",
      "loss: 0.006241  [  500/ 1536]\n",
      "loss: 0.002670  [  600/ 1536]\n",
      "loss: 0.019909  [  700/ 1536]\n",
      "loss: 0.032803  [  800/ 1536]\n",
      "loss: 0.048787  [  900/ 1536]\n",
      "loss: 0.104954  [ 1000/ 1536]\n",
      "loss: 0.003991  [ 1100/ 1536]\n",
      "loss: 0.000772  [ 1200/ 1536]\n",
      "loss: 0.148176  [ 1300/ 1536]\n",
      "loss: 0.040645  [ 1400/ 1536]\n",
      "loss: 0.001666  [ 1500/ 1536]\n",
      "loss: 0.080051  [    0/ 1536]\n",
      "loss: 0.000282  [  100/ 1536]\n",
      "loss: 0.005365  [  200/ 1536]\n",
      "loss: 0.052701  [  300/ 1536]\n",
      "loss: 0.008019  [  400/ 1536]\n",
      "loss: 0.000250  [  500/ 1536]\n",
      "loss: 0.000530  [  600/ 1536]\n",
      "loss: 0.015741  [  700/ 1536]\n",
      "loss: 0.008714  [  800/ 1536]\n",
      "loss: 0.014543  [  900/ 1536]\n",
      "loss: 0.014833  [ 1000/ 1536]\n",
      "loss: 0.007136  [ 1100/ 1536]\n",
      "loss: 0.001642  [ 1200/ 1536]\n",
      "loss: 0.042371  [ 1300/ 1536]\n",
      "loss: 0.000748  [ 1400/ 1536]\n",
      "loss: 0.049335  [ 1500/ 1536]\n",
      "loss: 0.001069  [    0/ 1536]\n",
      "loss: 0.040408  [  100/ 1536]\n",
      "loss: 0.009449  [  200/ 1536]\n",
      "loss: 0.000157  [  300/ 1536]\n",
      "loss: 0.066452  [  400/ 1536]\n",
      "loss: 0.005841  [  500/ 1536]\n",
      "loss: 0.022899  [  600/ 1536]\n",
      "loss: 0.038118  [  700/ 1536]\n",
      "loss: 0.084359  [  800/ 1536]\n",
      "loss: 0.000022  [  900/ 1536]\n",
      "loss: 0.000579  [ 1000/ 1536]\n",
      "loss: 0.279639  [ 1100/ 1536]\n",
      "loss: 0.019909  [ 1200/ 1536]\n",
      "loss: 0.118920  [ 1300/ 1536]\n",
      "loss: 0.000002  [ 1400/ 1536]\n",
      "loss: 0.023309  [ 1500/ 1536]\n",
      "loss: 0.002922  [    0/ 1536]\n",
      "loss: 0.005181  [  100/ 1536]\n",
      "loss: 0.001826  [  200/ 1536]\n",
      "loss: 0.341482  [  300/ 1536]\n",
      "loss: 0.018204  [  400/ 1536]\n",
      "loss: 0.000065  [  500/ 1536]\n",
      "loss: 0.000302  [  600/ 1536]\n",
      "loss: 0.216445  [  700/ 1536]\n",
      "loss: 0.177739  [  800/ 1536]\n",
      "loss: 0.194688  [  900/ 1536]\n",
      "loss: 0.065962  [ 1000/ 1536]\n",
      "loss: 0.000005  [ 1100/ 1536]\n",
      "loss: 0.037044  [ 1200/ 1536]\n",
      "loss: 0.000262  [ 1300/ 1536]\n",
      "loss: 0.044003  [ 1400/ 1536]\n",
      "loss: 0.004357  [ 1500/ 1536]\n",
      "loss: 0.003720  [    0/ 1536]\n",
      "loss: 0.055775  [  100/ 1536]\n",
      "loss: 0.056975  [  200/ 1536]\n",
      "loss: 0.002075  [  300/ 1536]\n",
      "loss: 0.040911  [  400/ 1536]\n",
      "loss: 0.000136  [  500/ 1536]\n",
      "loss: 0.053972  [  600/ 1536]\n",
      "loss: 0.001430  [  700/ 1536]\n",
      "loss: 0.008284  [  800/ 1536]\n",
      "loss: 0.046009  [  900/ 1536]\n",
      "loss: 0.110596  [ 1000/ 1536]\n",
      "loss: 0.000051  [ 1100/ 1536]\n",
      "loss: 0.056032  [ 1200/ 1536]\n",
      "loss: 0.125178  [ 1300/ 1536]\n",
      "loss: 0.047423  [ 1400/ 1536]\n",
      "loss: 0.006825  [ 1500/ 1536]\n",
      "loss: 0.061315  [    0/ 1536]\n",
      "loss: 0.026532  [  100/ 1536]\n",
      "loss: 0.002117  [  200/ 1536]\n",
      "loss: 0.002096  [  300/ 1536]\n",
      "loss: 0.006270  [  400/ 1536]\n",
      "loss: 0.005751  [  500/ 1536]\n",
      "loss: 0.027486  [  600/ 1536]\n",
      "loss: 0.002879  [  700/ 1536]\n",
      "loss: 0.003315  [  800/ 1536]\n",
      "loss: 0.037976  [  900/ 1536]\n",
      "loss: 0.192971  [ 1000/ 1536]\n",
      "loss: 0.057878  [ 1100/ 1536]\n",
      "loss: 0.075637  [ 1200/ 1536]\n",
      "loss: 0.020507  [ 1300/ 1536]\n",
      "loss: 0.001670  [ 1400/ 1536]\n",
      "loss: 0.001999  [ 1500/ 1536]\n",
      "loss: 0.002167  [    0/ 1536]\n",
      "loss: 0.000011  [  100/ 1536]\n",
      "loss: 0.000994  [  200/ 1536]\n",
      "loss: 0.008535  [  300/ 1536]\n",
      "loss: 0.002717  [  400/ 1536]\n",
      "loss: 0.005905  [  500/ 1536]\n",
      "loss: 0.003079  [  600/ 1536]\n",
      "loss: 0.010485  [  700/ 1536]\n",
      "loss: 0.000366  [  800/ 1536]\n",
      "loss: 0.015732  [  900/ 1536]\n",
      "loss: 0.003920  [ 1000/ 1536]\n",
      "loss: 0.027169  [ 1100/ 1536]\n",
      "loss: 0.082037  [ 1200/ 1536]\n",
      "loss: 0.000387  [ 1300/ 1536]\n",
      "loss: 0.013059  [ 1400/ 1536]\n",
      "loss: 0.006857  [ 1500/ 1536]\n",
      "loss: 0.002150  [    0/ 1536]\n",
      "loss: 0.071105  [  100/ 1536]\n",
      "loss: 0.019525  [  200/ 1536]\n",
      "loss: 0.047227  [  300/ 1536]\n",
      "loss: 0.013641  [  400/ 1536]\n",
      "loss: 0.009424  [  500/ 1536]\n",
      "loss: 0.056990  [  600/ 1536]\n",
      "loss: 0.179283  [  700/ 1536]\n",
      "loss: 0.071485  [  800/ 1536]\n",
      "loss: 0.018698  [  900/ 1536]\n",
      "loss: 0.151497  [ 1000/ 1536]\n",
      "loss: 0.000008  [ 1100/ 1536]\n",
      "loss: 0.017713  [ 1200/ 1536]\n",
      "loss: 0.032162  [ 1300/ 1536]\n",
      "loss: 0.085903  [ 1400/ 1536]\n",
      "loss: 0.000157  [ 1500/ 1536]\n",
      "loss: 0.000329  [    0/ 1536]\n",
      "loss: 0.015497  [  100/ 1536]\n",
      "loss: 0.034605  [  200/ 1536]\n",
      "loss: 0.004856  [  300/ 1536]\n",
      "loss: 0.012728  [  400/ 1536]\n",
      "loss: 0.009379  [  500/ 1536]\n",
      "loss: 0.001257  [  600/ 1536]\n",
      "loss: 0.093398  [  700/ 1536]\n",
      "loss: 0.108945  [  800/ 1536]\n",
      "loss: 0.212593  [  900/ 1536]\n",
      "loss: 0.008786  [ 1000/ 1536]\n",
      "loss: 0.001522  [ 1100/ 1536]\n",
      "loss: 0.095629  [ 1200/ 1536]\n",
      "loss: 0.036021  [ 1300/ 1536]\n",
      "loss: 0.119809  [ 1400/ 1536]\n",
      "loss: 0.119236  [ 1500/ 1536]\n",
      "loss: 0.000057  [    0/ 1536]\n",
      "loss: 0.136687  [  100/ 1536]\n",
      "loss: 0.008997  [  200/ 1536]\n",
      "loss: 0.007293  [  300/ 1536]\n",
      "loss: 0.043755  [  400/ 1536]\n",
      "loss: 0.000628  [  500/ 1536]\n",
      "loss: 0.035330  [  600/ 1536]\n",
      "loss: 0.000249  [  700/ 1536]\n",
      "loss: 0.025330  [  800/ 1536]\n",
      "loss: 0.065665  [  900/ 1536]\n",
      "loss: 0.001600  [ 1000/ 1536]\n",
      "loss: 0.035790  [ 1100/ 1536]\n",
      "loss: 0.013979  [ 1200/ 1536]\n",
      "loss: 0.002123  [ 1300/ 1536]\n",
      "loss: 0.074255  [ 1400/ 1536]\n",
      "loss: 0.042714  [ 1500/ 1536]\n",
      "loss: 0.000480  [    0/ 1536]\n",
      "loss: 0.036341  [  100/ 1536]\n",
      "loss: 0.240018  [  200/ 1536]\n",
      "loss: 0.182864  [  300/ 1536]\n",
      "loss: 0.049115  [  400/ 1536]\n",
      "loss: 0.000437  [  500/ 1536]\n",
      "loss: 0.047639  [  600/ 1536]\n",
      "loss: 0.389865  [  700/ 1536]\n",
      "loss: 0.013301  [  800/ 1536]\n",
      "loss: 0.009244  [  900/ 1536]\n",
      "loss: 0.010115  [ 1000/ 1536]\n",
      "loss: 0.039303  [ 1100/ 1536]\n",
      "loss: 0.048508  [ 1200/ 1536]\n",
      "loss: 0.000485  [ 1300/ 1536]\n",
      "loss: 0.026805  [ 1400/ 1536]\n",
      "loss: 0.001173  [ 1500/ 1536]\n",
      "loss: 0.040742  [    0/ 1536]\n",
      "loss: 0.034244  [  100/ 1536]\n",
      "loss: 0.059321  [  200/ 1536]\n",
      "loss: 0.233703  [  300/ 1536]\n",
      "loss: 0.119810  [  400/ 1536]\n",
      "loss: 0.138477  [  500/ 1536]\n",
      "loss: 0.098756  [  600/ 1536]\n",
      "loss: 0.014132  [  700/ 1536]\n",
      "loss: 0.113917  [  800/ 1536]\n",
      "loss: 0.004207  [  900/ 1536]\n",
      "loss: 0.008669  [ 1000/ 1536]\n",
      "loss: 0.044504  [ 1100/ 1536]\n",
      "loss: 0.158622  [ 1200/ 1536]\n",
      "loss: 0.001028  [ 1300/ 1536]\n",
      "loss: 0.064777  [ 1400/ 1536]\n",
      "loss: 0.000198  [ 1500/ 1536]\n",
      "loss: 0.000082  [    0/ 1536]\n",
      "loss: 0.027505  [  100/ 1536]\n",
      "loss: 0.000136  [  200/ 1536]\n",
      "loss: 0.029113  [  300/ 1536]\n",
      "loss: 0.061855  [  400/ 1536]\n",
      "loss: 0.002960  [  500/ 1536]\n",
      "loss: 0.001234  [  600/ 1536]\n",
      "loss: 0.147756  [  700/ 1536]\n",
      "loss: 0.261480  [  800/ 1536]\n",
      "loss: 0.044537  [  900/ 1536]\n",
      "loss: 0.030627  [ 1000/ 1536]\n",
      "loss: 0.006571  [ 1100/ 1536]\n",
      "loss: 0.097371  [ 1200/ 1536]\n",
      "loss: 0.006832  [ 1300/ 1536]\n",
      "loss: 0.009209  [ 1400/ 1536]\n",
      "loss: 0.034254  [ 1500/ 1536]\n",
      "loss: 0.018409  [    0/ 1536]\n",
      "loss: 0.000552  [  100/ 1536]\n",
      "loss: 0.006666  [  200/ 1536]\n",
      "loss: 0.011219  [  300/ 1536]\n",
      "loss: 0.025339  [  400/ 1536]\n",
      "loss: 0.070748  [  500/ 1536]\n",
      "loss: 0.000109  [  600/ 1536]\n",
      "loss: 0.006863  [  700/ 1536]\n",
      "loss: 0.000712  [  800/ 1536]\n",
      "loss: 0.046302  [  900/ 1536]\n",
      "loss: 0.000435  [ 1000/ 1536]\n",
      "loss: 0.005818  [ 1100/ 1536]\n",
      "loss: 0.016613  [ 1200/ 1536]\n",
      "loss: 0.010232  [ 1300/ 1536]\n",
      "loss: 0.000048  [ 1400/ 1536]\n",
      "loss: 0.000024  [ 1500/ 1536]\n",
      "loss: 0.000275  [    0/ 1536]\n",
      "loss: 0.000047  [  100/ 1536]\n",
      "loss: 0.037878  [  200/ 1536]\n",
      "loss: 0.000243  [  300/ 1536]\n",
      "loss: 0.173211  [  400/ 1536]\n",
      "loss: 0.012212  [  500/ 1536]\n",
      "loss: 0.015604  [  600/ 1536]\n",
      "loss: 0.294119  [  700/ 1536]\n",
      "loss: 0.038056  [  800/ 1536]\n",
      "loss: 0.011130  [  900/ 1536]\n",
      "loss: 0.012608  [ 1000/ 1536]\n",
      "loss: 0.082263  [ 1100/ 1536]\n",
      "loss: 0.008894  [ 1200/ 1536]\n",
      "loss: 0.000009  [ 1300/ 1536]\n",
      "loss: 0.027338  [ 1400/ 1536]\n",
      "loss: 0.007170  [ 1500/ 1536]\n",
      "loss: 0.084162  [    0/ 1536]\n",
      "loss: 0.056043  [  100/ 1536]\n",
      "loss: 0.039286  [  200/ 1536]\n",
      "loss: 0.019244  [  300/ 1536]\n",
      "loss: 0.064275  [  400/ 1536]\n",
      "loss: 0.171238  [  500/ 1536]\n",
      "loss: 0.011153  [  600/ 1536]\n",
      "loss: 0.046322  [  700/ 1536]\n",
      "loss: 0.027530  [  800/ 1536]\n",
      "loss: 0.093040  [  900/ 1536]\n",
      "loss: 0.013583  [ 1000/ 1536]\n",
      "loss: 0.003594  [ 1100/ 1536]\n",
      "loss: 0.011578  [ 1200/ 1536]\n",
      "loss: 0.109726  [ 1300/ 1536]\n",
      "loss: 0.036910  [ 1400/ 1536]\n",
      "loss: 0.000499  [ 1500/ 1536]\n",
      "loss: 0.033059  [    0/ 1536]\n",
      "loss: 0.066876  [  100/ 1536]\n",
      "loss: 0.114623  [  200/ 1536]\n",
      "loss: 0.027451  [  300/ 1536]\n",
      "loss: 0.002695  [  400/ 1536]\n",
      "loss: 0.066993  [  500/ 1536]\n",
      "loss: 0.099943  [  600/ 1536]\n",
      "loss: 0.002959  [  700/ 1536]\n",
      "loss: 0.130394  [  800/ 1536]\n",
      "loss: 0.001497  [  900/ 1536]\n",
      "loss: 0.058755  [ 1000/ 1536]\n",
      "loss: 0.004820  [ 1100/ 1536]\n",
      "loss: 0.067328  [ 1200/ 1536]\n",
      "loss: 0.239839  [ 1300/ 1536]\n",
      "loss: 0.026191  [ 1400/ 1536]\n",
      "loss: 0.017510  [ 1500/ 1536]\n",
      "loss: 0.055438  [    0/ 1536]\n",
      "loss: 0.000594  [  100/ 1536]\n",
      "loss: 0.007835  [  200/ 1536]\n",
      "loss: 0.004383  [  300/ 1536]\n",
      "loss: 0.012028  [  400/ 1536]\n",
      "loss: 0.016120  [  500/ 1536]\n",
      "loss: 0.107531  [  600/ 1536]\n",
      "loss: 0.005256  [  700/ 1536]\n",
      "loss: 0.004814  [  800/ 1536]\n",
      "loss: 0.018157  [  900/ 1536]\n",
      "loss: 0.001842  [ 1000/ 1536]\n",
      "loss: 0.161658  [ 1100/ 1536]\n",
      "loss: 0.020570  [ 1200/ 1536]\n",
      "loss: 0.000000  [ 1300/ 1536]\n",
      "loss: 0.067607  [ 1400/ 1536]\n",
      "loss: 0.000037  [ 1500/ 1536]\n",
      "loss: 0.096752  [    0/ 1536]\n",
      "loss: 0.003452  [  100/ 1536]\n",
      "loss: 0.040706  [  200/ 1536]\n",
      "loss: 0.033472  [  300/ 1536]\n",
      "loss: 0.007742  [  400/ 1536]\n",
      "loss: 0.000000  [  500/ 1536]\n",
      "loss: 0.002029  [  600/ 1536]\n",
      "loss: 0.016969  [  700/ 1536]\n",
      "loss: 0.141488  [  800/ 1536]\n",
      "loss: 0.038079  [  900/ 1536]\n",
      "loss: 0.004792  [ 1000/ 1536]\n",
      "loss: 0.002230  [ 1100/ 1536]\n",
      "loss: 0.000167  [ 1200/ 1536]\n",
      "loss: 0.000019  [ 1300/ 1536]\n",
      "loss: 0.041350  [ 1400/ 1536]\n",
      "loss: 0.148260  [ 1500/ 1536]\n",
      "loss: 0.012298  [    0/ 1536]\n",
      "loss: 0.000475  [  100/ 1536]\n",
      "loss: 0.003550  [  200/ 1536]\n",
      "loss: 0.007272  [  300/ 1536]\n",
      "loss: 0.023394  [  400/ 1536]\n",
      "loss: 0.005523  [  500/ 1536]\n",
      "loss: 0.093584  [  600/ 1536]\n",
      "loss: 0.000016  [  700/ 1536]\n",
      "loss: 0.009413  [  800/ 1536]\n",
      "loss: 0.005429  [  900/ 1536]\n",
      "loss: 0.199852  [ 1000/ 1536]\n",
      "loss: 0.007925  [ 1100/ 1536]\n",
      "loss: 0.001550  [ 1200/ 1536]\n",
      "loss: 0.014989  [ 1300/ 1536]\n",
      "loss: 0.001934  [ 1400/ 1536]\n",
      "loss: 0.005018  [ 1500/ 1536]\n",
      "loss: 0.076110  [    0/ 1536]\n",
      "loss: 0.003987  [  100/ 1536]\n",
      "loss: 0.039327  [  200/ 1536]\n",
      "loss: 0.000008  [  300/ 1536]\n",
      "loss: 0.027952  [  400/ 1536]\n",
      "loss: 0.026875  [  500/ 1536]\n",
      "loss: 0.004969  [  600/ 1536]\n",
      "loss: 0.001795  [  700/ 1536]\n",
      "loss: 0.199117  [  800/ 1536]\n",
      "loss: 0.112364  [  900/ 1536]\n",
      "loss: 0.000638  [ 1000/ 1536]\n",
      "loss: 0.024349  [ 1100/ 1536]\n",
      "loss: 0.026002  [ 1200/ 1536]\n",
      "loss: 0.001723  [ 1300/ 1536]\n",
      "loss: 0.083710  [ 1400/ 1536]\n",
      "loss: 0.023281  [ 1500/ 1536]\n",
      "loss: 0.002138  [    0/ 1536]\n",
      "loss: 0.001360  [  100/ 1536]\n",
      "loss: 0.002025  [  200/ 1536]\n",
      "loss: 0.005876  [  300/ 1536]\n",
      "loss: 0.003309  [  400/ 1536]\n",
      "loss: 0.028316  [  500/ 1536]\n",
      "loss: 0.017871  [  600/ 1536]\n",
      "loss: 0.021707  [  700/ 1536]\n",
      "loss: 0.045243  [  800/ 1536]\n",
      "loss: 0.024014  [  900/ 1536]\n",
      "loss: 0.034656  [ 1000/ 1536]\n",
      "loss: 0.003556  [ 1100/ 1536]\n",
      "loss: 0.006632  [ 1200/ 1536]\n",
      "loss: 0.017621  [ 1300/ 1536]\n",
      "loss: 0.005389  [ 1400/ 1536]\n",
      "loss: 0.009095  [ 1500/ 1536]\n",
      "loss: 0.055820  [    0/ 1536]\n",
      "loss: 0.014933  [  100/ 1536]\n",
      "loss: 0.021580  [  200/ 1536]\n",
      "loss: 0.042443  [  300/ 1536]\n",
      "loss: 0.000591  [  400/ 1536]\n",
      "loss: 0.000434  [  500/ 1536]\n",
      "loss: 0.002514  [  600/ 1536]\n",
      "loss: 0.194738  [  700/ 1536]\n",
      "loss: 0.025389  [  800/ 1536]\n",
      "loss: 0.000191  [  900/ 1536]\n",
      "loss: 0.048402  [ 1000/ 1536]\n",
      "loss: 0.019219  [ 1100/ 1536]\n",
      "loss: 0.030096  [ 1200/ 1536]\n",
      "loss: 0.003111  [ 1300/ 1536]\n",
      "loss: 0.005630  [ 1400/ 1536]\n",
      "loss: 0.143184  [ 1500/ 1536]\n",
      "loss: 0.022423  [    0/ 1536]\n",
      "loss: 0.000511  [  100/ 1536]\n",
      "loss: 0.003801  [  200/ 1536]\n",
      "loss: 0.037991  [  300/ 1536]\n",
      "loss: 0.124580  [  400/ 1536]\n",
      "loss: 0.059152  [  500/ 1536]\n",
      "loss: 0.007249  [  600/ 1536]\n",
      "loss: 0.211187  [  700/ 1536]\n",
      "loss: 0.041959  [  800/ 1536]\n",
      "loss: 0.061088  [  900/ 1536]\n",
      "loss: 0.095531  [ 1000/ 1536]\n",
      "loss: 0.000432  [ 1100/ 1536]\n",
      "loss: 0.004544  [ 1200/ 1536]\n",
      "loss: 0.001856  [ 1300/ 1536]\n",
      "loss: 0.034363  [ 1400/ 1536]\n",
      "loss: 0.000005  [ 1500/ 1536]\n",
      "loss: 0.001530  [    0/ 1536]\n",
      "loss: 0.004669  [  100/ 1536]\n",
      "loss: 0.139235  [  200/ 1536]\n",
      "loss: 0.014023  [  300/ 1536]\n",
      "loss: 0.006077  [  400/ 1536]\n",
      "loss: 0.038938  [  500/ 1536]\n",
      "loss: 0.104966  [  600/ 1536]\n",
      "loss: 0.043230  [  700/ 1536]\n",
      "loss: 0.027203  [  800/ 1536]\n",
      "loss: 0.047978  [  900/ 1536]\n",
      "loss: 0.014497  [ 1000/ 1536]\n",
      "loss: 0.045690  [ 1100/ 1536]\n",
      "loss: 0.036223  [ 1200/ 1536]\n",
      "loss: 0.011738  [ 1300/ 1536]\n",
      "loss: 0.000517  [ 1400/ 1536]\n",
      "loss: 0.246916  [ 1500/ 1536]\n",
      "loss: 0.139582  [    0/ 1536]\n",
      "loss: 0.038666  [  100/ 1536]\n",
      "loss: 0.023090  [  200/ 1536]\n",
      "loss: 0.286242  [  300/ 1536]\n",
      "loss: 0.027878  [  400/ 1536]\n",
      "loss: 0.095226  [  500/ 1536]\n",
      "loss: 0.126811  [  600/ 1536]\n",
      "loss: 0.007956  [  700/ 1536]\n",
      "loss: 0.023468  [  800/ 1536]\n",
      "loss: 0.055463  [  900/ 1536]\n",
      "loss: 0.060599  [ 1000/ 1536]\n",
      "loss: 0.008245  [ 1100/ 1536]\n",
      "loss: 0.004502  [ 1200/ 1536]\n",
      "loss: 0.068006  [ 1300/ 1536]\n",
      "loss: 0.016889  [ 1400/ 1536]\n",
      "loss: 0.000976  [ 1500/ 1536]\n",
      "loss: 0.000473  [    0/ 1536]\n",
      "loss: 0.005827  [  100/ 1536]\n",
      "loss: 0.002575  [  200/ 1536]\n",
      "loss: 0.060089  [  300/ 1536]\n",
      "loss: 0.008232  [  400/ 1536]\n",
      "loss: 0.032504  [  500/ 1536]\n",
      "loss: 0.144298  [  600/ 1536]\n",
      "loss: 0.221013  [  700/ 1536]\n",
      "loss: 0.012441  [  800/ 1536]\n",
      "loss: 0.032851  [  900/ 1536]\n",
      "loss: 0.004324  [ 1000/ 1536]\n",
      "loss: 0.003837  [ 1100/ 1536]\n",
      "loss: 0.007367  [ 1200/ 1536]\n",
      "loss: 0.051368  [ 1300/ 1536]\n",
      "loss: 0.000227  [ 1400/ 1536]\n",
      "loss: 0.015269  [ 1500/ 1536]\n",
      "loss: 0.003726  [    0/ 1536]\n",
      "loss: 0.032406  [  100/ 1536]\n",
      "loss: 0.005973  [  200/ 1536]\n",
      "loss: 0.086987  [  300/ 1536]\n",
      "loss: 0.006411  [  400/ 1536]\n",
      "loss: 0.006332  [  500/ 1536]\n",
      "loss: 0.191922  [  600/ 1536]\n",
      "loss: 0.009640  [  700/ 1536]\n",
      "loss: 0.012681  [  800/ 1536]\n",
      "loss: 0.002657  [  900/ 1536]\n",
      "loss: 0.000954  [ 1000/ 1536]\n",
      "loss: 0.047720  [ 1100/ 1536]\n",
      "loss: 0.010946  [ 1200/ 1536]\n",
      "loss: 0.005532  [ 1300/ 1536]\n",
      "loss: 0.072097  [ 1400/ 1536]\n",
      "loss: 0.092627  [ 1500/ 1536]\n",
      "loss: 0.030146  [    0/ 1536]\n",
      "loss: 0.000090  [  100/ 1536]\n",
      "loss: 0.002670  [  200/ 1536]\n",
      "loss: 0.000776  [  300/ 1536]\n",
      "loss: 0.005947  [  400/ 1536]\n",
      "loss: 0.016597  [  500/ 1536]\n",
      "loss: 0.101533  [  600/ 1536]\n",
      "loss: 0.067382  [  700/ 1536]\n",
      "loss: 0.181311  [  800/ 1536]\n",
      "loss: 0.005951  [  900/ 1536]\n",
      "loss: 0.226411  [ 1000/ 1536]\n",
      "loss: 0.064212  [ 1100/ 1536]\n",
      "loss: 0.018143  [ 1200/ 1536]\n",
      "loss: 0.310458  [ 1300/ 1536]\n",
      "loss: 0.075489  [ 1400/ 1536]\n",
      "loss: 0.036695  [ 1500/ 1536]\n",
      "loss: 0.063080  [    0/ 1536]\n",
      "loss: 0.075159  [  100/ 1536]\n",
      "loss: 0.005978  [  200/ 1536]\n",
      "loss: 0.035619  [  300/ 1536]\n",
      "loss: 0.004864  [  400/ 1536]\n",
      "loss: 0.192398  [  500/ 1536]\n",
      "loss: 0.088051  [  600/ 1536]\n",
      "loss: 0.004721  [  700/ 1536]\n",
      "loss: 0.001413  [  800/ 1536]\n",
      "loss: 0.001620  [  900/ 1536]\n",
      "loss: 0.003136  [ 1000/ 1536]\n",
      "loss: 0.016523  [ 1100/ 1536]\n",
      "loss: 0.011625  [ 1200/ 1536]\n",
      "loss: 0.005774  [ 1300/ 1536]\n",
      "loss: 0.056011  [ 1400/ 1536]\n",
      "loss: 0.003587  [ 1500/ 1536]\n",
      "loss: 0.026470  [    0/ 1536]\n",
      "loss: 0.086026  [  100/ 1536]\n",
      "loss: 0.059644  [  200/ 1536]\n",
      "loss: 0.049149  [  300/ 1536]\n",
      "loss: 0.109342  [  400/ 1536]\n",
      "loss: 0.106514  [  500/ 1536]\n",
      "loss: 0.001765  [  600/ 1536]\n",
      "loss: 0.028255  [  700/ 1536]\n",
      "loss: 0.015435  [  800/ 1536]\n",
      "loss: 0.007437  [  900/ 1536]\n",
      "loss: 0.188841  [ 1000/ 1536]\n",
      "loss: 0.000934  [ 1100/ 1536]\n",
      "loss: 0.000027  [ 1200/ 1536]\n",
      "loss: 0.001821  [ 1300/ 1536]\n",
      "loss: 0.131376  [ 1400/ 1536]\n",
      "loss: 0.045649  [ 1500/ 1536]\n",
      "loss: 0.113093  [    0/ 1536]\n",
      "loss: 0.161486  [  100/ 1536]\n",
      "loss: 0.000986  [  200/ 1536]\n",
      "loss: 0.017508  [  300/ 1536]\n",
      "loss: 0.009517  [  400/ 1536]\n",
      "loss: 0.062810  [  500/ 1536]\n",
      "loss: 0.075408  [  600/ 1536]\n",
      "loss: 0.008333  [  700/ 1536]\n",
      "loss: 0.050819  [  800/ 1536]\n",
      "loss: 0.073623  [  900/ 1536]\n",
      "loss: 0.075028  [ 1000/ 1536]\n",
      "loss: 0.017754  [ 1100/ 1536]\n",
      "loss: 0.007012  [ 1200/ 1536]\n",
      "loss: 0.009796  [ 1300/ 1536]\n",
      "loss: 0.004184  [ 1400/ 1536]\n",
      "loss: 0.006137  [ 1500/ 1536]\n",
      "loss: 0.211437  [    0/ 1536]\n",
      "loss: 0.029848  [  100/ 1536]\n",
      "loss: 0.018373  [  200/ 1536]\n",
      "loss: 0.021696  [  300/ 1536]\n",
      "loss: 0.040475  [  400/ 1536]\n",
      "loss: 0.007452  [  500/ 1536]\n",
      "loss: 0.003158  [  600/ 1536]\n",
      "loss: 0.337038  [  700/ 1536]\n",
      "loss: 0.254367  [  800/ 1536]\n",
      "loss: 0.000276  [  900/ 1536]\n",
      "loss: 0.268936  [ 1000/ 1536]\n",
      "loss: 0.043370  [ 1100/ 1536]\n",
      "loss: 0.134929  [ 1200/ 1536]\n",
      "loss: 0.149314  [ 1300/ 1536]\n",
      "loss: 0.030443  [ 1400/ 1536]\n",
      "loss: 0.000109  [ 1500/ 1536]\n",
      "loss: 0.006721  [    0/ 1536]\n",
      "loss: 0.015555  [  100/ 1536]\n",
      "loss: 0.001176  [  200/ 1536]\n",
      "loss: 0.002544  [  300/ 1536]\n",
      "loss: 0.249303  [  400/ 1536]\n",
      "loss: 0.050990  [  500/ 1536]\n",
      "loss: 0.021933  [  600/ 1536]\n",
      "loss: 0.048981  [  700/ 1536]\n",
      "loss: 0.022885  [  800/ 1536]\n",
      "loss: 0.001531  [  900/ 1536]\n",
      "loss: 0.005777  [ 1000/ 1536]\n",
      "loss: 0.024876  [ 1100/ 1536]\n",
      "loss: 0.097721  [ 1200/ 1536]\n",
      "loss: 0.002580  [ 1300/ 1536]\n",
      "loss: 0.004168  [ 1400/ 1536]\n",
      "loss: 0.017438  [ 1500/ 1536]\n",
      "loss: 0.000817  [    0/ 1536]\n",
      "loss: 0.023740  [  100/ 1536]\n",
      "loss: 0.010155  [  200/ 1536]\n",
      "loss: 0.002948  [  300/ 1536]\n",
      "loss: 0.017783  [  400/ 1536]\n",
      "loss: 0.051600  [  500/ 1536]\n",
      "loss: 0.015839  [  600/ 1536]\n",
      "loss: 0.043396  [  700/ 1536]\n",
      "loss: 0.021135  [  800/ 1536]\n",
      "loss: 0.029792  [  900/ 1536]\n",
      "loss: 0.001699  [ 1000/ 1536]\n",
      "loss: 0.092114  [ 1100/ 1536]\n",
      "loss: 0.000676  [ 1200/ 1536]\n",
      "loss: 0.005283  [ 1300/ 1536]\n",
      "loss: 0.004291  [ 1400/ 1536]\n",
      "loss: 0.001350  [ 1500/ 1536]\n",
      "loss: 0.008129  [    0/ 1536]\n",
      "loss: 0.004892  [  100/ 1536]\n",
      "loss: 0.000355  [  200/ 1536]\n",
      "loss: 0.038192  [  300/ 1536]\n",
      "loss: 0.013692  [  400/ 1536]\n",
      "loss: 0.014051  [  500/ 1536]\n",
      "loss: 0.094252  [  600/ 1536]\n",
      "loss: 0.176064  [  700/ 1536]\n",
      "loss: 0.039280  [  800/ 1536]\n",
      "loss: 0.003835  [  900/ 1536]\n",
      "loss: 0.054009  [ 1000/ 1536]\n",
      "loss: 0.006555  [ 1100/ 1536]\n",
      "loss: 0.040959  [ 1200/ 1536]\n",
      "loss: 0.034714  [ 1300/ 1536]\n",
      "loss: 0.000168  [ 1400/ 1536]\n",
      "loss: 0.001591  [ 1500/ 1536]\n",
      "loss: 0.039980  [    0/ 1536]\n",
      "loss: 0.123712  [  100/ 1536]\n",
      "loss: 0.005414  [  200/ 1536]\n",
      "loss: 0.012980  [  300/ 1536]\n",
      "loss: 0.024479  [  400/ 1536]\n",
      "loss: 0.013452  [  500/ 1536]\n",
      "loss: 0.018480  [  600/ 1536]\n",
      "loss: 0.017312  [  700/ 1536]\n",
      "loss: 0.007678  [  800/ 1536]\n",
      "loss: 0.089491  [  900/ 1536]\n",
      "loss: 0.000175  [ 1000/ 1536]\n",
      "loss: 0.000001  [ 1100/ 1536]\n",
      "loss: 0.088512  [ 1200/ 1536]\n",
      "loss: 0.038755  [ 1300/ 1536]\n",
      "loss: 0.079310  [ 1400/ 1536]\n",
      "loss: 0.046140  [ 1500/ 1536]\n",
      "loss: 0.000333  [    0/ 1536]\n",
      "loss: 0.000482  [  100/ 1536]\n",
      "loss: 0.016402  [  200/ 1536]\n",
      "loss: 0.011167  [  300/ 1536]\n",
      "loss: 0.225040  [  400/ 1536]\n",
      "loss: 0.003065  [  500/ 1536]\n",
      "loss: 0.002361  [  600/ 1536]\n",
      "loss: 0.038834  [  700/ 1536]\n",
      "loss: 0.059165  [  800/ 1536]\n",
      "loss: 0.008122  [  900/ 1536]\n",
      "loss: 0.016218  [ 1000/ 1536]\n",
      "loss: 0.222726  [ 1100/ 1536]\n",
      "loss: 0.012167  [ 1200/ 1536]\n",
      "loss: 0.020305  [ 1300/ 1536]\n",
      "loss: 0.019830  [ 1400/ 1536]\n",
      "loss: 0.005362  [ 1500/ 1536]\n",
      "loss: 0.008111  [    0/ 1536]\n",
      "loss: 0.016432  [  100/ 1536]\n",
      "loss: 0.004944  [  200/ 1536]\n",
      "loss: 0.029022  [  300/ 1536]\n",
      "loss: 0.000793  [  400/ 1536]\n",
      "loss: 0.004072  [  500/ 1536]\n",
      "loss: 0.081561  [  600/ 1536]\n",
      "loss: 0.090276  [  700/ 1536]\n",
      "loss: 0.006350  [  800/ 1536]\n",
      "loss: 0.156818  [  900/ 1536]\n",
      "loss: 0.023891  [ 1000/ 1536]\n",
      "loss: 0.004508  [ 1100/ 1536]\n",
      "loss: 0.042368  [ 1200/ 1536]\n",
      "loss: 0.120741  [ 1300/ 1536]\n",
      "loss: 0.156988  [ 1400/ 1536]\n",
      "loss: 0.097846  [ 1500/ 1536]\n",
      "loss: 0.008731  [    0/ 1536]\n",
      "loss: 0.026966  [  100/ 1536]\n",
      "loss: 0.044271  [  200/ 1536]\n",
      "loss: 0.008914  [  300/ 1536]\n",
      "loss: 0.000990  [  400/ 1536]\n",
      "loss: 0.076984  [  500/ 1536]\n",
      "loss: 0.000489  [  600/ 1536]\n",
      "loss: 0.003552  [  700/ 1536]\n",
      "loss: 0.012227  [  800/ 1536]\n",
      "loss: 0.008335  [  900/ 1536]\n",
      "loss: 0.001078  [ 1000/ 1536]\n",
      "loss: 0.009898  [ 1100/ 1536]\n",
      "loss: 0.001702  [ 1200/ 1536]\n",
      "loss: 0.015394  [ 1300/ 1536]\n",
      "loss: 0.008881  [ 1400/ 1536]\n",
      "loss: 0.000578  [ 1500/ 1536]\n",
      "loss: 0.012248  [    0/ 1536]\n",
      "loss: 0.013242  [  100/ 1536]\n",
      "loss: 0.000456  [  200/ 1536]\n",
      "loss: 0.146957  [  300/ 1536]\n",
      "loss: 0.106005  [  400/ 1536]\n",
      "loss: 0.003745  [  500/ 1536]\n",
      "loss: 0.048570  [  600/ 1536]\n",
      "loss: 0.000179  [  700/ 1536]\n",
      "loss: 0.001774  [  800/ 1536]\n",
      "loss: 0.014846  [  900/ 1536]\n",
      "loss: 0.065442  [ 1000/ 1536]\n",
      "loss: 0.025686  [ 1100/ 1536]\n",
      "loss: 0.008065  [ 1200/ 1536]\n",
      "loss: 0.007216  [ 1300/ 1536]\n",
      "loss: 0.032636  [ 1400/ 1536]\n",
      "loss: 0.010940  [ 1500/ 1536]\n",
      "loss: 0.071484  [    0/ 1536]\n",
      "loss: 0.101750  [  100/ 1536]\n",
      "loss: 0.005737  [  200/ 1536]\n",
      "loss: 0.003143  [  300/ 1536]\n",
      "loss: 0.109126  [  400/ 1536]\n",
      "loss: 0.002604  [  500/ 1536]\n",
      "loss: 0.053664  [  600/ 1536]\n",
      "loss: 0.000643  [  700/ 1536]\n",
      "loss: 0.197804  [  800/ 1536]\n",
      "loss: 0.000225  [  900/ 1536]\n",
      "loss: 0.107491  [ 1000/ 1536]\n",
      "loss: 0.000452  [ 1100/ 1536]\n",
      "loss: 0.033646  [ 1200/ 1536]\n",
      "loss: 0.009100  [ 1300/ 1536]\n",
      "loss: 0.033626  [ 1400/ 1536]\n",
      "loss: 0.001489  [ 1500/ 1536]\n",
      "loss: 0.087393  [    0/ 1536]\n",
      "loss: 0.002951  [  100/ 1536]\n",
      "loss: 0.005488  [  200/ 1536]\n",
      "loss: 0.002519  [  300/ 1536]\n",
      "loss: 0.014405  [  400/ 1536]\n",
      "loss: 0.052757  [  500/ 1536]\n",
      "loss: 0.053916  [  600/ 1536]\n",
      "loss: 0.000034  [  700/ 1536]\n",
      "loss: 0.006397  [  800/ 1536]\n",
      "loss: 0.028401  [  900/ 1536]\n",
      "loss: 0.000001  [ 1000/ 1536]\n",
      "loss: 0.054732  [ 1100/ 1536]\n",
      "loss: 0.043588  [ 1200/ 1536]\n",
      "loss: 0.001406  [ 1300/ 1536]\n",
      "loss: 0.016214  [ 1400/ 1536]\n",
      "loss: 0.019348  [ 1500/ 1536]\n",
      "loss: 0.012561  [    0/ 1536]\n",
      "loss: 0.088355  [  100/ 1536]\n",
      "loss: 0.105928  [  200/ 1536]\n",
      "loss: 0.105402  [  300/ 1536]\n",
      "loss: 0.199601  [  400/ 1536]\n",
      "loss: 0.000129  [  500/ 1536]\n",
      "loss: 0.000007  [  600/ 1536]\n",
      "loss: 0.001991  [  700/ 1536]\n",
      "loss: 0.005314  [  800/ 1536]\n",
      "loss: 0.000055  [  900/ 1536]\n",
      "loss: 0.028048  [ 1000/ 1536]\n",
      "loss: 0.005564  [ 1100/ 1536]\n",
      "loss: 0.194403  [ 1200/ 1536]\n",
      "loss: 0.006643  [ 1300/ 1536]\n",
      "loss: 0.025602  [ 1400/ 1536]\n",
      "loss: 0.055253  [ 1500/ 1536]\n",
      "loss: 0.038788  [    0/ 1536]\n",
      "loss: 0.117734  [  100/ 1536]\n",
      "loss: 0.137924  [  200/ 1536]\n",
      "loss: 0.400883  [  300/ 1536]\n",
      "loss: 0.027328  [  400/ 1536]\n",
      "loss: 0.000475  [  500/ 1536]\n",
      "loss: 0.003594  [  600/ 1536]\n",
      "loss: 0.014140  [  700/ 1536]\n",
      "loss: 0.020402  [  800/ 1536]\n",
      "loss: 0.011685  [  900/ 1536]\n",
      "loss: 0.000167  [ 1000/ 1536]\n",
      "loss: 0.104349  [ 1100/ 1536]\n",
      "loss: 0.057211  [ 1200/ 1536]\n",
      "loss: 0.190352  [ 1300/ 1536]\n",
      "loss: 0.011665  [ 1400/ 1536]\n",
      "loss: 0.025982  [ 1500/ 1536]\n",
      "loss: 0.304417  [    0/ 1536]\n",
      "loss: 0.134358  [  100/ 1536]\n",
      "loss: 0.003806  [  200/ 1536]\n",
      "loss: 0.077482  [  300/ 1536]\n",
      "loss: 0.001538  [  400/ 1536]\n",
      "loss: 0.076408  [  500/ 1536]\n",
      "loss: 0.030214  [  600/ 1536]\n",
      "loss: 0.001046  [  700/ 1536]\n",
      "loss: 0.004376  [  800/ 1536]\n",
      "loss: 0.026582  [  900/ 1536]\n",
      "loss: 0.010349  [ 1000/ 1536]\n",
      "loss: 0.000218  [ 1100/ 1536]\n",
      "loss: 0.055237  [ 1200/ 1536]\n",
      "loss: 0.059795  [ 1300/ 1536]\n",
      "loss: 0.031108  [ 1400/ 1536]\n",
      "loss: 0.014338  [ 1500/ 1536]\n",
      "loss: 0.007635  [    0/ 1536]\n",
      "loss: 0.149016  [  100/ 1536]\n",
      "loss: 0.009425  [  200/ 1536]\n",
      "loss: 0.229518  [  300/ 1536]\n",
      "loss: 0.024028  [  400/ 1536]\n",
      "loss: 0.012720  [  500/ 1536]\n",
      "loss: 0.063910  [  600/ 1536]\n",
      "loss: 0.088635  [  700/ 1536]\n",
      "loss: 0.035547  [  800/ 1536]\n",
      "loss: 0.086590  [  900/ 1536]\n",
      "loss: 0.065375  [ 1000/ 1536]\n",
      "loss: 0.000560  [ 1100/ 1536]\n",
      "loss: 0.008376  [ 1200/ 1536]\n",
      "loss: 0.069018  [ 1300/ 1536]\n",
      "loss: 0.012961  [ 1400/ 1536]\n",
      "loss: 0.017440  [ 1500/ 1536]\n",
      "loss: 0.000663  [    0/ 1536]\n",
      "loss: 0.117572  [  100/ 1536]\n",
      "loss: 0.000379  [  200/ 1536]\n",
      "loss: 0.058163  [  300/ 1536]\n",
      "loss: 0.043603  [  400/ 1536]\n",
      "loss: 0.000641  [  500/ 1536]\n",
      "loss: 0.041800  [  600/ 1536]\n",
      "loss: 0.149336  [  700/ 1536]\n",
      "loss: 0.004330  [  800/ 1536]\n",
      "loss: 0.007211  [  900/ 1536]\n",
      "loss: 0.032969  [ 1000/ 1536]\n",
      "loss: 0.049638  [ 1100/ 1536]\n",
      "loss: 0.002506  [ 1200/ 1536]\n",
      "loss: 0.009785  [ 1300/ 1536]\n",
      "loss: 0.102404  [ 1400/ 1536]\n",
      "loss: 0.000041  [ 1500/ 1536]\n",
      "loss: 0.036974  [    0/ 1536]\n",
      "loss: 0.008523  [  100/ 1536]\n",
      "loss: 0.037361  [  200/ 1536]\n",
      "loss: 0.056842  [  300/ 1536]\n",
      "loss: 0.022294  [  400/ 1536]\n",
      "loss: 0.000286  [  500/ 1536]\n",
      "loss: 0.011496  [  600/ 1536]\n",
      "loss: 0.001857  [  700/ 1536]\n",
      "loss: 0.008834  [  800/ 1536]\n",
      "loss: 0.149450  [  900/ 1536]\n",
      "loss: 0.045817  [ 1000/ 1536]\n",
      "loss: 0.000206  [ 1100/ 1536]\n",
      "loss: 0.015979  [ 1200/ 1536]\n",
      "loss: 0.054842  [ 1300/ 1536]\n",
      "loss: 0.320165  [ 1400/ 1536]\n",
      "loss: 0.164365  [ 1500/ 1536]\n",
      "loss: 0.024593  [    0/ 1536]\n",
      "loss: 0.009189  [  100/ 1536]\n",
      "loss: 0.193384  [  200/ 1536]\n",
      "loss: 0.008135  [  300/ 1536]\n",
      "loss: 0.001157  [  400/ 1536]\n",
      "loss: 0.157756  [  500/ 1536]\n",
      "loss: 0.153717  [  600/ 1536]\n",
      "loss: 0.000006  [  700/ 1536]\n",
      "loss: 0.048498  [  800/ 1536]\n",
      "loss: 0.173492  [  900/ 1536]\n",
      "loss: 0.000324  [ 1000/ 1536]\n",
      "loss: 0.111932  [ 1100/ 1536]\n",
      "loss: 0.034692  [ 1200/ 1536]\n",
      "loss: 0.002755  [ 1300/ 1536]\n",
      "loss: 0.215791  [ 1400/ 1536]\n",
      "loss: 0.055605  [ 1500/ 1536]\n",
      "loss: 0.013966  [    0/ 1536]\n",
      "loss: 0.033360  [  100/ 1536]\n",
      "loss: 0.005441  [  200/ 1536]\n",
      "loss: 0.092254  [  300/ 1536]\n",
      "loss: 0.011093  [  400/ 1536]\n",
      "loss: 0.013127  [  500/ 1536]\n",
      "loss: 0.038321  [  600/ 1536]\n",
      "loss: 0.261884  [  700/ 1536]\n",
      "loss: 0.050323  [  800/ 1536]\n",
      "loss: 0.026581  [  900/ 1536]\n",
      "loss: 0.017031  [ 1000/ 1536]\n",
      "loss: 0.004375  [ 1100/ 1536]\n",
      "loss: 0.000484  [ 1200/ 1536]\n",
      "loss: 0.145839  [ 1300/ 1536]\n",
      "loss: 0.008973  [ 1400/ 1536]\n",
      "loss: 0.203512  [ 1500/ 1536]\n",
      "loss: 0.033197  [    0/ 1536]\n",
      "loss: 0.001133  [  100/ 1536]\n",
      "loss: 0.000003  [  200/ 1536]\n",
      "loss: 0.040755  [  300/ 1536]\n",
      "loss: 0.001759  [  400/ 1536]\n",
      "loss: 0.043587  [  500/ 1536]\n",
      "loss: 0.021714  [  600/ 1536]\n",
      "loss: 0.000001  [  700/ 1536]\n",
      "loss: 0.013068  [  800/ 1536]\n",
      "loss: 0.186057  [  900/ 1536]\n",
      "loss: 0.006560  [ 1000/ 1536]\n",
      "loss: 0.050695  [ 1100/ 1536]\n",
      "loss: 0.005804  [ 1200/ 1536]\n",
      "loss: 0.003936  [ 1300/ 1536]\n",
      "loss: 0.065028  [ 1400/ 1536]\n",
      "loss: 0.002229  [ 1500/ 1536]\n",
      "loss: 0.006350  [    0/ 1536]\n",
      "loss: 0.010601  [  100/ 1536]\n",
      "loss: 0.009981  [  200/ 1536]\n",
      "loss: 0.004324  [  300/ 1536]\n",
      "loss: 0.004653  [  400/ 1536]\n",
      "loss: 0.026405  [  500/ 1536]\n",
      "loss: 0.019764  [  600/ 1536]\n",
      "loss: 0.021232  [  700/ 1536]\n",
      "loss: 0.010034  [  800/ 1536]\n",
      "loss: 0.005681  [  900/ 1536]\n",
      "loss: 0.016124  [ 1000/ 1536]\n",
      "loss: 0.009056  [ 1100/ 1536]\n",
      "loss: 0.005544  [ 1200/ 1536]\n",
      "loss: 0.143593  [ 1300/ 1536]\n",
      "loss: 0.000064  [ 1400/ 1536]\n",
      "loss: 0.023500  [ 1500/ 1536]\n",
      "loss: 0.309962  [    0/ 1536]\n",
      "loss: 0.014945  [  100/ 1536]\n",
      "loss: 0.107024  [  200/ 1536]\n",
      "loss: 0.007661  [  300/ 1536]\n",
      "loss: 0.044537  [  400/ 1536]\n",
      "loss: 0.039288  [  500/ 1536]\n",
      "loss: 0.063681  [  600/ 1536]\n",
      "loss: 0.010581  [  700/ 1536]\n",
      "loss: 0.007813  [  800/ 1536]\n",
      "loss: 0.220410  [  900/ 1536]\n",
      "loss: 0.010192  [ 1000/ 1536]\n",
      "loss: 0.003135  [ 1100/ 1536]\n",
      "loss: 0.002847  [ 1200/ 1536]\n",
      "loss: 0.126997  [ 1300/ 1536]\n",
      "loss: 0.020238  [ 1400/ 1536]\n",
      "loss: 0.003870  [ 1500/ 1536]\n",
      "loss: 0.053475  [    0/ 1536]\n",
      "loss: 0.030382  [  100/ 1536]\n",
      "loss: 0.042573  [  200/ 1536]\n",
      "loss: 0.047261  [  300/ 1536]\n",
      "loss: 0.192725  [  400/ 1536]\n",
      "loss: 0.199330  [  500/ 1536]\n",
      "loss: 0.002773  [  600/ 1536]\n",
      "loss: 0.067722  [  700/ 1536]\n",
      "loss: 0.001083  [  800/ 1536]\n",
      "loss: 0.012953  [  900/ 1536]\n",
      "loss: 0.089155  [ 1000/ 1536]\n",
      "loss: 0.006306  [ 1100/ 1536]\n",
      "loss: 0.028366  [ 1200/ 1536]\n",
      "loss: 0.016090  [ 1300/ 1536]\n",
      "loss: 0.082951  [ 1400/ 1536]\n",
      "loss: 0.202679  [ 1500/ 1536]\n",
      "loss: 0.027220  [    0/ 1536]\n",
      "loss: 0.071940  [  100/ 1536]\n",
      "loss: 0.024951  [  200/ 1536]\n",
      "loss: 0.004184  [  300/ 1536]\n",
      "loss: 0.023167  [  400/ 1536]\n",
      "loss: 0.011338  [  500/ 1536]\n",
      "loss: 0.000149  [  600/ 1536]\n",
      "loss: 0.007240  [  700/ 1536]\n",
      "loss: 0.008803  [  800/ 1536]\n",
      "loss: 0.003328  [  900/ 1536]\n",
      "loss: 0.003620  [ 1000/ 1536]\n",
      "loss: 0.004564  [ 1100/ 1536]\n",
      "loss: 0.000016  [ 1200/ 1536]\n",
      "loss: 0.000663  [ 1300/ 1536]\n",
      "loss: 0.026235  [ 1400/ 1536]\n",
      "loss: 0.001238  [ 1500/ 1536]\n",
      "loss: 0.009636  [    0/ 1536]\n",
      "loss: 0.009670  [  100/ 1536]\n",
      "loss: 0.000372  [  200/ 1536]\n",
      "loss: 0.007754  [  300/ 1536]\n",
      "loss: 0.013814  [  400/ 1536]\n",
      "loss: 0.000849  [  500/ 1536]\n",
      "loss: 0.065367  [  600/ 1536]\n",
      "loss: 0.134068  [  700/ 1536]\n",
      "loss: 0.006107  [  800/ 1536]\n",
      "loss: 0.021551  [  900/ 1536]\n",
      "loss: 0.086678  [ 1000/ 1536]\n",
      "loss: 0.009346  [ 1100/ 1536]\n",
      "loss: 0.009522  [ 1200/ 1536]\n",
      "loss: 0.001809  [ 1300/ 1536]\n",
      "loss: 0.085893  [ 1400/ 1536]\n",
      "loss: 0.000035  [ 1500/ 1536]\n",
      "loss: 0.010044  [    0/ 1536]\n",
      "loss: 0.003418  [  100/ 1536]\n",
      "loss: 0.062612  [  200/ 1536]\n",
      "loss: 0.079464  [  300/ 1536]\n",
      "loss: 0.002994  [  400/ 1536]\n",
      "loss: 0.003165  [  500/ 1536]\n",
      "loss: 0.021162  [  600/ 1536]\n",
      "loss: 0.066680  [  700/ 1536]\n",
      "loss: 0.028623  [  800/ 1536]\n",
      "loss: 0.067058  [  900/ 1536]\n",
      "loss: 0.070024  [ 1000/ 1536]\n",
      "loss: 0.001183  [ 1100/ 1536]\n",
      "loss: 0.068627  [ 1200/ 1536]\n",
      "loss: 0.000000  [ 1300/ 1536]\n",
      "loss: 0.010395  [ 1400/ 1536]\n",
      "loss: 0.039742  [ 1500/ 1536]\n",
      "loss: 0.008466  [    0/ 1536]\n",
      "loss: 0.009633  [  100/ 1536]\n",
      "loss: 0.032983  [  200/ 1536]\n",
      "loss: 0.004102  [  300/ 1536]\n",
      "loss: 0.000042  [  400/ 1536]\n",
      "loss: 0.000399  [  500/ 1536]\n",
      "loss: 0.080764  [  600/ 1536]\n",
      "loss: 0.107591  [  700/ 1536]\n",
      "loss: 0.045174  [  800/ 1536]\n",
      "loss: 0.029779  [  900/ 1536]\n",
      "loss: 0.000417  [ 1000/ 1536]\n",
      "loss: 0.004900  [ 1100/ 1536]\n",
      "loss: 0.007465  [ 1200/ 1536]\n",
      "loss: 0.000081  [ 1300/ 1536]\n",
      "loss: 0.080623  [ 1400/ 1536]\n",
      "loss: 0.097669  [ 1500/ 1536]\n",
      "loss: 0.012389  [    0/ 1536]\n",
      "loss: 0.045515  [  100/ 1536]\n",
      "loss: 0.012944  [  200/ 1536]\n",
      "loss: 0.035144  [  300/ 1536]\n",
      "loss: 0.013555  [  400/ 1536]\n",
      "loss: 0.001736  [  500/ 1536]\n",
      "loss: 0.003716  [  600/ 1536]\n",
      "loss: 0.110427  [  700/ 1536]\n",
      "loss: 0.030890  [  800/ 1536]\n",
      "loss: 0.043624  [  900/ 1536]\n",
      "loss: 0.007493  [ 1000/ 1536]\n",
      "loss: 0.002753  [ 1100/ 1536]\n",
      "loss: 0.000126  [ 1200/ 1536]\n",
      "loss: 0.024760  [ 1300/ 1536]\n",
      "loss: 0.066700  [ 1400/ 1536]\n",
      "loss: 0.031911  [ 1500/ 1536]\n",
      "loss: 0.013339  [    0/ 1536]\n",
      "loss: 0.097181  [  100/ 1536]\n",
      "loss: 0.000012  [  200/ 1536]\n",
      "loss: 0.109784  [  300/ 1536]\n",
      "loss: 0.016673  [  400/ 1536]\n",
      "loss: 0.000226  [  500/ 1536]\n",
      "loss: 0.000796  [  600/ 1536]\n",
      "loss: 0.020500  [  700/ 1536]\n",
      "loss: 0.032936  [  800/ 1536]\n",
      "loss: 0.006286  [  900/ 1536]\n",
      "loss: 0.031559  [ 1000/ 1536]\n",
      "loss: 0.014076  [ 1100/ 1536]\n",
      "loss: 0.168011  [ 1200/ 1536]\n",
      "loss: 0.000207  [ 1300/ 1536]\n",
      "loss: 0.393886  [ 1400/ 1536]\n",
      "loss: 0.159101  [ 1500/ 1536]\n",
      "loss: 0.036516  [    0/ 1536]\n",
      "loss: 0.065858  [  100/ 1536]\n",
      "loss: 0.057840  [  200/ 1536]\n",
      "loss: 0.113754  [  300/ 1536]\n",
      "loss: 0.000450  [  400/ 1536]\n",
      "loss: 0.042357  [  500/ 1536]\n",
      "loss: 0.015385  [  600/ 1536]\n",
      "loss: 0.133445  [  700/ 1536]\n",
      "loss: 0.092144  [  800/ 1536]\n",
      "loss: 0.005348  [  900/ 1536]\n",
      "loss: 0.010718  [ 1000/ 1536]\n",
      "loss: 0.050107  [ 1100/ 1536]\n",
      "loss: 0.153120  [ 1200/ 1536]\n",
      "loss: 0.011814  [ 1300/ 1536]\n",
      "loss: 0.106871  [ 1400/ 1536]\n",
      "loss: 0.001303  [ 1500/ 1536]\n",
      "loss: 0.025350  [    0/ 1536]\n",
      "loss: 0.201004  [  100/ 1536]\n",
      "loss: 0.005208  [  200/ 1536]\n",
      "loss: 0.019602  [  300/ 1536]\n",
      "loss: 0.000188  [  400/ 1536]\n",
      "loss: 0.120494  [  500/ 1536]\n",
      "loss: 0.162009  [  600/ 1536]\n",
      "loss: 0.037973  [  700/ 1536]\n",
      "loss: 0.002879  [  800/ 1536]\n",
      "loss: 0.006478  [  900/ 1536]\n",
      "loss: 0.101975  [ 1000/ 1536]\n",
      "loss: 0.096176  [ 1100/ 1536]\n",
      "loss: 0.007525  [ 1200/ 1536]\n",
      "loss: 0.116457  [ 1300/ 1536]\n",
      "loss: 0.012659  [ 1400/ 1536]\n",
      "loss: 0.000046  [ 1500/ 1536]\n",
      "loss: 0.001714  [    0/ 1536]\n",
      "loss: 0.032083  [  100/ 1536]\n",
      "loss: 0.201587  [  200/ 1536]\n",
      "loss: 0.210784  [  300/ 1536]\n",
      "loss: 0.141215  [  400/ 1536]\n",
      "loss: 0.024382  [  500/ 1536]\n",
      "loss: 0.011074  [  600/ 1536]\n",
      "loss: 0.001428  [  700/ 1536]\n",
      "loss: 0.044996  [  800/ 1536]\n",
      "loss: 0.031484  [  900/ 1536]\n",
      "loss: 0.021762  [ 1000/ 1536]\n",
      "loss: 0.004184  [ 1100/ 1536]\n",
      "loss: 0.000034  [ 1200/ 1536]\n",
      "loss: 0.004480  [ 1300/ 1536]\n",
      "loss: 0.034740  [ 1400/ 1536]\n",
      "loss: 0.103984  [ 1500/ 1536]\n",
      "loss: 0.125083  [    0/ 1536]\n",
      "loss: 0.000657  [  100/ 1536]\n",
      "loss: 0.000119  [  200/ 1536]\n",
      "loss: 0.000047  [  300/ 1536]\n",
      "loss: 0.001325  [  400/ 1536]\n",
      "loss: 0.011272  [  500/ 1536]\n",
      "loss: 0.104718  [  600/ 1536]\n",
      "loss: 0.000657  [  700/ 1536]\n",
      "loss: 0.000065  [  800/ 1536]\n",
      "loss: 0.107390  [  900/ 1536]\n",
      "loss: 0.095255  [ 1000/ 1536]\n",
      "loss: 0.002209  [ 1100/ 1536]\n",
      "loss: 0.073524  [ 1200/ 1536]\n",
      "loss: 0.002263  [ 1300/ 1536]\n",
      "loss: 0.003139  [ 1400/ 1536]\n",
      "loss: 0.015253  [ 1500/ 1536]\n",
      "loss: 0.091870  [    0/ 1536]\n",
      "loss: 0.011660  [  100/ 1536]\n",
      "loss: 0.000207  [  200/ 1536]\n",
      "loss: 0.027216  [  300/ 1536]\n",
      "loss: 0.021463  [  400/ 1536]\n",
      "loss: 0.000348  [  500/ 1536]\n",
      "loss: 0.002410  [  600/ 1536]\n",
      "loss: 0.083686  [  700/ 1536]\n",
      "loss: 0.012298  [  800/ 1536]\n",
      "loss: 0.001827  [  900/ 1536]\n",
      "loss: 0.002938  [ 1000/ 1536]\n",
      "loss: 0.000166  [ 1100/ 1536]\n",
      "loss: 0.150550  [ 1200/ 1536]\n",
      "loss: 0.004222  [ 1300/ 1536]\n",
      "loss: 0.000113  [ 1400/ 1536]\n",
      "loss: 0.012905  [ 1500/ 1536]\n",
      "loss: 0.008284  [    0/ 1536]\n",
      "loss: 0.033159  [  100/ 1536]\n",
      "loss: 0.026326  [  200/ 1536]\n",
      "loss: 0.000006  [  300/ 1536]\n",
      "loss: 0.021909  [  400/ 1536]\n",
      "loss: 0.040511  [  500/ 1536]\n",
      "loss: 0.073705  [  600/ 1536]\n",
      "loss: 0.144494  [  700/ 1536]\n",
      "loss: 0.052215  [  800/ 1536]\n",
      "loss: 0.000041  [  900/ 1536]\n",
      "loss: 0.017895  [ 1000/ 1536]\n",
      "loss: 0.165223  [ 1100/ 1536]\n",
      "loss: 0.027531  [ 1200/ 1536]\n",
      "loss: 0.000072  [ 1300/ 1536]\n",
      "loss: 0.007972  [ 1400/ 1536]\n",
      "loss: 0.145610  [ 1500/ 1536]\n",
      "loss: 0.061584  [    0/ 1536]\n",
      "loss: 0.001695  [  100/ 1536]\n",
      "loss: 0.000232  [  200/ 1536]\n",
      "loss: 0.003919  [  300/ 1536]\n",
      "loss: 0.041599  [  400/ 1536]\n",
      "loss: 0.000076  [  500/ 1536]\n",
      "loss: 0.024207  [  600/ 1536]\n",
      "loss: 0.004572  [  700/ 1536]\n",
      "loss: 0.457386  [  800/ 1536]\n",
      "loss: 0.041034  [  900/ 1536]\n",
      "loss: 0.060357  [ 1000/ 1536]\n",
      "loss: 0.041829  [ 1100/ 1536]\n",
      "loss: 0.073903  [ 1200/ 1536]\n",
      "loss: 0.001507  [ 1300/ 1536]\n",
      "loss: 0.007112  [ 1400/ 1536]\n",
      "loss: 0.080986  [ 1500/ 1536]\n",
      "loss: 0.000078  [    0/ 1536]\n",
      "loss: 0.005166  [  100/ 1536]\n",
      "loss: 0.002553  [  200/ 1536]\n",
      "loss: 0.004017  [  300/ 1536]\n",
      "loss: 0.000743  [  400/ 1536]\n",
      "loss: 0.006903  [  500/ 1536]\n",
      "loss: 0.001203  [  600/ 1536]\n",
      "loss: 0.058137  [  700/ 1536]\n",
      "loss: 0.003960  [  800/ 1536]\n",
      "loss: 0.004688  [  900/ 1536]\n",
      "loss: 0.069916  [ 1000/ 1536]\n",
      "loss: 0.055685  [ 1100/ 1536]\n",
      "loss: 0.043523  [ 1200/ 1536]\n",
      "loss: 0.192752  [ 1300/ 1536]\n",
      "loss: 0.029943  [ 1400/ 1536]\n",
      "loss: 0.026829  [ 1500/ 1536]\n",
      "loss: 0.003462  [    0/ 1536]\n",
      "loss: 0.004882  [  100/ 1536]\n",
      "loss: 0.123675  [  200/ 1536]\n",
      "loss: 0.001261  [  300/ 1536]\n",
      "loss: 0.054694  [  400/ 1536]\n",
      "loss: 0.007593  [  500/ 1536]\n",
      "loss: 0.002892  [  600/ 1536]\n",
      "loss: 0.000401  [  700/ 1536]\n",
      "loss: 0.007098  [  800/ 1536]\n",
      "loss: 0.021076  [  900/ 1536]\n",
      "loss: 0.056647  [ 1000/ 1536]\n",
      "loss: 0.114030  [ 1100/ 1536]\n",
      "loss: 0.052169  [ 1200/ 1536]\n",
      "loss: 0.000012  [ 1300/ 1536]\n",
      "loss: 0.050387  [ 1400/ 1536]\n",
      "loss: 0.001643  [ 1500/ 1536]\n",
      "loss: 0.024381  [    0/ 1536]\n",
      "loss: 0.014806  [  100/ 1536]\n",
      "loss: 0.013264  [  200/ 1536]\n",
      "loss: 0.019444  [  300/ 1536]\n",
      "loss: 0.049128  [  400/ 1536]\n",
      "loss: 0.000356  [  500/ 1536]\n",
      "loss: 0.003714  [  600/ 1536]\n",
      "loss: 0.006786  [  700/ 1536]\n",
      "loss: 0.005994  [  800/ 1536]\n",
      "loss: 0.041377  [  900/ 1536]\n",
      "loss: 0.018754  [ 1000/ 1536]\n",
      "loss: 0.178579  [ 1100/ 1536]\n",
      "loss: 0.072261  [ 1200/ 1536]\n",
      "loss: 0.021489  [ 1300/ 1536]\n",
      "loss: 0.008376  [ 1400/ 1536]\n",
      "loss: 0.003838  [ 1500/ 1536]\n",
      "loss: 0.163352  [    0/ 1536]\n",
      "loss: 0.005111  [  100/ 1536]\n",
      "loss: 0.221907  [  200/ 1536]\n",
      "loss: 0.050959  [  300/ 1536]\n",
      "loss: 0.033509  [  400/ 1536]\n",
      "loss: 0.012464  [  500/ 1536]\n",
      "loss: 0.165934  [  600/ 1536]\n",
      "loss: 0.001887  [  700/ 1536]\n",
      "loss: 0.064849  [  800/ 1536]\n",
      "loss: 0.042393  [  900/ 1536]\n",
      "loss: 0.005019  [ 1000/ 1536]\n",
      "loss: 0.132740  [ 1100/ 1536]\n",
      "loss: 0.002285  [ 1200/ 1536]\n",
      "loss: 0.005818  [ 1300/ 1536]\n",
      "loss: 0.115912  [ 1400/ 1536]\n",
      "loss: 0.039145  [ 1500/ 1536]\n",
      "loss: 0.000005  [    0/ 1536]\n",
      "loss: 0.097882  [  100/ 1536]\n",
      "loss: 0.024839  [  200/ 1536]\n",
      "loss: 0.003605  [  300/ 1536]\n",
      "loss: 0.066003  [  400/ 1536]\n",
      "loss: 0.144909  [  500/ 1536]\n",
      "loss: 0.006915  [  600/ 1536]\n",
      "loss: 0.166866  [  700/ 1536]\n",
      "loss: 0.157102  [  800/ 1536]\n",
      "loss: 0.038902  [  900/ 1536]\n",
      "loss: 0.032026  [ 1000/ 1536]\n",
      "loss: 0.000453  [ 1100/ 1536]\n",
      "loss: 0.027122  [ 1200/ 1536]\n",
      "loss: 0.195266  [ 1300/ 1536]\n",
      "loss: 0.008751  [ 1400/ 1536]\n",
      "loss: 0.000137  [ 1500/ 1536]\n",
      "loss: 0.046588  [    0/ 1536]\n",
      "loss: 0.000149  [  100/ 1536]\n",
      "loss: 0.232277  [  200/ 1536]\n",
      "loss: 0.034884  [  300/ 1536]\n",
      "loss: 0.005316  [  400/ 1536]\n",
      "loss: 0.015177  [  500/ 1536]\n",
      "loss: 0.001160  [  600/ 1536]\n",
      "loss: 0.098195  [  700/ 1536]\n",
      "loss: 0.004363  [  800/ 1536]\n",
      "loss: 0.018265  [  900/ 1536]\n",
      "loss: 0.009012  [ 1000/ 1536]\n",
      "loss: 0.000026  [ 1100/ 1536]\n",
      "loss: 0.002062  [ 1200/ 1536]\n",
      "loss: 0.006321  [ 1300/ 1536]\n",
      "loss: 0.000131  [ 1400/ 1536]\n",
      "loss: 0.001330  [ 1500/ 1536]\n",
      "loss: 0.000842  [    0/ 1536]\n",
      "loss: 0.008261  [  100/ 1536]\n",
      "loss: 0.001985  [  200/ 1536]\n",
      "loss: 0.034221  [  300/ 1536]\n",
      "loss: 0.246907  [  400/ 1536]\n",
      "loss: 0.000228  [  500/ 1536]\n",
      "loss: 0.016500  [  600/ 1536]\n",
      "loss: 0.012416  [  700/ 1536]\n",
      "loss: 0.104658  [  800/ 1536]\n",
      "loss: 0.010271  [  900/ 1536]\n",
      "loss: 0.002053  [ 1000/ 1536]\n",
      "loss: 0.048736  [ 1100/ 1536]\n",
      "loss: 0.033382  [ 1200/ 1536]\n",
      "loss: 0.037372  [ 1300/ 1536]\n",
      "loss: 0.189846  [ 1400/ 1536]\n",
      "loss: 0.008213  [ 1500/ 1536]\n",
      "loss: 0.148681  [    0/ 1536]\n",
      "loss: 0.015844  [  100/ 1536]\n",
      "loss: 0.005542  [  200/ 1536]\n",
      "loss: 0.197143  [  300/ 1536]\n",
      "loss: 0.001075  [  400/ 1536]\n",
      "loss: 0.002157  [  500/ 1536]\n",
      "loss: 0.005686  [  600/ 1536]\n",
      "loss: 0.004575  [  700/ 1536]\n",
      "loss: 0.089859  [  800/ 1536]\n",
      "loss: 0.007617  [  900/ 1536]\n",
      "loss: 0.031658  [ 1000/ 1536]\n",
      "loss: 0.142795  [ 1100/ 1536]\n",
      "loss: 0.020861  [ 1200/ 1536]\n",
      "loss: 0.016841  [ 1300/ 1536]\n",
      "loss: 0.010170  [ 1400/ 1536]\n",
      "loss: 0.018754  [ 1500/ 1536]\n",
      "loss: 0.000185  [    0/ 1536]\n",
      "loss: 0.001363  [  100/ 1536]\n",
      "loss: 0.016144  [  200/ 1536]\n",
      "loss: 0.016221  [  300/ 1536]\n",
      "loss: 0.061393  [  400/ 1536]\n",
      "loss: 0.037489  [  500/ 1536]\n",
      "loss: 0.003821  [  600/ 1536]\n",
      "loss: 0.028089  [  700/ 1536]\n",
      "loss: 0.001075  [  800/ 1536]\n",
      "loss: 0.130650  [  900/ 1536]\n",
      "loss: 0.046534  [ 1000/ 1536]\n",
      "loss: 0.008857  [ 1100/ 1536]\n",
      "loss: 0.019194  [ 1200/ 1536]\n",
      "loss: 0.143079  [ 1300/ 1536]\n",
      "loss: 0.007814  [ 1400/ 1536]\n",
      "loss: 0.026227  [ 1500/ 1536]\n",
      "loss: 0.009202  [    0/ 1536]\n",
      "loss: 0.000386  [  100/ 1536]\n",
      "loss: 0.061729  [  200/ 1536]\n",
      "loss: 0.007295  [  300/ 1536]\n",
      "loss: 0.027435  [  400/ 1536]\n",
      "loss: 0.041579  [  500/ 1536]\n",
      "loss: 0.016618  [  600/ 1536]\n",
      "loss: 0.001556  [  700/ 1536]\n",
      "loss: 0.183436  [  800/ 1536]\n",
      "loss: 0.001688  [  900/ 1536]\n",
      "loss: 0.100078  [ 1000/ 1536]\n",
      "loss: 0.008157  [ 1100/ 1536]\n",
      "loss: 0.003043  [ 1200/ 1536]\n",
      "loss: 0.009814  [ 1300/ 1536]\n",
      "loss: 0.004279  [ 1400/ 1536]\n",
      "loss: 0.019566  [ 1500/ 1536]\n",
      "loss: 0.000002  [    0/ 1536]\n",
      "loss: 0.073843  [  100/ 1536]\n",
      "loss: 0.001956  [  200/ 1536]\n",
      "loss: 0.049684  [  300/ 1536]\n",
      "loss: 0.003424  [  400/ 1536]\n",
      "loss: 0.000682  [  500/ 1536]\n",
      "loss: 0.006870  [  600/ 1536]\n",
      "loss: 0.076144  [  700/ 1536]\n",
      "loss: 0.001441  [  800/ 1536]\n",
      "loss: 0.059462  [  900/ 1536]\n",
      "loss: 0.000036  [ 1000/ 1536]\n",
      "loss: 0.180328  [ 1100/ 1536]\n",
      "loss: 0.442853  [ 1200/ 1536]\n",
      "loss: 0.007516  [ 1300/ 1536]\n",
      "loss: 0.203607  [ 1400/ 1536]\n",
      "loss: 0.015654  [ 1500/ 1536]\n",
      "loss: 0.176811  [    0/ 1536]\n",
      "loss: 0.205682  [  100/ 1536]\n",
      "loss: 0.110669  [  200/ 1536]\n",
      "loss: 0.217786  [  300/ 1536]\n",
      "loss: 0.156017  [  400/ 1536]\n",
      "loss: 0.006569  [  500/ 1536]\n",
      "loss: 0.005887  [  600/ 1536]\n",
      "loss: 0.026846  [  700/ 1536]\n",
      "loss: 0.019097  [  800/ 1536]\n",
      "loss: 0.056809  [  900/ 1536]\n",
      "loss: 0.095936  [ 1000/ 1536]\n",
      "loss: 0.001552  [ 1100/ 1536]\n",
      "loss: 0.034191  [ 1200/ 1536]\n",
      "loss: 0.013428  [ 1300/ 1536]\n",
      "loss: 0.006612  [ 1400/ 1536]\n",
      "loss: 0.000941  [ 1500/ 1536]\n",
      "loss: 0.023861  [    0/ 1536]\n",
      "loss: 0.115454  [  100/ 1536]\n",
      "loss: 0.012316  [  200/ 1536]\n",
      "loss: 0.030529  [  300/ 1536]\n",
      "loss: 0.001187  [  400/ 1536]\n",
      "loss: 0.072070  [  500/ 1536]\n",
      "loss: 0.075721  [  600/ 1536]\n",
      "loss: 0.042893  [  700/ 1536]\n",
      "loss: 0.101199  [  800/ 1536]\n",
      "loss: 0.026980  [  900/ 1536]\n",
      "loss: 0.038802  [ 1000/ 1536]\n",
      "loss: 0.015843  [ 1100/ 1536]\n",
      "loss: 0.018530  [ 1200/ 1536]\n",
      "loss: 0.019547  [ 1300/ 1536]\n",
      "loss: 0.024942  [ 1400/ 1536]\n",
      "loss: 0.023819  [ 1500/ 1536]\n",
      "loss: 0.004012  [    0/ 1536]\n",
      "loss: 0.007743  [  100/ 1536]\n",
      "loss: 0.043154  [  200/ 1536]\n",
      "loss: 0.005078  [  300/ 1536]\n",
      "loss: 0.005619  [  400/ 1536]\n",
      "loss: 0.000001  [  500/ 1536]\n",
      "loss: 0.000859  [  600/ 1536]\n",
      "loss: 0.052335  [  700/ 1536]\n",
      "loss: 0.095027  [  800/ 1536]\n",
      "loss: 0.000119  [  900/ 1536]\n",
      "loss: 0.004050  [ 1000/ 1536]\n",
      "loss: 0.002314  [ 1100/ 1536]\n",
      "loss: 0.024030  [ 1200/ 1536]\n",
      "loss: 0.006047  [ 1300/ 1536]\n",
      "loss: 0.120545  [ 1400/ 1536]\n",
      "loss: 0.056038  [ 1500/ 1536]\n",
      "loss: 0.057635  [    0/ 1536]\n",
      "loss: 0.013780  [  100/ 1536]\n",
      "loss: 0.013225  [  200/ 1536]\n",
      "loss: 0.041151  [  300/ 1536]\n",
      "loss: 0.014502  [  400/ 1536]\n",
      "loss: 0.002601  [  500/ 1536]\n",
      "loss: 0.027614  [  600/ 1536]\n",
      "loss: 0.001483  [  700/ 1536]\n",
      "loss: 0.006677  [  800/ 1536]\n",
      "loss: 0.050583  [  900/ 1536]\n",
      "loss: 0.000002  [ 1000/ 1536]\n",
      "loss: 0.087454  [ 1100/ 1536]\n",
      "loss: 0.194249  [ 1200/ 1536]\n",
      "loss: 0.000732  [ 1300/ 1536]\n",
      "loss: 0.004541  [ 1400/ 1536]\n",
      "loss: 0.000387  [ 1500/ 1536]\n",
      "loss: 0.050470  [    0/ 1536]\n",
      "loss: 0.001797  [  100/ 1536]\n",
      "loss: 0.031033  [  200/ 1536]\n",
      "loss: 0.454698  [  300/ 1536]\n",
      "loss: 0.045848  [  400/ 1536]\n",
      "loss: 0.052135  [  500/ 1536]\n",
      "loss: 0.020990  [  600/ 1536]\n",
      "loss: 0.091910  [  700/ 1536]\n",
      "loss: 0.028890  [  800/ 1536]\n",
      "loss: 0.006485  [  900/ 1536]\n",
      "loss: 0.032616  [ 1000/ 1536]\n",
      "loss: 0.004517  [ 1100/ 1536]\n",
      "loss: 0.048979  [ 1200/ 1536]\n",
      "loss: 0.074372  [ 1300/ 1536]\n",
      "loss: 0.038420  [ 1400/ 1536]\n",
      "loss: 0.000867  [ 1500/ 1536]\n",
      "loss: 0.038263  [    0/ 1536]\n",
      "loss: 0.000145  [  100/ 1536]\n",
      "loss: 0.006843  [  200/ 1536]\n",
      "loss: 0.006341  [  300/ 1536]\n",
      "loss: 0.009334  [  400/ 1536]\n",
      "loss: 0.000156  [  500/ 1536]\n",
      "loss: 0.064543  [  600/ 1536]\n",
      "loss: 0.002489  [  700/ 1536]\n",
      "loss: 0.000089  [  800/ 1536]\n",
      "loss: 0.009300  [  900/ 1536]\n",
      "loss: 0.020974  [ 1000/ 1536]\n",
      "loss: 0.011512  [ 1100/ 1536]\n",
      "loss: 0.006415  [ 1200/ 1536]\n",
      "loss: 0.120262  [ 1300/ 1536]\n",
      "loss: 0.045226  [ 1400/ 1536]\n",
      "loss: 0.006553  [ 1500/ 1536]\n",
      "loss: 0.005911  [    0/ 1536]\n",
      "loss: 0.002925  [  100/ 1536]\n",
      "loss: 0.167979  [  200/ 1536]\n",
      "loss: 0.011035  [  300/ 1536]\n",
      "loss: 0.055617  [  400/ 1536]\n",
      "loss: 0.004825  [  500/ 1536]\n",
      "loss: 0.026944  [  600/ 1536]\n",
      "loss: 0.034703  [  700/ 1536]\n",
      "loss: 0.000013  [  800/ 1536]\n",
      "loss: 0.003040  [  900/ 1536]\n",
      "loss: 0.003663  [ 1000/ 1536]\n",
      "loss: 0.031076  [ 1100/ 1536]\n",
      "loss: 0.000872  [ 1200/ 1536]\n",
      "loss: 0.074030  [ 1300/ 1536]\n",
      "loss: 0.042845  [ 1400/ 1536]\n",
      "loss: 0.059786  [ 1500/ 1536]\n",
      "loss: 0.004518  [    0/ 1536]\n",
      "loss: 0.015862  [  100/ 1536]\n",
      "loss: 0.092429  [  200/ 1536]\n",
      "loss: 0.035494  [  300/ 1536]\n",
      "loss: 0.156224  [  400/ 1536]\n",
      "loss: 0.017437  [  500/ 1536]\n",
      "loss: 0.011882  [  600/ 1536]\n",
      "loss: 0.008183  [  700/ 1536]\n",
      "loss: 0.043407  [  800/ 1536]\n",
      "loss: 0.004929  [  900/ 1536]\n",
      "loss: 0.150776  [ 1000/ 1536]\n",
      "loss: 0.002528  [ 1100/ 1536]\n",
      "loss: 0.000289  [ 1200/ 1536]\n",
      "loss: 0.022365  [ 1300/ 1536]\n",
      "loss: 0.048649  [ 1400/ 1536]\n",
      "loss: 0.001475  [ 1500/ 1536]\n",
      "loss: 0.003748  [    0/ 1536]\n",
      "loss: 0.013017  [  100/ 1536]\n",
      "loss: 0.015452  [  200/ 1536]\n",
      "loss: 0.000072  [  300/ 1536]\n",
      "loss: 0.024443  [  400/ 1536]\n",
      "loss: 0.122746  [  500/ 1536]\n",
      "loss: 0.021236  [  600/ 1536]\n",
      "loss: 0.027761  [  700/ 1536]\n",
      "loss: 0.001014  [  800/ 1536]\n",
      "loss: 0.001091  [  900/ 1536]\n",
      "loss: 0.003010  [ 1000/ 1536]\n",
      "loss: 0.001450  [ 1100/ 1536]\n",
      "loss: 0.069065  [ 1200/ 1536]\n",
      "loss: 0.002915  [ 1300/ 1536]\n",
      "loss: 0.000507  [ 1400/ 1536]\n",
      "loss: 0.005406  [ 1500/ 1536]\n",
      "loss: 0.039238  [    0/ 1536]\n",
      "loss: 0.315121  [  100/ 1536]\n",
      "loss: 0.106911  [  200/ 1536]\n",
      "loss: 0.010302  [  300/ 1536]\n",
      "loss: 0.001586  [  400/ 1536]\n",
      "loss: 0.011534  [  500/ 1536]\n",
      "loss: 0.039434  [  600/ 1536]\n",
      "loss: 0.000735  [  700/ 1536]\n",
      "loss: 0.012918  [  800/ 1536]\n",
      "loss: 0.002386  [  900/ 1536]\n",
      "loss: 0.125919  [ 1000/ 1536]\n",
      "loss: 0.112935  [ 1100/ 1536]\n",
      "loss: 0.020883  [ 1200/ 1536]\n",
      "loss: 0.005211  [ 1300/ 1536]\n",
      "loss: 0.100436  [ 1400/ 1536]\n",
      "loss: 0.099647  [ 1500/ 1536]\n",
      "loss: 0.013727  [    0/ 1536]\n",
      "loss: 0.043798  [  100/ 1536]\n",
      "loss: 0.024874  [  200/ 1536]\n",
      "loss: 0.002981  [  300/ 1536]\n",
      "loss: 0.002058  [  400/ 1536]\n",
      "loss: 0.000510  [  500/ 1536]\n",
      "loss: 0.006871  [  600/ 1536]\n",
      "loss: 0.000961  [  700/ 1536]\n",
      "loss: 0.002653  [  800/ 1536]\n",
      "loss: 0.011226  [  900/ 1536]\n",
      "loss: 0.000593  [ 1000/ 1536]\n",
      "loss: 0.004218  [ 1100/ 1536]\n",
      "loss: 0.036600  [ 1200/ 1536]\n",
      "loss: 0.007076  [ 1300/ 1536]\n",
      "loss: 0.030612  [ 1400/ 1536]\n",
      "loss: 0.104897  [ 1500/ 1536]\n",
      "loss: 0.000002  [    0/ 1536]\n",
      "loss: 0.008706  [  100/ 1536]\n",
      "loss: 0.001657  [  200/ 1536]\n",
      "loss: 0.007901  [  300/ 1536]\n",
      "loss: 0.003873  [  400/ 1536]\n",
      "loss: 0.004912  [  500/ 1536]\n",
      "loss: 0.011102  [  600/ 1536]\n",
      "loss: 0.004261  [  700/ 1536]\n",
      "loss: 0.005530  [  800/ 1536]\n",
      "loss: 0.088674  [  900/ 1536]\n",
      "loss: 0.027445  [ 1000/ 1536]\n",
      "loss: 0.004473  [ 1100/ 1536]\n",
      "loss: 0.006839  [ 1200/ 1536]\n",
      "loss: 0.000017  [ 1300/ 1536]\n",
      "loss: 0.088533  [ 1400/ 1536]\n",
      "loss: 0.070445  [ 1500/ 1536]\n",
      "loss: 0.001137  [    0/ 1536]\n",
      "loss: 0.013229  [  100/ 1536]\n",
      "loss: 0.029065  [  200/ 1536]\n",
      "loss: 0.002637  [  300/ 1536]\n",
      "loss: 0.075320  [  400/ 1536]\n",
      "loss: 0.001012  [  500/ 1536]\n",
      "loss: 0.000774  [  600/ 1536]\n",
      "loss: 0.000577  [  700/ 1536]\n",
      "loss: 0.039326  [  800/ 1536]\n",
      "loss: 0.029516  [  900/ 1536]\n",
      "loss: 0.191649  [ 1000/ 1536]\n",
      "loss: 0.000612  [ 1100/ 1536]\n",
      "loss: 0.060014  [ 1200/ 1536]\n",
      "loss: 0.003971  [ 1300/ 1536]\n",
      "loss: 0.015941  [ 1400/ 1536]\n",
      "loss: 0.007501  [ 1500/ 1536]\n",
      "loss: 0.010895  [    0/ 1536]\n",
      "loss: 0.016864  [  100/ 1536]\n",
      "loss: 0.063760  [  200/ 1536]\n",
      "loss: 0.001222  [  300/ 1536]\n",
      "loss: 0.075750  [  400/ 1536]\n",
      "loss: 0.005966  [  500/ 1536]\n",
      "loss: 0.006681  [  600/ 1536]\n",
      "loss: 0.114325  [  700/ 1536]\n",
      "loss: 0.010657  [  800/ 1536]\n",
      "loss: 0.000664  [  900/ 1536]\n",
      "loss: 0.214829  [ 1000/ 1536]\n",
      "loss: 0.004710  [ 1100/ 1536]\n",
      "loss: 0.045955  [ 1200/ 1536]\n",
      "loss: 0.006828  [ 1300/ 1536]\n",
      "loss: 0.020802  [ 1400/ 1536]\n",
      "loss: 0.142987  [ 1500/ 1536]\n",
      "loss: 0.039453  [    0/ 1536]\n",
      "loss: 0.010475  [  100/ 1536]\n",
      "loss: 0.075164  [  200/ 1536]\n",
      "loss: 0.194885  [  300/ 1536]\n",
      "loss: 0.000614  [  400/ 1536]\n",
      "loss: 0.011846  [  500/ 1536]\n",
      "loss: 0.039692  [  600/ 1536]\n",
      "loss: 0.047460  [  700/ 1536]\n",
      "loss: 0.024118  [  800/ 1536]\n",
      "loss: 0.064992  [  900/ 1536]\n",
      "loss: 0.000527  [ 1000/ 1536]\n",
      "loss: 0.003466  [ 1100/ 1536]\n",
      "loss: 0.067771  [ 1200/ 1536]\n",
      "loss: 0.061124  [ 1300/ 1536]\n",
      "loss: 0.145720  [ 1400/ 1536]\n",
      "loss: 0.007105  [ 1500/ 1536]\n",
      "loss: 0.007091  [    0/ 1536]\n",
      "loss: 0.019616  [  100/ 1536]\n",
      "loss: 0.200105  [  200/ 1536]\n",
      "loss: 0.281154  [  300/ 1536]\n",
      "loss: 0.014399  [  400/ 1536]\n",
      "loss: 0.000000  [  500/ 1536]\n",
      "loss: 0.135138  [  600/ 1536]\n",
      "loss: 0.001723  [  700/ 1536]\n",
      "loss: 0.004396  [  800/ 1536]\n",
      "loss: 0.012648  [  900/ 1536]\n",
      "loss: 0.105072  [ 1000/ 1536]\n",
      "loss: 0.038385  [ 1100/ 1536]\n",
      "loss: 0.008150  [ 1200/ 1536]\n",
      "loss: 0.000669  [ 1300/ 1536]\n",
      "loss: 0.015157  [ 1400/ 1536]\n",
      "loss: 0.005960  [ 1500/ 1536]\n",
      "loss: 0.026870  [    0/ 1536]\n",
      "loss: 0.006863  [  100/ 1536]\n",
      "loss: 0.028874  [  200/ 1536]\n",
      "loss: 0.000020  [  300/ 1536]\n",
      "loss: 0.116517  [  400/ 1536]\n",
      "loss: 0.084957  [  500/ 1536]\n",
      "loss: 0.000779  [  600/ 1536]\n",
      "loss: 0.055463  [  700/ 1536]\n",
      "loss: 0.002840  [  800/ 1536]\n",
      "loss: 0.102801  [  900/ 1536]\n",
      "loss: 0.076173  [ 1000/ 1536]\n",
      "loss: 0.000332  [ 1100/ 1536]\n",
      "loss: 0.070474  [ 1200/ 1536]\n",
      "loss: 0.057127  [ 1300/ 1536]\n",
      "loss: 0.028933  [ 1400/ 1536]\n",
      "loss: 0.007380  [ 1500/ 1536]\n",
      "loss: 0.103303  [    0/ 1536]\n",
      "loss: 0.066681  [  100/ 1536]\n",
      "loss: 0.004804  [  200/ 1536]\n",
      "loss: 0.003141  [  300/ 1536]\n",
      "loss: 0.031645  [  400/ 1536]\n",
      "loss: 0.066214  [  500/ 1536]\n",
      "loss: 0.151265  [  600/ 1536]\n",
      "loss: 0.029505  [  700/ 1536]\n",
      "loss: 0.000159  [  800/ 1536]\n",
      "loss: 0.092568  [  900/ 1536]\n",
      "loss: 0.010996  [ 1000/ 1536]\n",
      "loss: 0.035295  [ 1100/ 1536]\n",
      "loss: 0.001063  [ 1200/ 1536]\n",
      "loss: 0.015980  [ 1300/ 1536]\n",
      "loss: 0.000241  [ 1400/ 1536]\n",
      "loss: 0.011358  [ 1500/ 1536]\n",
      "loss: 0.002356  [    0/ 1536]\n",
      "loss: 0.115890  [  100/ 1536]\n",
      "loss: 0.134346  [  200/ 1536]\n",
      "loss: 0.021865  [  300/ 1536]\n",
      "loss: 0.000074  [  400/ 1536]\n",
      "loss: 0.037323  [  500/ 1536]\n",
      "loss: 0.000021  [  600/ 1536]\n",
      "loss: 0.000877  [  700/ 1536]\n",
      "loss: 0.000484  [  800/ 1536]\n",
      "loss: 0.032887  [  900/ 1536]\n",
      "loss: 0.074521  [ 1000/ 1536]\n",
      "loss: 0.019373  [ 1100/ 1536]\n",
      "loss: 0.000000  [ 1200/ 1536]\n",
      "loss: 0.001497  [ 1300/ 1536]\n",
      "loss: 0.000289  [ 1400/ 1536]\n",
      "loss: 0.065392  [ 1500/ 1536]\n",
      "loss: 0.007018  [    0/ 1536]\n",
      "loss: 0.008415  [  100/ 1536]\n",
      "loss: 0.089783  [  200/ 1536]\n",
      "loss: 0.100132  [  300/ 1536]\n",
      "loss: 0.002964  [  400/ 1536]\n",
      "loss: 0.000183  [  500/ 1536]\n",
      "loss: 0.031088  [  600/ 1536]\n",
      "loss: 0.060626  [  700/ 1536]\n",
      "loss: 0.003209  [  800/ 1536]\n",
      "loss: 0.104822  [  900/ 1536]\n",
      "loss: 0.012290  [ 1000/ 1536]\n",
      "loss: 0.000026  [ 1100/ 1536]\n",
      "loss: 0.000606  [ 1200/ 1536]\n",
      "loss: 0.073836  [ 1300/ 1536]\n",
      "loss: 0.029010  [ 1400/ 1536]\n",
      "loss: 0.021652  [ 1500/ 1536]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('stack.0.weight',\n",
       "              tensor([[ 0.4937, -0.5461],\n",
       "                      [-0.5199,  0.3299],\n",
       "                      [-0.8803, -0.1291],\n",
       "                      [ 0.0139, -0.1499],\n",
       "                      [-0.0717,  0.0534],\n",
       "                      [-0.6146, -0.5296],\n",
       "                      [-0.2350,  0.1356],\n",
       "                      [ 0.0263, -0.0972],\n",
       "                      [ 0.4822,  0.5150],\n",
       "                      [-0.4421,  0.6307]])),\n",
       "             ('stack.0.bias',\n",
       "              tensor([-0.2991, -0.0216,  0.8711, -0.0269, -0.2996, -0.0379,  0.5708,  1.1408,\n",
       "                      -0.8981,  0.6996])),\n",
       "             ('stack.2.weight',\n",
       "              tensor([[ 0.2464,  0.4884, -1.1437, -0.1888, -0.1078,  0.1264,  0.8006,  0.3658,\n",
       "                       -1.6183,  0.1611]])),\n",
       "             ('stack.2.bias', tensor([0.2295]))])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = range(500)\n",
    "for eporch in epochs:\n",
    "    train(data_train_loader, model, loss_fn, optimizer)\n",
    "    \n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8600, 0.8404])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1097])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[23][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1132])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(data_test[23][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with data\n",
    "There are two `classes` that we usually use to working with the data\n",
    "\n",
    "<div align=\"center\">\n",
    "    <span style =\"background-color:rgb(220, 220, 220);color:rgb(192, 57, 57)\">torch.utils.data.DataLoader</span> and <span style=\"background-color:rgb(220, 220, 220);color:rgb(192, 57, 57)\">torch.utils.data.Dataset </span>\n",
    "</div>\n",
    "\n",
    "`Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset\n",
    "\n",
    "\n",
    "<!-- torch.utils.data.DataLoader and `torch.utils.data.Dataset. Dataset -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as `TorchText`, `TorchVision`, and `TorchAudio`, all of which include datasets. For this tutorial, we will be using a `TorchVision` dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like `CIFAR`, `COCO`.\n",
    "\n",
    "In this tutorial, we use the `FashionMNIST` dataset. Every TorchVision `Dataset` includes two arguments:\n",
    "\n",
    "* `root` is the name of the directory where the files will be stored from the current directory.\n",
    "* `train` is boolean type.\n",
    "* `download` point out if we want to download or not\n",
    "* `transform` and `target_transform` to modify the samples and labels respectively.\n",
    "\n",
    "\n",
    "> `ToTensor()` Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root='data-pytorch',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data-pytorch',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the images that represent the data, we use the followinf code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAKQCAYAAAABnneSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABn30lEQVR4nO3deXxV5bX/8W+AkJCBDCQkjGFQZhxwBAdQBEqdqmJrLRXUOuBV77ValVtFxKsoVtte/aHWKqjtVawjzlVBvAoKOCGC4MQchoQpQEJCsn9/+CK3kWc9cI4JSXg+79eLP1j7rL33Odn77MUma+2EKIoiAQAA4IDXpL53AAAAAPsHhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhd8+SEhI2Kc/77zzjrmON954Q0OHDlXbtm2VlJSktm3batCgQbrzzjv32NaVV165132aOnWqEhIStGzZsn16D5MnT9bUqVP36bXA/lQb5xcA24cffqizzjpLHTt2VFJSkvLy8tS/f39de+219b1rkqROnTrptNNOq+/dCEaz+t6BxmDOnDk1/n7bbbdp5syZmjFjRo14r169nPkPPvigxowZo3POOUf333+/srOztXLlSs2ePVvPPPOMbrzxxpj36dRTT9WcOXPUpk2bfXr95MmTlZOTo9GjR8e8LaAu/djzC4DtlVde0RlnnKFBgwZp0qRJatOmjQoLCzV//nw99dRTuueee+p7F7GfUfjtg2OPPbbG33Nzc9WkSZM94paJEyfqxBNP1DPPPFMj/utf/1pVVVVx7VNubq5yc3P3+rodO3YoJSUlrm0A+0O851djPbYb636jcZo0aZI6d+6sN954Q82a/d8l/7zzztOkSZPqcc/2H865mviv3v2guLjYvDPXpIn7R/DEE0+oZ8+eSklJ0aGHHqqXX365xnLXf/UOGjRIffr00bvvvqsBAwYoJSVFF110kTp16qQvvvhCs2bNqv5vs06dOtXW2wPqnHVsS9KKFSs0cuRItW7dWklJSerZs6fuueeeGv+oeuedd5z/Xbxs2TIlJCTU+DWIb7/9Vuedd171r2Xk5eVp8ODB+vTTT2vkTps2Tf3791dqaqrS0tI0bNgwffLJJzVeM3r0aKWlpenzzz/X0KFDlZ6ersGDB9fqZwP4FBcXKycnp0bRt9u/Xn92/3fr66+/rn79+qlFixbq0aOHHn300T3y1q5dq8suu0zt27dX8+bN1blzZ916663atWtXjdfdeuutOuaYY5Sdna2WLVuqX79+euSRRxRF0V73e/LkyWrWrJluueWW6thbb72lwYMHq2XLlkpJSdFxxx2nt99+u0be+PHjlZCQoI8//lgjRoxQVlaWunbtutfthYQ7fvtB//799eyzz2r8+PE666yz1KdPHzVt2tR8/SuvvKJ58+ZpwoQJSktL06RJk3TWWWdpyZIl6tKli3dbhYWFGjlypK6//nrdcccdatKkiW644QaNGDFCGRkZmjx5siQpKSmpVt8jUNdcx/aGDRs0YMAAlZeX67bbblOnTp308ssv67rrrtM333xTfbzH4qc//akqKys1adIkdezYUUVFRZo9e7Y2b95c/Zo77rhDN910ky688ELddNNNKi8v1913360TTjhBc+fOrfHf0uXl5TrjjDN02WWX6cYbb9zj4gjUpf79++uvf/2rrr76av3qV79Sv379lJiY6HztZ599pmuvvVY33nij8vLy9Ne//lUXX3yxDjroIJ144omSvi/6jj76aDVp0kTjxo1T165dNWfOHP3Xf/2Xli1bpilTplSvb9myZbrsssvUsWNHSdIHH3ygq666SqtXr9a4ceOc+xBFkX73u9/pv//7v/XXv/61+teT/va3v+mCCy7QmWeeqccee0yJiYl66KGHNGzYML3xxht7/IPq7LPP1nnnnafLL79c27dv/7Ef44ElQsxGjRoVpaam7vPrv/7666hPnz6RpEhS1KJFi2jw4MHR/fffH5WXl9d4raQoLy8v2rp1a3Vs7dq1UZMmTaKJEydWx6ZMmRJJir777rvq2MCBAyNJ0dtvv73HPvTu3TsaOHDgvr9JoJ64zi/r2L7xxhsjSdGHH35YIz5mzJgoISEhWrJkSRRFUTRz5sxIUjRz5swar/vuu+8iSdGUKVOiKIqioqKiSFL0pz/9ydy/FStWRM2aNYuuuuqqGvGSkpIoPz8/+vnPf17jvUiKHn300X1670BtKyoqio4//vjq609iYmI0YMCAaOLEiVFJSUn16woKCqLk5ORo+fLl1bHS0tIoOzs7uuyyy6pjl112WZSWllbjdVEURX/4wx8iSdEXX3zh3I/KysqooqIimjBhQtSqVauoqqqqxrZPPfXUaMeOHdE555wTZWRkRG+99Vb18u3bt0fZ2dnR6aefvsc6Dz300Ojoo4+ujt1yyy2RpGjcuHExflLh4L96a0kURdq1a1eNP7t17dpVn332mWbNmqVbb71Vp5xyiubNm6crr7xS/fv3V1lZWY11nXTSSUpPT6/+e15enlq3bq3ly5fvdT+ysrJ08skn194bAxoI17E9Y8YM9erVS0cffXSN+OjRoxVF0R4NInuTnZ2trl276u6779a9996rTz75ZI/fw33jjTe0a9cuXXDBBTXO9+TkZA0cONDZfXzOOefEtB9AbWnVqpX+93//V/PmzdOdd96pM888U0uXLtXYsWPVt29fFRUVVb/2sMMOq747J0nJycnq1q1bjWvPyy+/rJNOOklt27atcfwPHz5ckjRr1qzq186YMUOnnHKKMjIy1LRpUyUmJmrcuHEqLi7W+vXra+xncXGxTj75ZM2dO1fvvfdejTt4s2fP1saNGzVq1Kga26yqqtJPfvITzZs3b4+7epxzNgq/WrL71vO//vlXTZo00Yknnqhx48Zp+vTpWrNmjX7xi1/oo48+2uN3KFq1arXH+pOSklRaWrrX/djXLl+gsXEd29bvz7Zt27Z6eSwSEhL09ttva9iwYZo0aZL69eun3NxcXX311SopKZEkrVu3TpJ01FFH7XHOT5s2rcaFVJJSUlLUsmXLmPYDqG1HHnmkbrjhBv3jH//QmjVrdM0112jZsmU1Gjz25dqzbt06vfTSS3sc+71795ak6uN/7ty5Gjp0qCTp4Ycf1vvvv6958+bp97//vSTtcT1bunSpPvzwQw0fPlx9+vSpsWz3OTdixIg9tnvXXXcpiiJt3LixRg7XQhu/41dLTj/9dM2bN2+fX5+amqqxY8dq2rRpWrhwYa3tR0JCQq2tC2hIXMd2q1atVFhYuEd8zZo1kqScnBxJ39+5kKSdO3fWeN0PizRJKigo0COPPCLp+4vR008/rfHjx6u8vFwPPvhg9TqfeeYZFRQUxLXfQH1KTEzULbfcoj/+8Y8xX39ycnJ0yCGH6Pbbb3cu3/2PrqeeekqJiYl6+eWXq88/SXrhhRecef3799e5556riy++WJL0wAMPVDef7D7n7rvvPrPbPy8vr8bfOe9sFH61pFWrVs5/LUnf/1K6618fixcvlvR/J0pd2tc7hkBjMnjwYE2cOFEff/yx+vXrVx1//PHHlZCQoJNOOkmSqrvYFyxYoGHDhlW/bvr06d71d+vWTTfddJOeffZZffzxx5KkYcOGqVmzZvrmm2/47yQ0eLV9/TnttNP06quvqmvXrsrKyjJfl5CQoGbNmtVoZCwtLdUTTzxh5owaNUqpqak6//zztX37dj322GNq2rSpjjvuOGVmZmrRokX79IAD+FH47Qe9e/fW4MGDNXz4cHXt2lVlZWX68MMPdc899ygvL6/6Xzh1qW/fvnrqqac0bdo0denSRcnJyerbt2+dbxeoS9dcc40ef/xxnXrqqZowYYIKCgr0yiuvaPLkyRozZoy6desmScrPz9cpp5yiiRMnKisrSwUFBXr77bf13HPP1VjfggULdOWVV+rcc8/VwQcfrObNm2vGjBlasGBB9aD1Tp06acKECfr973+vb7/9Vj/5yU+UlZWldevWae7cuUpNTdWtt9663z8LwGXYsGFq3769Tj/9dPXo0UNVVVX69NNPdc899ygtLU3//u//HtP6JkyYoDfffFMDBgzQ1Vdfre7du6usrEzLli3Tq6++qgcffFDt27fXqaeeqnvvvVfnn3++Lr30UhUXF+sPf/jDXidKjBgxQikpKRoxYoRKS0v15JNPKi0tTffdd59GjRqljRs3asSIEWrdurU2bNigzz77TBs2bNADDzzwYz6moFD47Qd33nmn3njjDd1+++1au3atdu3apQ4dOuj888/X73//+/3yuwi33nqrCgsLdckll6ikpEQFBQX7/Lg3oKHKzc3V7NmzNXbsWI0dO1Zbt25Vly5dNGnSJP32t7+t8donnnhCV111lW644QZVVlbq9NNP15NPPqkjjzyy+jX5+fnq2rWrJk+erJUrVyohIUFdunTRPffco6uuuqr6dWPHjlWvXr305z//WU8++aR27typ/Px8HXXUUbr88sv32/sH9uamm27Siy++qD/+8Y8qLCzUzp071aZNG51yyikaO3asevbsGdP62rRpo/nz5+u2227T3XffrVWrVik9PV2dO3eu/keQJJ188sl69NFHddddd+n0009Xu3btdMkll6h169Z7vdnx05/+VK+++qpOP/10nXnmmXruuec0cuRIdezYUZMmTdJll12mkpIStW7dWocddhhPpIpRQhTtwyRFAAAANHp09QIAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAICj8AAAAAkHhBwAAEIh9HuDMc+9wIGqIYyw516SBAwc64/86bPmHdj+f94dSUlLMnN3PAo1FeXm5M56bm2vmfPfdd874s88+G/P2GyvONWD/2Nu5xh0/AACAQFD4AQAABILCDwAAIBAUfgAAAIFIiPbxN275JVgciPiF84Zp+/btzvi3335r5jRv3twZ9/2MKysrnfHU1FQzx1pfaWmpmWM1fvgaQg40nGvA/kFzBwAAACRR+AEAAASDwg8AACAQFH4AAACBoPADAAAIBIUfAABAIPb5Wb0AUJsOO+wwc5n1fN3NmzebOdYIg6qqKjPHelbvli1bzBxrBExycrKZ06VLF2e8R48eZs6XX35pLgOAeHHHDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQVcvgHpx+OGHm8uKioqc8bKyMjMnMzPTGfc9sDw7O9sZ93X1lpeXO+O+juONGzc640cddZSZQ1cvgLrAHT8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAY5wKgXhQUFJjLtm3b5ow3aWL/W9Uas+JjjW3xjXNJSEhwxq19luxRL+3atbN3DgDqAHf8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQP7qr1+pwk/wPR491ffGsy6d58+bO+CWXXGLmpKWlOeN33XVXrexTXejQoYO5LC8vzxmfP39+Xe0OUK1t27bmsg0bNjjjGzduNHNyc3Od8aSkJDOnqqrKGU9OTjZzduzYEdO6JLvjuGPHjmYOANQF7vgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAo/AAAAALxo8e51DZrbIs1SkWSTj31VGe8W7duZk5GRoYzvnXrVjNnyJAhzvi4cePMHOvh7KtXrzZzSkpKnPGioiIzx3qovO/B8dZnev7555s58+bNc8anT59u5mzfvt1chnBlZmaay5o0cf+bNDEx0cyJ57vDGnf05ZdfmjnWubZz504zp6Kiwhn3fQYAUBe44wcAABAICj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgfjRXb1WJ128rC63q6++2swpLS2NKS5Jy5Ytc8Z37dpl5rz99tvOuK8z77DDDnPG8/PzzRzrIfBWh7Akbdq0yRkvKyszcyorK53xrKwsM+fwww93xk866SQz5/HHH3fG33vvPTPHkpCQYC6r7WMRdcvXodusmfurydc5ay2zOmol6bXXXnPGjzzySDNn/fr1zrjv+LPeD8csgP2NO34AAACBoPADAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgED86HEutW3gwIHOuO+h6dYYB9+4iLy8PGfcGqUi2SMefGNjvvjiC2f8s88+M3OsEQ++sTHWg+ibNm1q5ljLqqqqzJy5c+c6476xFMcdd5wz/u2335o5a9asiXk7aFw2bNhgLjv00EOdcWssiiSlpqY64ykpKWbOU0895YwPHTrUzLHW5xs1lJGR4Yx//fXXZg4QK98xaInnO7VJE/c9I9+1ozGq7fFh1vp8tYo1Yu7HfNbc8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQNRLV292dnbMy7777jszJz8/3xn3dehanTKVlZVmzqZNm5xxq8PJt2++bqF4unisnPLycjOnRYsWznhJSYmZY3Uwt2vXzsyxfnZHHHGEmWN19eLA8c0335jLfvaznznjvvMmKSnJGU9OTjZzli9f7oxbXfK+7Wzfvt3MsTqB6erF/uDrQLXOKV9OPB2lrVq1csZ79uxp5lx//fXO+Lhx48ycTz/91Bn3Xaet91Pbn5u1zHedrgvc8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABKJexrkcc8wx5rKioiJn/KCDDjJzVq9e7YxXVFSYOVb7tK9N3WoHb9q0qZkTT5u4tR1fjjWGxtfCbo25WL9+vZljvR9rxIVkf9a+z61ZM/ehaY2tQeOzaNEic1nz5s2d8XjGSPhGwFijH6yxRZI9Bsm3Hetc842pAmJlfd/7zhvfdcXSpUsXZ/yWW24xcy644AJn/PXXXzdzrHPtrrvuMnOGDRvmjMfz3eETzzgXy2OPPWYue/LJJ51x3+e2N9zxAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBA1EtXb9++fc1l8+bNc8Z79Ohh5lgdeB999JGZk5OT44z7Hs6+c+dOZ7ysrMzMiaeTyOrQjadr0OqOlOzPbcOGDWZObm6uM/7uu++aOVZH9rZt28ycTp06OeO+h9rXZpcV6t4HH3xgLrO6xLOzs82cxMREZ9x33li2b99uLrM6zjMyMswcq4N94cKFse0Y4GFdO3wOO+wwZ/zaa681c0444QRnvLCw0Mz5xz/+4Yxb1y7JnuJw8MEHmzlnnXWWM/7888+bOfGI59r+9NNPO+OHH364mZOVleWM09ULAACAvaLwAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBA7PM4l9oclWGN95CkL7/80hn3tYm3atXKGS8vLzdzNm3a5IxbYxcke5xLRUWFmWM9ZNoaVyHZ42GsB3BLdhu/b9SMNbbF2mdJWrZsmTM+aNAgMyclJcUZf/PNN82ceMa5MLalcfGNDbJ+lr7RD9Y5EM9x4TvXrDEO1pgXn82bN8ecg7oXzwigeL6f45GZmWkuO+ecc5zxkSNHmjkdOnRwxouKisycuXPnOuO+8WHWCLV4rp/WKDJJuv32253x4cOHmzl/+9vfnHHfSDirVrj77rvNnG7dujnj1nVVktq1a2cuixd3/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEPvchhZPZ5zVgZefn2/mWB0sq1evNnOsTqZp06aZOVbH75o1a8wcXyeuxdd9ZLE6o3ydYdbPx7fPVpdySUmJmWN1MvXu3dvMmTdvnjPu6+ay9jsvL8/MWbdunbkMjcvixYud8f3V1RtPV2dqaqq5zNeNjvpTm9Mq4uncHTJkiLns2muvdcbbtGlj5lgd5ytXrjRz3nvvPWfcN+HC6tCN57zxndM7duxwxrdv327mWFM+fF29Z599tjO+YsUKM8fq4vZdc63pJL7pG9b1uG3btmbO3nDHDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQiNifKv4DvgdTWy3fPr169XLGZ86caeakpaXFvB2rHdz3kGnrIey+HKtN2/dAd2vMihWXpPT0dGfcN07GWrZt2zYzx9pva2SLJHXu3NkZf//9980caywA41zC8O677zrjp5xyipljjQfyjUqw+EZzWMem73vAej+oX/GMbbGua9nZ2WbO6NGjnXHf8Wx9ny1ZssTMsY5N3/FsjT9JSUkxczZt2hRzjnXt8J2fVn3hu35aI9l814fMzExnvEWLFmaOdZ3cvHmzmbNr1y5n3Hedtt6rdc3fF9zxAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBA/Oiu3pYtW5rLrC4nX3fNMccc44xPnDjRzMnKynLGfZ0/VneN70HrVgeYr5uvvLzcXGaxOrN8+2Z1H/k6dK2uIOuh0JK0ceNGZ/y6664zcx5++GFn3NcRbn3WP6aTCY3H119/7YyfccYZZo7V1ZuYmBjz9n3HpvW94nvY/KpVq2LeB9Q9a4rE//t//8/Msb5TfcdZ06ZNnfHCwkIzx7qu+I4z63vT171s7bfvuta2bVtn3HfNtc6peCZp+LqUrffj6x62OnF9n1tSUpIzbl2/fct8x06PHj2c8aOPPtrM2Rvu+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAICj8AAAAAvGjx7l0797dXLZ9+/aY4pLUtWtXZ9z3AOyMjAxnvHPnzmaO1ZJvjYTw8bWwW8t827Ha231t4tbYGN84F2sMjvUAbkk64ogjnHHfCJjZs2c747m5uWaO9X58n3U87wcNk/Uget/P3xpzEc9IpdWrV5vL4hkftXLlypj3AXVv6dKlzviIESPMHOsaddRRR5k51viTbt26mTmZmZnOuG+ci3Xt8B2b1piw0tJSM8fah507d5o5vuuXxRq75huzYo388n0PWOtLSUkxc6xr69atW80ca6SMNSZNsr8Lrfi+4I4fAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAATiR3f1HnLIIeay999/3xn3dbAUFRU544MGDTJz7rzzzpi3Y3Xm+Tp/rE4mq/NI8ncfWawHevtUVVXFvP14Hl5v7ZvV7SvZn4/1kGvJ/qx93cNWt938+fPNHDRMBx98sDPuezi71YH3wgsvxLz9l19+2Vx28cUXO+O+fbM6NFG/WrVq5YwXFxebOXPnzo0pXtuaNLHv11hdvT7WtcO3Hes6aa0LtS+eLunduOMHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAjEPo9zadGihTN+0EEHmTmvvvqqM+57kHNGRoYzfvzxx5s548aNc8Y7duxo5mzevNkZ97WwW3wt7FbLtW+UijWCxTc2xuLLsR7CnZaWZubMmjXLGV+4cKGZ07p1a2fcN24nPz/fGV+xYoWZk52dbS5D43LSSSc5475zzXoQ/bJly2Levu/YtPjONd/YK9QfazxUXl6emWN9d/vG+VjHre9auH37dme8oqLCzLH2wTfWK57rjXVds+qEeP2YkSV1vX1rma+GsEaY+Ua4xTPebW+44wcAABAICj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgdjnrt6hQ4c649bDmiVpy5Ytzrivo9V6yPS2bdvMHKtrb8iQIWbOnDlznHFf95Ova8tidRrG0y3k62i09s3XYVRWVuaMZ2VlmTlLly51xr/77jszp0ePHs747NmzzRyrk8nXBWd1THXt2tXMQcNkdQdaneiS/fO3zkEfq6tcsrsqfd2W9d2dCLcdO3bEFPfxHWfWsek7Lqycli1bmjlWJ248kyfiUdvbqc19813bre34cqxrq2+freu077pmsa7f+4I7fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQOzznAOrdfmjjz4yc6wxHh06dDBzrIdmb9682cyxRsqsXr3azLHazuN5WLLvYdZW+7Y1tsa3b7Xdju5bFquVK1eay1JSUpxx32dgjfPIyMgwc6z3U5vvE/tHYWGhM96tW7eY1xXPzz8tLS3mHN84j88//zzm9aFx8V0HfMuA/Y07fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQiH3u6n3++edjXrn1oPPMzEwzx+oA/fTTT2Pevu/Bx9YDsH0dgImJic54PA+Bt7Yv2V29vs4w64HN8Tww2rdvlilTppjL7rnnHmfc90DvL774whlfsmSJmePr/Ebj8uGHHzrjp5xyipljHc/bt2+PefurVq0yl1nnlG8iwMKFC2PeBwCoC9zxAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEIvY5JDFYu3atM/6LX/zCzLn88sud8eeeey7m7e/YscNcZo16scaiSFKTJu46uUWLFmaONeKhpKQk5pzS0lIzp6KiwhkvLy83c6zRNVlZWWaO9RmsXr3azHn88ced8X/84x9mztatW81lFmvffGNj0DAtW7bMGfeNTLEUFhbGnLNmzRpzmXWc+UZB+c53ANifuOMHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGo067eeDz44IO1tq5Zs2bV2rrg5+ucfOSRR/bLPtC9e+CwjiffzzghISGmdfn4unATExNjzvnqq69i3gcAqAvc8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABKLBjXMBgEWLFjnjW7ZsMXOys7Od8Y0bN8a8/U2bNpnLrHEu27ZtM3OKi4tj3gcAqAvc8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQNDVC6DRKCoqMpfl5uY647t27Yp5O8uWLYs5x7dvANBQcMcPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAo/AAAAAJB4QcAABAIxrkAaDS2bNliLmvatGmtbcc3AiYhIcEZLy4urrXtA0Bd4Y4fAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCrl4AjYavc3bnzp3OuK9D17Jjx46Yl9HVC6Ax4I4fAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQCVEURfW9EwAAAKh73PEDAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQFH51ZOrUqUpISKjxJzc3V4MGDdLLL79c37sHNEg/PGesP++88465jjfeeENDhw5V27ZtlZSUpLZt22rQoEG6884799jWlVdeudd92n0uL1u2bJ/ew+TJkzV16tR9ei3QWCxYsEAXXnihOnfurOTkZKWlpalfv36aNGmSNm7cWCfbnD17tsaPH6/NmzfXyfpD1ay+d+BAN2XKFPXo0UNRFGnt2rW6//77dfrpp2v69Ok6/fTT63v3gAZlzpw5Nf5+2223aebMmZoxY0aNeK9evZz5Dz74oMaMGaNzzjlH999/v7Kzs7Vy5UrNnj1bzzzzjG688caY9+nUU0/VnDlz1KZNm316/eTJk5WTk6PRo0fHvC2gIXr44Yd1xRVXqHv37vrd736nXr16qaKiQvPnz9eDDz6oOXPm6Pnnn6/17c6ePVu33nqrRo8erczMzFpff6go/OpYnz59dOSRR1b//Sc/+YmysrL05JNPUvgBP3DsscfW+Htubq6aNGmyR9wyceJEnXjiiXrmmWdqxH/961+rqqoqrn3Kzc1Vbm7uXl+3Y8cOpaSkxLUNoKGaM2eOxowZoyFDhuiFF15QUlJS9bIhQ4bo2muv1euvv16Pe4hY8V+9+1lycrKaN2+uxMTE6titt96qY445RtnZ2WrZsqX69eunRx55RFEU1cjduXOnrr32WuXn5yslJUUnnniiPvroI3Xq1Im7C4Ck4uJi885ckybur7snnnhCPXv2VEpKig499NA9fhXD9V+9gwYNUp8+ffTuu+9qwIABSklJ0UUXXaROnTrpiy++0KxZs6r/W7pTp0619faA/e6OO+5QQkKC/vKXv9Qo+nZr3ry5zjjjDElSVVWVJk2apB49eigpKUmtW7fWBRdcoFWrVtXIefPNN3XmmWeqffv2Sk5O1kEHHaTLLrtMRUVF1a8ZP368fve730mSOnfuvE+/5oF9wx2/OlZZWaldu3YpiiKtW7dOd999t7Zv367zzz+/+jXLli3TZZddpo4dO0qSPvjgA1111VVavXq1xo0bV/26Cy+8UNOmTdP111+vk08+WYsWLdJZZ52lrVu37vf3BTRE/fv317PPPqvx48frrLPOUp8+fdS0aVPz9a+88ormzZunCRMmKC0tTZMmTdJZZ52lJUuWqEuXLt5tFRYWauTIkbr++ut1xx13qEmTJrrhhhs0YsQIZWRkaPLkyZLkvFgCjUFlZaVmzJihI444Qh06dNjr68eMGaO//OUvuvLKK3Xaaadp2bJluvnmm/XOO+/o448/Vk5OjiTpm2++Uf/+/fWb3/xGGRkZWrZsme69914df/zx+vzzz5WYmKjf/OY32rhxo+677z4999xz1f+gs37NAzGIUCemTJkSSdrjT1JSUjR58mQzr7KyMqqoqIgmTJgQtWrVKqqqqoqiKIq++OKLSFJ0ww031Hj9k08+GUmKRo0aVZdvB6gXo0aNilJTU/f59V9//XXUp0+f6vOtRYsW0eDBg6P7778/Ki8vr/FaSVFeXl60devW6tjatWujJk2aRBMnTqyO7T6Xv/vuu+rYwIEDI0nR22+/vcc+9O7dOxo4cOC+v0mggVq7dm0kKTrvvPP2+trFixdHkqIrrriiRvzDDz+MJEX/+Z//6cyrqqqKKioqouXLl0eSohdffLF62d13373HuYcfj//qrWOPP/645s2bp3nz5um1117TqFGj9G//9m+6//77q18zY8YMnXLKKcrIyFDTpk2VmJiocePGqbi4WOvXr5ckzZo1S5L085//vMb6R4wYoWbNuHGLcERRpF27dtX4s1vXrl312WefadasWbr11lt1yimnaN68ebryyivVv39/lZWV1VjXSSedpPT09Oq/5+XlqXXr1lq+fPle9yMrK0snn3xy7b0xoBGbOXOmJO3xa0dHH320evbsqbfffrs6tn79el1++eXq0KGDmjVrpsTERBUUFEiSFi9evN/2OVRUDHWsZ8+eezR3LF++XNdff71GjhyppUuXaujQoRo0aJAefvhhtW/fXs2bN9cLL7yg22+/XaWlpZK+/90l6fsL079q1qyZWrVqtf/eEFDPHnvsMV144YU1YtG//D5skyZNdOKJJ+rEE0+UJG3fvl0XX3yxpk2bpkcffVRXXHFF9Wtd505SUlL1eeezr12+QGOVk5OjlJQUfffdd3t97e5rlOu8aNu2bfU/pqqqqjR06FCtWbNGN998s/r27avU1FRVVVXp2GOP3adzDz8OhV89OOSQQ/TGG29o6dKleuqpp5SYmKiXX35ZycnJ1a954YUXauTsvkCtW7dO7dq1q47v2rWr+oQDQnD66adr3rx5+/z61NRUjR07VtOmTdPChQtrbT8SEhJqbV1AQ9S0aVMNHjxYr732mlatWqX27dubr919jSosLNzjdWvWrKn+/b6FCxfqs88+09SpUzVq1Kjq13z99dd18A7gwn/11oNPP/1U0vdjIhISEtSsWbMav4BeWlqqJ554okbO7rsX06ZNqxF/5plnavxXF3Cga9WqlY488sgaf3YrLCx05uz+76O2bdvW+f7t6x1DoDEYO3asoijSJZdcovLy8j2WV1RU6KWXXqr+tYe//e1vNZbPmzdPixcv1uDBgyX93z+Yftj09NBDD+2x7t2v4XyqXdzxq2MLFy6sLsyKi4v13HPP6c0339RZZ52lzp0769RTT9W9996r888/X5deeqmKi4v1hz/8YY+Tonfv3vrlL3+pe+65R02bNtXJJ5+sL774Qvfcc48yMjLMURVASHr37q3Bgwdr+PDh6tq1q8rKyvThhx/qnnvuUV5eni6++OI634e+ffvqqaee0rRp09SlSxclJyerb9++db5doC70799fDzzwgK644godccQRGjNmjHr37q2Kigp98skn+stf/qI+ffro+eef16WXXqr77rtPTZo00fDhw6u7ejt06KBrrrlGktSjRw917dpVN954o6IoUnZ2tl566SW9+eabe2x793nz5z//WaNGjVJiYqK6d+9e4/dyEYf67S05cLm6ejMyMqLDDjssuvfee6OysrLq1z766KNR9+7do6SkpKhLly7RxIkTo0ceeWSPbqaysrLot7/9bdS6desoOTk5OvbYY6M5c+ZEGRkZ0TXXXFMP7xKoW7F29T700EPR2WefHXXp0iVKSUmJmjdvHnXt2jW6/PLLo5UrV9Z4raTo3/7t3/ZYR0FBQY0ueaurt3fv3s59WLZsWTR06NAoPT09khQVFBTs8/4DDdWnn34ajRo1KurYsWPUvHnzKDU1NTr88MOjcePGRevXr4+i6PupFHfddVfUrVu3KDExMcrJyYlGjhy5x7m3aNGiaMiQIVF6enqUlZUVnXvuudGKFSsiSdEtt9xS47Vjx46N2rZtGzVp0iSSFM2cOXM/veMDV0IU/WBKMBqV2bNn67jjjtPf//73GrMBAQAAfojCrxF58803NWfOHB1xxBFq0aKFPvvsM915553KyMjQggULajSHAAAA/BC/49eItGzZUv/85z/1pz/9SSUlJcrJydHw4cM1ceJEij4AALBX3PEDAAAIBK2gAAAAgaDwAwAACASFHwAAQCAo/AAAAAKxz129PJdS5tMxqqqqYl7XpZdeai7r2rWrM/7888+bOR988IEz/q+PgvuhyspKc1koGmJvE+eazfUA+N2sx7XtL7169TKXLVq0aD/uScPEuVY74tnn2v7sDznkkJhzVq9e7Yz7rp8/fILVbomJiWbOypUrY9uxA9Deft7c8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQPCs3hjE0707YcIEZ/yjjz4ycx5//HFn3NfVO2nSJGd85syZZk6zZu4f/65du8wcIFYnnXSSM37RRReZOaeffroz7utE37p1qzNuHeeSVFJS4oz7uod37NgR83asc+qRRx4xc+677z5nfMOGDWYOGhdfh248nbi12b3r65wdPXq0M+67dmzbts0ZLy4uNnPOOOMMZ/yZZ54xcx5++GFzGb7HHT8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCASon3s/26MD7OORzwjGY477jgzp2PHjs74k08+GduO7cX06dOdcasd3qdJE/vfA/GMtGnIeHB8bK6//npn/JxzzjFzWrduHfN2rNEPPunp6c5406ZNzRzrePYdF6Wlpc54WVmZmWM9bD4tLS3m7WzevNnM+clPfuKMb9q0yczZXzjXGibrGCwoKDBzrOvaZZddZubk5uY64927dzdzli9f7owPGTLEzOnUqZMz/vHHH5s5B5q9nWvc8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQNDV+wPxPDR7/PjxZs7tt9/ujFdUVJg5Vmex7wHYZ555pjOekpJi5lidxc2bNzdzysvLzWWNEZ2GezrppJPMZX//+9+d8fXr15s5++uYiafjPJ7P2jo/4+mG953T1vry8/PNnKKiImf88MMPN3P2F8612mF1r0tShw4dnPHExEQz56CDDnLGN27caOa8//77zviCBQvMnKysLGfcd1xYUylWrVpl5lx88cXO+OzZs82cRYsWOeOFhYVmTkNGVy8AAAAkUfgBAAAEg8IPAAAgEBR+AAAAgaDwAwAACASFHwAAQCDccwkCYI1K8I2EsB60vnPnTjPHGtviG/1gPVTeN/rh9ddfd8ZHjx5t5lh828GBb9y4ceayTZs2OeO+c8AaJeEbORDP6A/fORWreMZ8+L47rPdjjYaR7O8B34gJa2TG8ccfb+a899575jLUn+TkZGfc97O0zk/fuKXVq1fHtH3JHtH0xz/+0cx58MEHnfGHHnrIzJk7d64zfvrpp8ec4xsB06NHD2f8hBNOMHOmT5/ujJeVlZk5DQV3/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEMF29Vodc77OvKOOOsoZX7ZsWczb9z0029chGWtO8+bNY16X7zOwuh0b4gPYEZ8+ffqYy6yOUl93qnU8+Y6zeMSzPmu/fcez1T3sy7HOG1/3sLUsnk7gc845x8yhq7dhOvroo53x0tJSM6eoqMgZ910H1qxZ44y3bt3azPn3f/93Z7xLly5mjtUF69u3M844wxkvKSkxc9atW+eM+84163vNmuQhSf369XPGZ8+ebeY0FNzxAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgnEuP1BRUWHmdO3a1Rm3Hlhd23xt79Y++EY/FBQUOOPLly83c6zPbdeuXWYOGqasrCxn3DdqKJ6xPdb4k8rKypjXFc/2fWMc4llfPGNjrH3wrcvaN985bTn++ONjzkHd69Chg7ksNzfXGbdGtkhSTk6OM26NbJGkjRs3mssszz33nDO+detWM6dz584xxSX7u2Px4sVmzvbt251x6/OUpJSUFGfcVw9kZGTEFJekLVu2mMv2J+74AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAggu3qjaebr1u3bs74/nrIua870eLr0GzTpo0z7uvqtbqs0Pi0a9fOGfcdZ9ZDy33dqTt37nTGfd2p1vriOQd8ObV5PPs+A+u9+roGre8o3z5b5/tXX31l5iA21vEUzzWlT58+5rLVq1fHvJ3MzExnvHXr1mZO//79nfEVK1aYOampqc74jBkzzJy8vDxnvLS01MxZtGiRM96lSxcz56CDDnLGN2zYYOZY59TmzZvNHGsqQt++fc2c/VUr7A1XcQAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAIIId5xLPg9atdvSysrKY11XbD5u3NG3a1FzWsWNHZ/yDDz4wc+LZbzRMCxcudMatEQqSlJ+f74z7RoxYIyZ84xWs8Se1ffzV5ngY37lmfT6+95OdnR3Tunz7UFhYaOYgNrV5DFpjUSRpwIABzvg333xj5nz77bfOuDVSSZK+/vprZ/yLL74wc6xrh++6ao0JS0tLM3MGDRrkjPvONWs8zbPPPmvmWD8H3/tJT093xq06QWKcCwAAAPYzCj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgQi2q9f3cHSL1X1UXl4e87p27doVc04822nRooW5rFWrVjGvL579RsN01113OeOdOnUyc5o3b+6M+zodrQ5dX1ev1bXnO/7i6Zy1xNPtm5SUZC6zPoN41ufbjvUZDB061Mz52c9+5oy/8MILZg5iY3V6+rptrW74Y4891sx58cUXnXHfd73V3Z+RkWHmbN682Rn3HZsjR450xh977DEzZ9asWc54ZWWlmTN9+nRnvF27dmbOli1bnPETTzzRzJk7d64zfswxx5g5WVlZzvimTZvMHOu76Md0l3PHDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQiHoZ5+J7yHg8YxSstmbfunzt4JaUlBRnfMWKFTGvK57PwPd+rIdJJyYmmjnWuACfePbN4mtHr80HocOtuLjYGbfGFEjS8OHDnfGNGzeaOU8++WRM65Ls8Qa+h7Nb4jmWfOenxTciyhrbkZycbOa88847zrg1FkOyf3Zff/21mfPxxx+by1A7unfv7oz7xhOtWrXKGf+P//gPM+e2225zxn3jXHr06OGM+0bNWONPfL766itn3Bp1I8V3nS4pKXHGv/32WzPnqquucsaHDRtm5jz99NPO+FFHHWXmWCNlfONc6uJayB0/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAhEvXT1Wh2oDd1Pf/pTZ3zlypUxr8vXzVWbcnJyzGVWp5lPY/3ZYU+TJk3aL9u54YYbnPH09HQzZ926dc641Vkv1W73m29dVsevr7Pd6pBs3769mfPqq68647/+9a/NHDRMBQUFzviGDRvMHOuY8XXo9urVyxkvLS01c6zjOSMjw8yxutHLysrMnNWrVzvj2dnZZs6XX37pjFufpyR16dLFGfd16lvfK74pAtb6ysvLzRxrvxcuXGjm1AXu+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAICj8AAAAAlEv41wasszMTHOZNWKiIfM9/Nl6YDTCYI0fqe2Hgufn5zvjvtFA8eybb5yKpS4egO7SrJn7q9b3Gfi+i9DwpKWlxbysqKjIzNmyZYsz7rsOWaNefONCrLEklZWVZo51bFpjXiSpRYsWzrjvHLDW5xvNsmrVKmfcOgclacWKFc54cXGxmdOyZUtn3DfSpnnz5uay/Yk7fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQiHrp6n333XfNZdbDpK0OGknatWuXM96zZ08zJykpyRlfsmSJmWM9UPvzzz83c6yOJV8HotVNddhhh5k51j4kJiaaOVZH2YIFC8ycHj16OOMff/yxmWM9tNr3AOzbbrvNGX/99dfNHMRmf3W0VlRUOOO+jkarA8+3z9bx5OtOtM5D3/lp7Zv1PeRbn69DMz093VyGhsfX1Wt9D/u6U63O2fbt25s51vHku35ax9nOnTvNHKvbNSMjw8wpLCx0xq2uf0lKTU11xn3dsdb5uXTpUjPn1FNPdcZ916jWrVs7477vgZSUFGfc6niW7Jrox+COHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEPUyzsV6iLIktW3b1hn3Pch527Ztzvjs2bPNHGu8gq8l32q5tsZVSPZ+WyNOfPv20ksvmTnWmAtfG7/VKr9p0yYzxxrb4htLYG2npKTEzPEtQ+NijbLwjUqwjiffOJfaHE/j+76xxsP4RidZOb7zxjcaAw1Pu3btzGXW+BHfdaBz587O+Ny5c82clStXOuMdOnQwc3bs2OGMW9/bkn2NOvLII82c3NxcZ/z99983c6xRJr5RUNZ++8bgbNy40Rlfs2aNmWN9f/muXVaO9dlI0ooVK8xl8eKOHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEol66en0PMc7KynLGy8rKzByrmy6eh2ZbD1GWpLVr1zrjvgcsW9vxdQ0uWrTIGe/Vq5eZY3UW+x70bj2Ee+vWrWaO1WmYlJRk5lhdY77PrU2bNuYyNC5WJ5uvo7W++TqErQ5dq9NRsj8DX2ezryMfDY/vu7a4uNgZ9x1nVpfwnXfeaeZkZmaayyzW8ezbN+v65ZvYYV0HfOeAdY3yddBbHb8FBQVmztdff+2M/+xnPzNzrAkkVp0gSdnZ2TGtS6KrFwAAAD8ChR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABKJexrnMmzfPXDZ8+HBn/JtvvjFzrAc5+0azNGvmfuvJyclmjtVeb41S8S2z2vslu337+OOPN3OsETm+VnmrJd5q75fskRW+cS7Wg8gPOuggM+err74yl6FxscYq+UYaWceZb2SKNX7CNzbGyvFtx+L7HrDW5xuZ4fsuQsNjjSKT7O/A/Px8M2fZsmXO+PTp082cs88+2xlfuXKlmWPxHc/WOW1diyWpsLDQGbeuq5K0YcMGZ9x3jbKWWeNkJPv6+de//tXMGTx4sDP+5JNPmjnWqLTWrVubOXWBO34AAACBoPADAAAIBIUfAABAICj8AAAAAkHhBwAAEIh66epdvHixuczqNN20aVPMOVbnrmQ//NnX+WMpKyszl1mdUR06dDBzfvWrXznjvgdTW52Lvq5eq7PY6tjy2b59u7nM1+llWbduXcw5aJh8x60lnq7a+lbb+5yamlqr60Pd8nVhW9ciX1fv//7v/zrj8VwH0tPTzRxfd73F6k7t0aOHmWNNfti2bZuZ06JFC2fcd65ZHdS+a5R1zfN16I4ePdoZ79atm5nz+eefO+Ndu3Y1c+oCd/wAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGol3EuvgdGW63Y8YxKsNrHfaw2dSm+h6anpKTEnGONgPF9BlYbv+8B9ZmZmTHtl8+WLVvMZdboHN8D6nHgsEYy+EYNWQ9N941oqm++8zOeY903ggP1J57xRNZ1zTc6q6ioyBm3zifJHi3mO9es600844SsEWGS/bmVlJSYOdZ54/sZWN8RvmthPOea9V5zc3PNHOtnF88x9WNwxw8AACAQFH4AAACBoPADAAAIBIUfAABAICj8AAAAAlEvLXJLly41l1kPbLa6onx8HYCVlZXOuK8L1+raW7FihZmTl5fnjOfk5Jg51mfg60qyllnvU7K7nq0uL98y38/HyrE6NyV/dxgaF6uTrba7uhtyl7i1b1VVVWZOPFMEUPd8HbIWq2vUt6527do542vXrjVzduzY4Yz7OoFLS0udcV9Xr7UdX3fswQcf7IwvWbLEzLHeq68TuHnz5s54PFMEfBYsWOCM+76HrPPd6saW7JrE+hnsC+74AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACUS/jXHztzqtXr3bGN23aZOb4xpxYrLZqX1u31fbuaxO3tlOb+yzZo2Z8OTt37ox5H6z99o1zsbbj275vpAwaF2ssiW/UkHU8+1jfK/Gsa3/ZX+NcfJ9BQx6D0xBZ40K2b99u5vTs2dMZ941MsbbjG+dijQ/zXdesZb73k5iY6Ixv3brVzPnwww+d8V69epk51iiTdevWmTnW8ez7volnXJz1+XTr1s3MWbhwoTOekZFh5vjG0sWLO34AAACBoPADAAAIBIUfAABAICj8AAAAAkHhBwAAEIh66er1sbpefA+Ztjp+fZ06VvfTypUrY95Ohw4dYt7Ohg0bzJx4unisrj1fx7HVoevrfrK6EH2dgdYDqH2d2jhwpKWlOeO+jtYDTTydxdbnhvrVsmVLZ9zqdJXsLtQTTzzRzLG6dw877DAzp127ds647/pZUlLijFtdxZK0efNmZ9zXnVpcXOyMr1q1ysyxruG5ublmzo4dO5xxa58lqVWrVs54PJ+bbyJF27ZtnXHfZ10XuOMHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAhEgxvnsmzZMmd86NChZs5nn33mjPvaqq2RJV26dDFzOnXq5Iz7RqYUFRU5476HZlst5PGMZvGNWanNETC+HOuz/uabb8wc1B/f6BHf8WSJZyxJPONP4smJR21uxzfSJikpqda2g9pjjdvyjeT49ttvnXHfiJHOnTs74/PmzTNzrJEyWVlZZo41/sQ3msUaNeMbBZaSkuKMJycnmznWddJ33ljXz8zMTDPH4qsHdu7c6Yz7xrFZ+/DVV1+ZOR07dnTGrdF3+4I7fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQiAbX1bt+/Xpn3Nf5Y3Ua+jpnt2zZ4oxv2rTJs3duvg4jq8spnocy+zoq4+m2tHJ8XYvWMt/Px+oEtrqicGCxutx856fFOpak2j2e4+mG97G+B3zvpzYf3F7bndohszozt2/fbubk5eU5476f8YsvvuiMW5MiJCknJ8cZLy4uNnOsY2PdunVmjnXMWB3CknT88cc74x9//LGZY9m4caO5zPpMS0tLzRyru3rNmjVmjnX9uvTSS82cbdu2OeNlZWVmTjzdyHvDHT8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAa3DgX6+HPLVu2NHNatWrljPtGGMQzYsR6ALavJb9p06YxxaX997D5eFj75vsMrBxf6z/qT22P/li1apUz3rp1azPHN+YkVrU9riSe9cXzfgoLC2POsTTk75TGxvqus0aESVKbNm2c8fnz55s5r7zySmw7JunTTz91xhvCOJ9p06btl+3sL9bot+XLl5s51tgYa9yTFN/Yq73hjh8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABKLBdfVaD7pOTU01c6yOGN+Dj6uqqpxxX3eq9bD5eB4cH0+njrV9H1+3kK+z2GLtt+9zs/bBemA1DixWN7x1Dkr2eeM7nq3z0Hd+Wt2Ovk5Ha1k8Ob7PwDrffeet9fnQ1Vt72rZt64z7rlE9e/Z0xn0doLVpf3XuQlq/fr25bMOGDc54PBNIfgzu+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAICj8AAAAAtHgxrl8/vnnzrhvNEu7du2c8ZycHDOnW7duzriv7d0aoxBPu3U8ox/qoq07VmvWrHHG09PTzRzrYdZvvfVWrewTGrajjz7aGV+7dq2ZY405qaioMHOsUUO+ETC+cSqxSkpKMpdZ4xp8YxxatGjhjPvONesh8Kg98+fPd8b79Olj5lg/51WrVsW8/XjGh4Wkvj+D5ORkc1mvXr1iXl9dnNP1X0kAAABgv6DwAwAACASFHwAAQCAo/AAAAAJB4QcAABCIhGgfW2Dq+yHfBQUF5rKzzz7bGS8tLTVz8vPznfFt27aZOVYHVvPmzc2ceDqMrM86nq5eXxektaxjx45mjrUPO3fuNHPKy8ud8QceeMDM2V/quwPMpb7PNd/24/m8LrjgAme8S5cuZk737t2d8YyMDDPHWmZ1x0p2V28879N3rm3fvt0ZX716tZmzcOFCZ/wPf/hDbDum2v+ZxoNzDSE4/vjjzWWZmZnOuHWuS3ZXr6/bd2/nGnf8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQFH4AAACB2OdxLgAAAGjcuOMHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPzqwIIFC3ThhReqc+fOSk5OVlpamvr166dJkyZp48aNdbLN2bNna/z48dq8eXOdrB+oC1OnTlVCQkL1n2bNmql9+/a68MILtXr16pjXl5CQoPHjx1f//Z133lFCQoLeeeed2ttpoJH613PN94fz5cDWrL534EDz8MMP64orrlD37t31u9/9Tr169VJFRYXmz5+vBx98UHPmzNHzzz9f69udPXu2br31Vo0ePVqZmZm1vn6gLk2ZMkU9evRQaWmp3n33XU2cOFGzZs3S559/rtTU1PrePeCAMGfOnBp/v+222zRz5kzNmDGjRrxXr177c7ewn1H41aI5c+ZozJgxGjJkiF544QUlJSVVLxsyZIiuvfZavf766/W4h0DD1KdPHx155JGSpJNOOkmVlZW67bbb9MILL+hXv/pVPe9d3SktLVVycrISEhLqe1cQgGOPPbbG33Nzc9WkSZM94j+0Y8cOpaSk1OWu1YnGut91jf/qrUV33HGHEhIS9Je//KVG0bdb8+bNdcYZZ0iSqqqqNGnSJPXo0UNJSUlq3bq1LrjgAq1atapGzptvvqkzzzxT7du3V3Jysg466CBddtllKioqqn7N+PHj9bvf/U6S1LlzZ27Xo9HbfSFavny5Bg0apEGDBu3xmtGjR6tTp05xrX/69Onq37+/UlJSlJ6eriFDhtS4G/LCCy8oISFBb7/99h65DzzwgBISErRgwYLq2Pz583XGGWcoOztbycnJOvzww/X000/XyNv939r//Oc/ddFFFyk3N1cpKSnauXNnXO8BqAuDBg1Snz599O6772rAgAFKSUnRRRddJElasWKFRo4cqdatWyspKUk9e/bUPffco6qqqup869crli1bpoSEBE2dOrU69u233+q8885T27ZtlZSUpLy8PA0ePFiffvppjdxp06apf//+Sk1NVVpamoYNG6ZPPvmkxmtGjx6ttLQ0ff755xo6dKjS09M1ePDgWv1sDhQUfrWksrJSM2bM0BFHHKEOHTrs9fVjxozRDTfcoCFDhmj69Om67bbb9Prrr2vAgAE1irpvvvlG/fv31wMPPKB//vOfGjdunD788EMdf/zxqqiokCT95je/0VVXXSVJeu655zRnzhzNmTNH/fr1q5s3C9Sxr7/+WtL3dyRq2//8z//ozDPPVMuWLfXkk0/qkUce0aZNmzRo0CC99957kqTTTjtNrVu31pQpU/bInzp1qvr166dDDjlEkjRz5kwdd9xx2rx5sx588EG9+OKLOuyww/SLX/yixkVut4suukiJiYl64okn9MwzzygxMbHW3yPwYxQWFmrkyJE6//zz9eqrr+qKK67Qhg0bNGDAAP3zn//UbbfdpunTp+uUU07RddddpyuvvDKu7fz0pz/VRx99pEmTJunNN9/UAw88oMMPP7zG76rfcccd+uUvf6levXrp6aef1hNPPKGSkhKdcMIJWrRoUY31lZeX64wzztDJJ5+sF198UbfeeuuP+RgOXBFqxdq1ayNJ0XnnnbfX1y5evDiSFF1xxRU14h9++GEkKfrP//xPZ15VVVVUUVERLV++PJIUvfjii9XL7r777khS9N133/2o9wHsT1OmTIkkRR988EFUUVERlZSURC+//HKUm5sbpaenR2vXro0GDhwYDRw4cI/cUaNGRQUFBTVikqJbbrml+u8zZ86MJEUzZ86MoiiKKisro7Zt20Z9+/aNKisrq19XUlIStW7dOhowYEB17Le//W3UokWLaPPmzdWxRYsWRZKi++67rzrWo0eP6PDDD48qKipq7Mtpp50WtWnTpno7u9/rBRdcEOvHBNSJUaNGRampqTViAwcOjCRFb7/9do34jTfeGEmKPvzwwxrxMWPGRAkJCdGSJUuiKNrznNvtu+++iyRFU6ZMiaIoioqKiiJJ0Z/+9Cdz/1asWBE1a9Ysuuqqq2rES0pKovz8/OjnP/95jfciKXr00Uf36b2HjDt+9WDmzJmSvr81/a+OPvpo9ezZs8Z/L61fv16XX365OnTooGbNmikxMVEFBQWSpMWLF++3fQbq0rHHHqvExESlp6frtNNOU35+vl577TXl5eXV6naWLFmiNWvW6Ne//rWaNPm/r7+0tDSdc845+uCDD7Rjxw5J39+ZKy0t1bRp06pfN2XKFCUlJen888+X9P2dyS+//LL69xB37dpV/eenP/2pCgsLtWTJkhr7cM4559TqewJqW1ZWlk4++eQasRkzZqhXr146+uija8RHjx6tKIr2aBDZm+zsbHXt2lV333237r33Xn3yySc1/stYkt544w3t2rVLF1xwQY1zKzk5WQMHDnT+OhPn197R3FFLcnJylJKSou+++26vry0uLpYktWnTZo9lbdu21fLlyyV9/3uAQ4cO1Zo1a3TzzTerb9++Sk1NVVVVlY499liVlpbW7psA6snjjz+unj17qlmzZsrLy3OeG7Vhb+deVVWVNm3apJSUFPXu3VtHHXWUpkyZoksvvVSVlZX629/+pjPPPFPZ2dmSpHXr1kmSrrvuOl133XXObf7rr25Y2wYaEtcxWlxc7Pyd2rZt21Yvj8Xu36GdMGGCJk2apGuvvVbZ2dn61a9+pdtvv13p6enV59dRRx3lXMe//uNNklJSUtSyZcuY9iNEFH61pGnTpho8eLBee+01rVq1Su3btzdf26pVK0nf/x7FD1+3Zs0a5eTkSJIWLlyozz77TFOnTtWoUaOqX7P795+AA0XPnj2ru3p/KDk5WVu2bNkj/sOCal/867n3Q2vWrFGTJk2UlZVVHbvwwgt1xRVXaPHixfr2229VWFioCy+8sHr57nN17NixOvvss53b7N69e42/08GLhs51jLZq1co8b6T/OxeSk5MlaY+mJdf5WlBQoEceeUSStHTpUj399NMaP368ysvL9eCDD1av85lnnqn+n65Y9xt74r96a9HYsWMVRZEuueQSlZeX77G8oqJCL730UvUt9L/97W81ls+bN0+LFy+u7kTafRD/sEP4oYce2mPdu1/DXUAcaDp16qSlS5fWuJAUFxdr9uzZMa+re/fuateunf7nf/5HURRVx7dv365nn322utN3t1/+8pdKTk7W1KlTNXXqVLVr105Dhw6tsb6DDz5Yn332mY488kjnn/T09DjfOdBwDB48WIsWLdLHH39cI/74448rISFBJ510kiRV3xX816536ftOep9u3brppptuUt++fau3MWzYMDVr1kzffPONeX4hdtzxq0W7u2+vuOIKHXHEERozZox69+6tiooKffLJJ/rLX/6iPn366Pnnn9ell16q++67T02aNNHw4cO1bNky3XzzzerQoYOuueYaSVKPHj3UtWtX3XjjjYqiSNnZ2XrppZf05ptv7rHtvn37SpL+/Oc/a9SoUUpMTFT37t256KDR+/Wvf62HHnpII0eO1CWXXKLi4mJNmjQprv/SadKkiSZNmqRf/epXOu2003TZZZdp586duvvuu7V582bdeeedNV6fmZmps846S1OnTtXmzZt13XXX7fHfSw899JCGDx+uYcOGafTo0WrXrp02btyoxYsX6+OPP9Y//vGPH/X+gYbgmmuu0eOPP65TTz1VEyZMUEFBgV555RVNnjxZY8aMUbdu3SRJ+fn5OuWUUzRx4kRlZWWpoKBAb7/9tp577rka61uwYIGuvPJKnXvuuTr44IPVvHlzzZgxQwsWLNCNN94o6fsicsKECfr973+vb7/9Vj/5yU+UlZWldevWae7cuUpNTaVzNx7121tyYPr000+jUaNGRR07doyaN28epaamRocffng0bty4aP369VEUfd9deNddd0XdunWLEhMTo5ycnGjkyJHRypUra6xr0aJF0ZAhQ6L09PQoKysrOvfcc6MVK1bs0b0YRVE0duzYqG3btlGTJk2cXVVAQ7O703XevHne1z322GNRz549o+Tk5KhXr17RtGnT4urq3e2FF16IjjnmmCg5OTlKTU2NBg8eHL3//vvObf/zn/+MJEWSoqVLlzpf89lnn0U///nPo9atW0eJiYlRfn5+dPLJJ0cPPvhgzO8V2F+srt7evXs7X798+fLo/PPPj1q1ahUlJiZG3bt3j+6+++4aHfJRFEWFhYXRiBEjouzs7CgjIyMaOXJkNH/+/BpdvevWrYtGjx4d9ejRI0pNTY3S0tKiQw45JPrjH/8Y7dq1q8b6Xnjhheikk06KWrZsGSUlJUUFBQXRiBEjorfeesv7XuCWEEX/8v8dAAAAOGDxO34AAACBoPADAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAARin5/ccaA9A69p06bOeGVlZczr2v2QapczzzzTGZ85c6aZs2vXLmc8Pz/fzElNTXXG33jjDTPH8sMnE+wL3zjIhjwqsiHu24F2rtUm63ySpCuuuMIZX7x4sZnjerSi5D8HDjvsMGd8/vz5Zs6f/vQnZ3zt2rVmTm3yHVP76xzgXKt7EyZMcMat650krVq1yhmvqqoyc7p06eKMX3/99WaO9YjF//7v/zZzdj+rNxbWE33mzZtn5rz11lsxb6ch29u5xh0/AACAQFD4AQAABILCDwAAIBAUfgAAAIFIiPbxN24PtF+CjcdZZ53ljD/33HNmjvXx7ty508xJTk52xsvKymLO2bZtm5mTnp5uLgsFv3BeO1q3bm0uO+SQQ5zx7t27mzk9evRwxouKisycgQMHOuO9evUyc/Ly8pzxkpISM+frr792xn2/PF5cXOyMp6WlxZwzd+5cM+e1114zl1ms4622zw3Otbr397//3Rn/5ptvzJyUlBRn3NfoaDUgnnjiiWbO9OnTnXGrMVGSEhMTnXHfdS03N9cZ37Bhg5kzceJEc1ljRHMHAAAAJFH4AQAABIPCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAg9vlZvfWttp81abWwP/TQQ2bOz3/+c2e8tLTUzKmoqHDGfe/HapWPp73eNy7CWt/tt99u5jz11FPO+KJFi8wci+95qL7nRKL+HH300c740KFDzRzr/PD9jN9//31n3BrvINljTnzPuLbGIPmebWqNxrCe+yvZo5OysrJi3rf+/fubOZ06dXLGH3jgATOnIY5Zgc33/FrrO9V3PGdmZjrjX3zxhZmTkZHhjL/77rtmjm8EiyUpKckZ930PxLOd0HDHDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAAC0Wi6euPpPJswYYK57Oqrr3bGre47SdqyZYsz7uvQbdGihTPu6wC0OrDi6dC19lmyO8BuuukmM+fmm292xn/729+aOX/84x/NZWh4rG5SSRo0aJAzvmnTJjPH6rLbvHmzmdOhQwdn3HfenHvuuc74q6++auaUlJQ4474uSGvf2rVrZ+ZY56714HpJ6tixozNeWFgY87716dPHzFm4cKG5DA1P165dzWUbN250xtetW2fmfPXVV864NZFCiu96bHX3+6Y7bNiwwRm3rquSfa41a9Zoyp06xx0/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgGlx/szUaJZ728TFjxpjLrPUVFxebOdaYC9/D5q0Wdl87urVvvvEXVqu6b9SMtd9WC70kpaSkOOPnn3++mWONc/F9blaLvy8HtePoo482l1nH044dO8wca8TEeeedZ+bk5uY642vWrDFzUlNTnfHt27ebOQcddJAzvnXrVjPHGvlkbV+yx+D4vtesB95nZmaaObt27XLG+/bta+YwzqVxSUpKMpdZo5OssUW+9cXzXes7Nq3z0Deiydo332dgfRf5RjSFdr3hjh8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABKLBdfXG46677nLGc3JyzJz169c741bXqmQ/tNr3kOnKykpnfPXq1WZORkaGM+7r6m3VqpUz7nswtdVRaHUvS9LOnTud8SOPPNLMueSSS5zxhx9+2MxB/WnZsqW5zDpmfN181vHUsWNHM2fmzJnO+BlnnGHmTJkyxRk/+OCDzZxNmzY549aD3iX7HPjTn/5k5hxzzDHOePfu3c2cdevWOeNFRUVmjvX95fuZonHxXW98nasWq0PWmkgh+TvlLda1yLedxMREZ9w6ByW7E9d3bbdqBatOaOy44wcAABAICj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACESDG+fie2i55fzzz3fGrfErktS8eXNn3PfAaOsB6FbLuWS3sFvbl+yxLb6RDNZ2rHEyvmW+HOu9+n5uF1xwgTPuG+dyoD4cuzHwPQA9ISHBGW/Tpo2Zs3jxYmfcNwJm9uzZzvjFF19s5ljnrm9kSmFhoTNu7bMk3XPPPc54586dzZyNGzc6474RE1lZWc74qlWrzBzrO8L6uaHxadGihbnMuub5xnpZOb7vgXiuhVu2bIk5x3qvvhEw1r5Zccn/Xg9E3PEDAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEA0uK7eeFgdoL4HOVsPs/Z1v1mdq76HZlv75nsIvNXl5OsEtjqWfPtmLfN1WVl8n3VeXl7M60P9sbpJJfvh7B07djRzunTp4ox/9NFHZs6ECROc8Y8//tjMsaSnp8ecY3X7StKf//xnZ/yiiy4yc9auXeuM+7oJN2zY4IwPHz485u1wDh44fNMdtm7d6oz7rh07duyIOSfW7Uv29dO3Hd+EiVj5Jk/4uusPRNzxAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEotGMc8nNzTWXZWdnO+O+hzJbD632tXxb40+sh1z7cnz7ZrW3WyNoJLvt3Ron4+PbjrU+3/tp06aNM96qVSszp7i42FyGupWTk2Mu27x5szPetWtXM2fo0KHO+KWXXmrmjBgxwhkfMGCAmdOzZ09n/L333jNzCgoKnPG+ffuaORkZGc64b0TTrFmznPGlS5eaOd26dXPGrfNJkhYtWuSMW/uMxqdFixbmMmvckm9kinVOW9dIyb5O+vbNGplSVlZm5ljXFd/1Jp4RMKmpqTHnNGbc8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQDSarl5fV5rVLbRly5aYt5OQkGAus7p3fV2w1jJfx5TVleTrVrK6bX05VqeXrxPYej87d+40czIzM51xqwtT8ndiom4lJiaay6yfv68r7v7773fGjz/+eDPnuOOOc8bXrl1r5rRv394Z9313WMf6woULzZxhw4bFvG+nnnqqM37MMceYOTfddJMzPmTIEDPH+i4sLy83c3DgsL7vfZMnrOvAjh07Ys5ZvXq1mWNN3/Adm1b3sO/9WJ3NSUlJZo41feNAFda7BQAACBiFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEotGMc7FawSX/CBaL1b5ttY9L9giWjRs3mjnWg9utsQs+vveZnJzsjPvGrFhjO+J5yLVvBIzlhBNOMJcxzqX+tGzZ0lxmHU+lpaVmzosvvuiMjxo1ysyxxsbk5eXFnOMb57JixQpnvF+/fmbOunXrnHHf+fnFF18449boCZ+tW7eay6yRFYxzCYM1ZqW4uNjMsUYx+Y4Za0TXzJkzzZycnBxn3HcdWL9+vbnMYp2Hu3btMnPiuX41ZtzxAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBANJqu3vz8fHOZ1cXj69C1OgB9nXlWF6zV4SRJ27Ztc8Z9D7W3Oo6tdUlSSUmJM96lSxczx3rQtfXZSPZn4Osethx66KEx56D2xNP9Zh23vvPGOp58nYbjxo1zxq+88kozZ8mSJc6471yzOnR9+zZ06FBnfNWqVWaO1aXet29fM8f6TDds2GDmWNMC4pl8gIbJ+t6ON8c6333fA9a11XcdsK5RvvPT2gdrwoZv33z1gNUNfaDijh8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBCNZpxLenq6uczXpm2prKx0xn1t79aYFevB6JLddu57OLs1kqG0tNTM2bJlizPuG81i5aSlpZk51vuJZ1xEQUFBzDmoPdZx5jsHsrOznXHfcdaxY0dn3HecWWNWrO1L9jH4+uuvmzn333+/M+4b59KqVauYt2OxxiNJ9vvZuHGjmZObm+uM+8ZsWPsQz9gQ1L2qqipzmfX9bF27JPvn7MuxFBYWmsus65dvO/GMCbM+H9/nFhru+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAIBpNV2/btm1jzvF1mrZo0cIZ37Ztm5ljdUz5OvOsjmNfJ5PVcezbjvX5+DrzrPX5Pjfr84nnM8jPzzdzUPfy8vKccV/3uNU16vv5W+vzdexlZWU54zk5OWbOUUcd5Yz7uvk2bNjgjE+ePNnMufbaa53xE044wcwpKipyxsvLy80c6+H1S5cuNXOs7wHrO0WS2rVr54wvW7bMzEH98U2RsK4rvq576zzMzMw0c6yJED6+a6vF+l7xndO+Yx3f444fAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQjWacy4ABA8xlVvu2byzJkiVLnPF4xsb4WKNMrLhkt977RrNY42l84yKSk5Od8dWrV5s5DzzwgDN+9913mznWfvtGc6DupaenO+NpaWlmjjWSwTcCxnpwuzUeSZJatmzpjPuOTWv8iW/MijXKwhrzIkkpKSnOeDznmu97YNeuXc74pk2bzBzrO8/3WTPOpXHxjUGyxrlY57pkn1PWOSj5j3WLdTz7vjuaN29ea9v38Z2HByLu+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAIBpNV++RRx5pLisrK3PGfQ+Zvvrqq53x3//+92ZOt27dnHHfA6vj6bKzHkDty7EeZm11Ukl2p9fatWvNnL/+9a/OuK+r1/r5xPOgb9SerKwsZ9zXNWh1/FodtZJ93PoeHG/ZunWruczahx07dpg5rVu3dsbHjx9v5nzwwQfOeJcuXcycZ5991lxmyc/Pd8atDmEpvu5+388OjYvVIWt1x0r2NcrXbVubXbW+7xtrv33ntPW94vu+8V1bD0Tc8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABKLR9DBXVlaay6zxJz7WeIX/+I//MHOstveKigozJykpyRn3jVmxtuNjfQbxPHz622+/NZdt3rw55vVZrf85OTlmjjWywhoNg9jFc5xZD4HfuHGjmbNp0yZn3BpB5FNYWGgua9OmjTPuOz/XrVvnjPtGP3Ts2NEZt0bDSNLFF1/sjD/wwANmTjzfa9b57hvNEdoD6hs765oi2T/nli1bmjnWd6017kuSli5dai6Lle/Y9I2hsVjnrm9kSzyjpRoz7vgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAaTVdvZmamuczXIWuxHvLs636yWJ2OvmW+jj2r29HXfWetrzY7hOMVT5fV8ccf74y/9dZbtbJPsI9N3/FsdRT6uuKsjl9fx15ubq4zvnDhQjPn5JNPdsZ9D4G3zjXfeWOdH2vWrDFzrA72W265xcy5+eabnXHf55adne2M+36m8Xx/onHxfddax0ZKSoqZE890B8u2bdvMZfn5+c647/1YxzPH+f/hjh8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBANbpxLQUGBMx7PA93jYY1DkOx2cN/oB2sEi2+8QjwPjK6srKy1dVljMeJltd77xtP4fg6oHdYxWFxcbOYcfPDBzrjvge6pqanOuG8siXU8l5eXmznWMmtdkj2aJZ7zxnc8W+OjrLE1knTIIYc4419//XXM26moqDBzNm3aZC5Dw+O73ljngO/6aZ0DvmuU7zyMlTXuSbLHufj2LR6+74gDEXf8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQjaar19fFY3UsbdiwIebt+7pJt27d6oz7OgCtbiFfB6DVZeVj5fgeZm11ALZv3z7m7fs6s6yuTl932v7q4g5ZUlKSM56VlWXmWN18ZWVlZs5BBx3kjPuO8507dzrjvgfHW+daPA+o952f1vnuy4mnS9nqerZ+bpL9XpOTk80cHl7fuPiO53h+ltZ3re/YjOfaatm8ebO5zDoHWrRoYeZY++2bIhAa7vgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAo/AAAAALR4Ma5tGrVyhmPp4V95cqVZo7vofKxbsc3zsUa8eB7PxZfjrUPvpEZtflg6oULF5rLTjjhhJjX16ZNmx+zO9gH1vG8ZcsWMyctLc0ZX7x4sZljjR/xnTfWSIaioiIzxxoL4RsFZZ0D1vv0rc8a9yTZ3wPW2BrJ/o7yjb8oKSmJeTt5eXnOuO+cRv3xjWyxjrOKigozJzMz0xnftm2bmbN9+3ZzWax83zfW94BvpJFv5BO+xx0/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAhEg+vq7dSpkzPu6wC0OvM2btxo5gwePDim/fLtg69r0Oqq9XXb+h72bklISHDGfR1gvv22nHnmmc6477OOpxu6NjuO4Zafn++M+z777OxsZ3zatGlmzty5c53x3/72t2aOdcz4HrQ+b948Z9w6N3zLfMezddwmJyebOdb78Z3r1vv56quvzByr29H3PcC5duCwriu+n791PPvONd/1K1a+Tv2cnBxnfM2aNWZOYmKiM16b+9zYcccPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAo/AAAAAJB4QcAABCIBjfOxRpH4GvFtsaSfPnll2bOW2+9FduOyX4Iu2+Mg7XMl9OsmfvH4nvQttV673tgtfVw7NLSUjPnxRdfdMYvv/xyM8cameEbZVFWVmYuQ+1IS0tzxlNTU80c62e5atUqM6dLly7O+KGHHmrmbNiwwRlv0aKFmWN9D/jOG2uURYcOHcwc67vIt53Nmzc747169TJzSkpKnPFnnnnGzLEeau8bzYHGxTd+xzoHrGuXZF8j9tcxY50bkrRz505n3BrZ4ltmXVcl//fKgYg7fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQiAbX1Wt1FPo6QK1OJt+D1rdt2+aM+x7obrE6AyWpVatWznh+fr6Zs337dmd8zJgxZo7VtXX//ffHnON7oLfF+hlI9mfq69Tu06dPzPuA2Fhd3b6OOevY/OCDD8wcq4N96tSpZs7q1atj3jdrmdUZKNldsL7vG6s70NdtaZ1TS5YsMXO++uorZ9x33uzYscMZt6YlSHT8NjZWt7dkX1d8Xb1WR6vvXKtNvuO5uLg45vVZ0wp8Xb2+a/iBiDt+AAAAgaDwAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBANLhxLq1bt3bGfaMSrFEFRUVFtbJPe+Pbt/Xr18cU9xk3bpy5rLS01Bn3jaWoTd988425bOjQoc64r40/Ozv7R+8T/KzRSTk5OWaOb5SEpayszBm/6667Yl4X/KzvopYtW5o5vnE3aHh83+nWz9k32syyefPmmHPi4bsOWMviycnMzDRzfKNeDkTc8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQDS4VpaUlBRn3HqYumR35KxYsSLm7fseTF1RUeGMx/OA54SEhJhzrAew+/gezm59pk2a2P8esLoGFy1aFNuO7UU8Xc+ITfPmzZ3xtm3bmjnbt2+vte37jrPatL86233ntLUPvhxf56LF6qD2fQ9YEwHQMG3bts1cZnX1+o4l67oWz/WmtlnHs68L1/p80tLSzJx4phU0ZtzxAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEosGNc+nZs6cznp6ebuZYoxJWr14d8/bjGbNijTipbb7xF9ZnEM8D2OP5DLZu3Rpzjm90TosWLWJeH2KTl5cXU1yK7+dsiWdcSUO2v8bG+LRp08YZ37Vrl5nTtWtXZ3zu3Lm1sk+oXb7xYdbPf82aNTGvryGMc0lOTnbGfeOJrDFVvutnu3btYtuxRo47fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQiAbX1XvjjTc644ceeqiZs2rVKmd8/vz5MW/f1/1W3/ZXF2Q83YmffvqpuWzevHnOeGZmppkza9asmPcBsZk+fbozvnnzZjPH94D4WPm6xxtCh2xj9M477zjjvi7IBQsW1NHeoC5Y1ztJWrJkiTPuO6dzc3Od8cLCwpj2qy6sXbvWGe/QoYOZU1JS4oyXl5fHvJ0DFXf8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBSIiYmwAAABAE7vgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAE4v8Dy5hLyulLHB4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic `batching`, `sampling`, `shuffling` and `multiprocess` data loading. \n",
    "Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features (Xs) and labels.\n",
    "\n",
    "100 batch, where each batch has a size at 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <img src = \"https://miro.medium.com/max/1838/1*kcTPIyRkFD7uTKsGulHS3A.png\" style = \"width:70%\">\n",
    "</div>\n",
    "<!-- datascience\\machine_learning_models\\assets\\pytorch\\tensor-dimension.png -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar\n",
    "torch.tensor(10) # or  torch.tensor([10])\n",
    "# Rank 1\n",
    "torch.tensor([10, 5, 9])\n",
    "# Rank 2\n",
    "torch.tensor([[10, 5, 9], \n",
    "              [10, 5, 9]])\n",
    "# Rank 3\n",
    "torch.tensor([[[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]]])\n",
    "\n",
    "# Rank 4\n",
    "torch.tensor([[[[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]]],\n",
    "              [[[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of each image when we load the image is `(1, 28, 28)`, but the images are appended when we load, so we only see `(6000, 28, 28)` as the dimension of the data, that is, the data already appended.\n",
    "\n",
    "When apply the `batch_size = 64` \n",
    "1. Split the data in `int(6000/64)` batches\n",
    "2. In each batch there are 64 observations, but each is split to original dimencional, that is, `(1, 28, 28)`. So each batch is `Rank 4`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits from `nn.Module`. \n",
    "\n",
    "We define the layers of the network in the `__init__` function and specify how data will pass through the network in the forward function. \n",
    "\n",
    "To accelerate operations in the neural network, we move it to the `GPU` if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # --> shape --> (64, 10)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `28*28` is the width and height of the imagen, but flattened. This is the reason we pass `28*28`, a multiplication.\n",
    "* `512` is the output to the next layer. This point out the amount of nodes to next layer.\n",
    "* `Linear()` is the `preactivation function` $\\sum WX$\n",
    "* `ReLU()` is the activation function\n",
    "* Here we don't pass the output layer since the loss function (`CrossEntropyLoss`) need ``logits`` ($log \\frac{Pr(G = 1|X = x)}{Pr(G = K|X = x)} = β_{10} + β_{1}^T x \\; ... $) as input to compute the loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"./assets/batch.svg\" height=\"500\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Model Parameters\n",
    "\n",
    "To train a model, we need a `loss function` and an `optimizer`.\n",
    "\n",
    "The optimizer need to pass the parameters that it will optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross entropy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Surprise*\n",
    "\n",
    "It's the oppose to *probability*. If an event has higher probability it's means its complement has higher suprise, since its probability is lower. \n",
    "\n",
    "A possible meaures of *suprise* is $\\frac{1}{p}$, but it has a problem with lower $p$, $\\frac{1}{p}$ can grown to $\\infty$, so we use a version *log*, so suprise is defined by \n",
    "$$log(\\frac{1}{p})$$\n",
    "\n",
    "And the entropy is the expectation of *surprise*, for different events\n",
    "\n",
    "$$H = \\sum_{k=0}^{K-1}\\log(\\frac{1}{\\hat{p}_{k}})p_k$$\n",
    "\n",
    "For $n^{th}$ record , the output is $(\\hat{p}_0, \\hat{p}_1, \\dots, \\hat{p}_{K-1})$\n",
    "\n",
    "Recall the $n^{th}$ record only can belong to one class. So we can suppose the outcome is $(0, 1, 0, \\dots, 0)$, then, its cross-entropy would be \n",
    "\n",
    "$$H = \\log(\\frac{1}{\\hat{p}_{0}})0 + \\log(\\frac{1}{\\hat{p}_{1}})1  + ... + \\log(\\frac{1}{\\hat{p}_{K-1}})0 $$\n",
    "$$H=\\log(\\frac{1}{\\hat{p}_{1}})$$\n",
    "\n",
    "We can see that if $\\hat{p}_{1}$ increases, then $H$ decreases, that is, if the probability  the $n^{th}$ record belong to class $1$ increases, the cross-entropy is reduced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way more formal is done\n",
    "\n",
    "$$L=-\\log(\\hat{p})$$\n",
    "\n",
    "The metrics that is reduced is $\\sum_{1}^{N}-log(\\hat{p})$,  $N$ is size of training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # model.train() \n",
    "    # doesn't directly perform calculus itself, \n",
    "    # it sets a crucial flag that enables the model to learn through backpropagation,\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        # pred => (60, 10), \n",
    "        # pred is in term of logits\n",
    "        pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # sets all parameter gradients to zero.\n",
    "        # Without resetting, gradients from previous backpropagation steps would accumulate,\n",
    "        # leading to incorrect updates and potential model instability.\n",
    "        optimizer.zero_grad()\n",
    "        # \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, cumm = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{cumm:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"s-table\" align = \"center\">\n",
    "    <thead>\n",
    "    <tr>\n",
    "        <th><code>model.train()</code></th>\n",
    "        <th><a href=\"https://stackoverflow.com/a/66843176/9067615\"><code>model.eval()</code></a></th>\n",
    "    </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Sets model in <strong>train</strong>ing mode i.e. <br> <br> • <code>BatchNorm</code> layers use per-batch statistics <br> • <code>Dropout</code> layers activated <a href=\"https://stackoverflow.com/questions/66534762/which-pytorch-modules-are-affected-by-model-eval-and-model-train\">etc</a></td>\n",
    "            <td>Sets model in <strong>eval</strong>uation (inference) mode i.e. <br><br> • <code>BatchNorm</code> layers use running statistics <br> • <code>Dropout</code> layers de-activated etc</td>\n",
    "        </tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Equivalent to <code>model.train(False)</code>.</td>\n",
    "    </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `optimizer.zero_grad()` Sets the gradients of all optimized torch.Tensor s to zero.\n",
    "\n",
    "* `loss.backward()`\n",
    "\n",
    "  - It initiates the backward pass of backpropagation.\n",
    "  - It takes the calculated loss value (e.g., MSE, cross-entropy) and propagates it backward through the computational graph that was created during the forward pass.\n",
    "  - During this process, it calculates the gradients for each parameter (weights and biases) in all layers of the model. These gradients essentially tell us how much a change in each parameter would affect the overall loss.\n",
    "\n",
    "* `optimizer.step()` Performs a single optimization step (parameter update)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the model’s performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.159456  [    0/60000]\n",
      "loss: 2.156684  [ 6400/60000]\n",
      "loss: 2.094385  [12800/60000]\n",
      "loss: 2.107095  [19200/60000]\n",
      "loss: 2.061434  [25600/60000]\n",
      "loss: 1.995222  [32000/60000]\n",
      "loss: 2.030684  [38400/60000]\n",
      "loss: 1.952778  [44800/60000]\n",
      "loss: 1.939452  [51200/60000]\n",
      "loss: 1.883204  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.877073 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.903572  [    0/60000]\n",
      "loss: 1.888093  [ 6400/60000]\n",
      "loss: 1.762307  [12800/60000]\n",
      "loss: 1.800406  [19200/60000]\n",
      "loss: 1.704656  [25600/60000]\n",
      "loss: 1.645696  [32000/60000]\n",
      "loss: 1.671834  [38400/60000]\n",
      "loss: 1.578195  [44800/60000]\n",
      "loss: 1.589045  [51200/60000]\n",
      "loss: 1.498618  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 1.508925 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.569175  [    0/60000]\n",
      "loss: 1.549487  [ 6400/60000]\n",
      "loss: 1.390301  [12800/60000]\n",
      "loss: 1.463606  [19200/60000]\n",
      "loss: 1.357040  [25600/60000]\n",
      "loss: 1.338383  [32000/60000]\n",
      "loss: 1.362880  [38400/60000]\n",
      "loss: 1.290085  [44800/60000]\n",
      "loss: 1.320315  [51200/60000]\n",
      "loss: 1.235307  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.249877 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.320163  [    0/60000]\n",
      "loss: 1.313855  [ 6400/60000]\n",
      "loss: 1.141252  [12800/60000]\n",
      "loss: 1.248790  [19200/60000]\n",
      "loss: 1.133506  [25600/60000]\n",
      "loss: 1.142705  [32000/60000]\n",
      "loss: 1.176542  [38400/60000]\n",
      "loss: 1.114404  [44800/60000]\n",
      "loss: 1.152688  [51200/60000]\n",
      "loss: 1.081753  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.089723 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.153917  [    0/60000]\n",
      "loss: 1.165284  [ 6400/60000]\n",
      "loss: 0.978002  [12800/60000]\n",
      "loss: 1.113772  [19200/60000]\n",
      "loss: 0.994776  [25600/60000]\n",
      "loss: 1.010041  [32000/60000]\n",
      "loss: 1.061403  [38400/60000]\n",
      "loss: 1.002225  [44800/60000]\n",
      "loss: 1.043197  [51200/60000]\n",
      "loss: 0.984997  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.985846 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.037533  [    0/60000]\n",
      "loss: 1.068297  [ 6400/60000]\n",
      "loss: 0.865517  [12800/60000]\n",
      "loss: 1.023706  [19200/60000]\n",
      "loss: 0.906882  [25600/60000]\n",
      "loss: 0.915610  [32000/60000]\n",
      "loss: 0.986069  [38400/60000]\n",
      "loss: 0.928271  [44800/60000]\n",
      "loss: 0.966985  [51200/60000]\n",
      "loss: 0.919744  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.914493 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.951271  [    0/60000]\n",
      "loss: 1.000122  [ 6400/60000]\n",
      "loss: 0.784145  [12800/60000]\n",
      "loss: 0.959684  [19200/60000]\n",
      "loss: 0.847562  [25600/60000]\n",
      "loss: 0.845206  [32000/60000]\n",
      "loss: 0.932929  [38400/60000]\n",
      "loss: 0.877699  [44800/60000]\n",
      "loss: 0.911576  [51200/60000]\n",
      "loss: 0.872365  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.862505 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.884389  [    0/60000]\n",
      "loss: 0.948314  [ 6400/60000]\n",
      "loss: 0.722510  [12800/60000]\n",
      "loss: 0.911525  [19200/60000]\n",
      "loss: 0.804997  [25600/60000]\n",
      "loss: 0.791428  [32000/60000]\n",
      "loss: 0.892434  [38400/60000]\n",
      "loss: 0.841831  [44800/60000]\n",
      "loss: 0.869695  [51200/60000]\n",
      "loss: 0.835418  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.822683 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.830713  [    0/60000]\n",
      "loss: 0.906371  [ 6400/60000]\n",
      "loss: 0.673901  [12800/60000]\n",
      "loss: 0.874045  [19200/60000]\n",
      "loss: 0.772703  [25600/60000]\n",
      "loss: 0.749660  [32000/60000]\n",
      "loss: 0.859374  [38400/60000]\n",
      "loss: 0.815133  [44800/60000]\n",
      "loss: 0.836966  [51200/60000]\n",
      "loss: 0.805340  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.790762 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.786439  [    0/60000]\n",
      "loss: 0.870720  [ 6400/60000]\n",
      "loss: 0.634531  [12800/60000]\n",
      "loss: 0.843940  [19200/60000]\n",
      "loss: 0.747107  [25600/60000]\n",
      "loss: 0.716600  [32000/60000]\n",
      "loss: 0.830936  [38400/60000]\n",
      "loss: 0.794123  [44800/60000]\n",
      "loss: 0.810247  [51200/60000]\n",
      "loss: 0.779912  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.764140 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.748753  [    0/60000]\n",
      "loss: 0.839257  [ 6400/60000]\n",
      "loss: 0.601877  [12800/60000]\n",
      "loss: 0.819132  [19200/60000]\n",
      "loss: 0.726045  [25600/60000]\n",
      "loss: 0.690026  [32000/60000]\n",
      "loss: 0.805448  [38400/60000]\n",
      "loss: 0.776735  [44800/60000]\n",
      "loss: 0.787870  [51200/60000]\n",
      "loss: 0.757721  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.741177 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.716108  [    0/60000]\n",
      "loss: 0.810978  [ 6400/60000]\n",
      "loss: 0.574208  [12800/60000]\n",
      "loss: 0.798252  [19200/60000]\n",
      "loss: 0.708076  [25600/60000]\n",
      "loss: 0.668270  [32000/60000]\n",
      "loss: 0.782273  [38400/60000]\n",
      "loss: 0.761856  [44800/60000]\n",
      "loss: 0.768772  [51200/60000]\n",
      "loss: 0.738026  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.720899 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.687557  [    0/60000]\n",
      "loss: 0.785140  [ 6400/60000]\n",
      "loss: 0.550221  [12800/60000]\n",
      "loss: 0.780253  [19200/60000]\n",
      "loss: 0.692479  [25600/60000]\n",
      "loss: 0.650190  [32000/60000]\n",
      "loss: 0.760851  [38400/60000]\n",
      "loss: 0.748587  [44800/60000]\n",
      "loss: 0.752256  [51200/60000]\n",
      "loss: 0.720324  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.702675 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.662335  [    0/60000]\n",
      "loss: 0.761364  [ 6400/60000]\n",
      "loss: 0.529284  [12800/60000]\n",
      "loss: 0.764316  [19200/60000]\n",
      "loss: 0.678720  [25600/60000]\n",
      "loss: 0.634988  [32000/60000]\n",
      "loss: 0.740731  [38400/60000]\n",
      "loss: 0.736582  [44800/60000]\n",
      "loss: 0.737704  [51200/60000]\n",
      "loss: 0.704298  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.686097 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.639904  [    0/60000]\n",
      "loss: 0.739401  [ 6400/60000]\n",
      "loss: 0.510818  [12800/60000]\n",
      "loss: 0.750055  [19200/60000]\n",
      "loss: 0.666597  [25600/60000]\n",
      "loss: 0.622032  [32000/60000]\n",
      "loss: 0.721730  [38400/60000]\n",
      "loss: 0.725771  [44800/60000]\n",
      "loss: 0.724955  [51200/60000]\n",
      "loss: 0.689540  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.670925 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.619857  [    0/60000]\n",
      "loss: 0.719242  [ 6400/60000]\n",
      "loss: 0.494446  [12800/60000]\n",
      "loss: 0.736972  [19200/60000]\n",
      "loss: 0.655858  [25600/60000]\n",
      "loss: 0.610807  [32000/60000]\n",
      "loss: 0.703963  [38400/60000]\n",
      "loss: 0.716121  [44800/60000]\n",
      "loss: 0.713648  [51200/60000]\n",
      "loss: 0.675919  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.657036 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.601931  [    0/60000]\n",
      "loss: 0.700869  [ 6400/60000]\n",
      "loss: 0.479813  [12800/60000]\n",
      "loss: 0.724930  [19200/60000]\n",
      "loss: 0.646451  [25600/60000]\n",
      "loss: 0.601123  [32000/60000]\n",
      "loss: 0.687431  [38400/60000]\n",
      "loss: 0.707689  [44800/60000]\n",
      "loss: 0.703747  [51200/60000]\n",
      "loss: 0.663266  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.644299 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.585876  [    0/60000]\n",
      "loss: 0.684050  [ 6400/60000]\n",
      "loss: 0.466681  [12800/60000]\n",
      "loss: 0.713737  [19200/60000]\n",
      "loss: 0.638061  [25600/60000]\n",
      "loss: 0.592672  [32000/60000]\n",
      "loss: 0.672062  [38400/60000]\n",
      "loss: 0.700302  [44800/60000]\n",
      "loss: 0.695101  [51200/60000]\n",
      "loss: 0.651539  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.632596 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.571381  [    0/60000]\n",
      "loss: 0.668683  [ 6400/60000]\n",
      "loss: 0.454873  [12800/60000]\n",
      "loss: 0.703296  [19200/60000]\n",
      "loss: 0.630548  [25600/60000]\n",
      "loss: 0.585210  [32000/60000]\n",
      "loss: 0.657757  [38400/60000]\n",
      "loss: 0.693924  [44800/60000]\n",
      "loss: 0.687636  [51200/60000]\n",
      "loss: 0.640487  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.621829 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.558144  [    0/60000]\n",
      "loss: 0.654664  [ 6400/60000]\n",
      "loss: 0.444108  [12800/60000]\n",
      "loss: 0.693460  [19200/60000]\n",
      "loss: 0.623713  [25600/60000]\n",
      "loss: 0.578589  [32000/60000]\n",
      "loss: 0.644438  [38400/60000]\n",
      "loss: 0.688508  [44800/60000]\n",
      "loss: 0.681172  [51200/60000]\n",
      "loss: 0.630116  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.611918 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.545997  [    0/60000]\n",
      "loss: 0.641794  [ 6400/60000]\n",
      "loss: 0.434258  [12800/60000]\n",
      "loss: 0.684140  [19200/60000]\n",
      "loss: 0.617397  [25600/60000]\n",
      "loss: 0.572642  [32000/60000]\n",
      "loss: 0.632086  [38400/60000]\n",
      "loss: 0.683965  [44800/60000]\n",
      "loss: 0.675677  [51200/60000]\n",
      "loss: 0.620294  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.602778 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.534745  [    0/60000]\n",
      "loss: 0.629997  [ 6400/60000]\n",
      "loss: 0.425216  [12800/60000]\n",
      "loss: 0.675267  [19200/60000]\n",
      "loss: 0.611465  [25600/60000]\n",
      "loss: 0.567273  [32000/60000]\n",
      "loss: 0.620731  [38400/60000]\n",
      "loss: 0.680309  [44800/60000]\n",
      "loss: 0.671068  [51200/60000]\n",
      "loss: 0.610861  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.594325 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.524320  [    0/60000]\n",
      "loss: 0.619117  [ 6400/60000]\n",
      "loss: 0.416789  [12800/60000]\n",
      "loss: 0.666867  [19200/60000]\n",
      "loss: 0.605842  [25600/60000]\n",
      "loss: 0.562271  [32000/60000]\n",
      "loss: 0.610421  [38400/60000]\n",
      "loss: 0.677325  [44800/60000]\n",
      "loss: 0.667255  [51200/60000]\n",
      "loss: 0.601799  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.586513 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.514544  [    0/60000]\n",
      "loss: 0.608971  [ 6400/60000]\n",
      "loss: 0.409005  [12800/60000]\n",
      "loss: 0.658928  [19200/60000]\n",
      "loss: 0.600405  [25600/60000]\n",
      "loss: 0.557626  [32000/60000]\n",
      "loss: 0.600841  [38400/60000]\n",
      "loss: 0.675071  [44800/60000]\n",
      "loss: 0.664099  [51200/60000]\n",
      "loss: 0.592978  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.579276 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.505361  [    0/60000]\n",
      "loss: 0.599494  [ 6400/60000]\n",
      "loss: 0.401873  [12800/60000]\n",
      "loss: 0.651414  [19200/60000]\n",
      "loss: 0.595098  [25600/60000]\n",
      "loss: 0.553286  [32000/60000]\n",
      "loss: 0.591943  [38400/60000]\n",
      "loss: 0.673360  [44800/60000]\n",
      "loss: 0.661454  [51200/60000]\n",
      "loss: 0.584462  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.572556 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.496656  [    0/60000]\n",
      "loss: 0.590786  [ 6400/60000]\n",
      "loss: 0.395280  [12800/60000]\n",
      "loss: 0.644283  [19200/60000]\n",
      "loss: 0.589885  [25600/60000]\n",
      "loss: 0.549141  [32000/60000]\n",
      "loss: 0.583712  [38400/60000]\n",
      "loss: 0.672187  [44800/60000]\n",
      "loss: 0.659204  [51200/60000]\n",
      "loss: 0.576157  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.566301 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.488391  [    0/60000]\n",
      "loss: 0.582706  [ 6400/60000]\n",
      "loss: 0.389123  [12800/60000]\n",
      "loss: 0.637445  [19200/60000]\n",
      "loss: 0.584652  [25600/60000]\n",
      "loss: 0.545074  [32000/60000]\n",
      "loss: 0.576074  [38400/60000]\n",
      "loss: 0.671447  [44800/60000]\n",
      "loss: 0.657290  [51200/60000]\n",
      "loss: 0.568047  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.560468 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.480497  [    0/60000]\n",
      "loss: 0.575186  [ 6400/60000]\n",
      "loss: 0.383321  [12800/60000]\n",
      "loss: 0.630892  [19200/60000]\n",
      "loss: 0.579407  [25600/60000]\n",
      "loss: 0.541074  [32000/60000]\n",
      "loss: 0.568967  [38400/60000]\n",
      "loss: 0.671093  [44800/60000]\n",
      "loss: 0.655623  [51200/60000]\n",
      "loss: 0.560126  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.555014 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.472957  [    0/60000]\n",
      "loss: 0.568184  [ 6400/60000]\n",
      "loss: 0.377866  [12800/60000]\n",
      "loss: 0.624604  [19200/60000]\n",
      "loss: 0.574158  [25600/60000]\n",
      "loss: 0.537075  [32000/60000]\n",
      "loss: 0.562424  [38400/60000]\n",
      "loss: 0.671028  [44800/60000]\n",
      "loss: 0.654117  [51200/60000]\n",
      "loss: 0.552399  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.549907 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.465756  [    0/60000]\n",
      "loss: 0.561618  [ 6400/60000]\n",
      "loss: 0.372679  [12800/60000]\n",
      "loss: 0.618555  [19200/60000]\n",
      "loss: 0.568941  [25600/60000]\n",
      "loss: 0.533141  [32000/60000]\n",
      "loss: 0.556357  [38400/60000]\n",
      "loss: 0.671210  [44800/60000]\n",
      "loss: 0.652737  [51200/60000]\n",
      "loss: 0.544869  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.545112 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.458887  [    0/60000]\n",
      "loss: 0.555482  [ 6400/60000]\n",
      "loss: 0.367767  [12800/60000]\n",
      "loss: 0.612721  [19200/60000]\n",
      "loss: 0.563711  [25600/60000]\n",
      "loss: 0.529227  [32000/60000]\n",
      "loss: 0.550726  [38400/60000]\n",
      "loss: 0.671548  [44800/60000]\n",
      "loss: 0.651420  [51200/60000]\n",
      "loss: 0.537525  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.540607 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.452287  [    0/60000]\n",
      "loss: 0.549747  [ 6400/60000]\n",
      "loss: 0.363150  [12800/60000]\n",
      "loss: 0.607071  [19200/60000]\n",
      "loss: 0.558477  [25600/60000]\n",
      "loss: 0.525358  [32000/60000]\n",
      "loss: 0.545466  [38400/60000]\n",
      "loss: 0.671989  [44800/60000]\n",
      "loss: 0.650200  [51200/60000]\n",
      "loss: 0.530423  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.536360 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.445941  [    0/60000]\n",
      "loss: 0.544389  [ 6400/60000]\n",
      "loss: 0.358795  [12800/60000]\n",
      "loss: 0.601645  [19200/60000]\n",
      "loss: 0.553289  [25600/60000]\n",
      "loss: 0.521515  [32000/60000]\n",
      "loss: 0.540600  [38400/60000]\n",
      "loss: 0.672517  [44800/60000]\n",
      "loss: 0.648953  [51200/60000]\n",
      "loss: 0.523540  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.532356 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.439857  [    0/60000]\n",
      "loss: 0.539412  [ 6400/60000]\n",
      "loss: 0.354671  [12800/60000]\n",
      "loss: 0.596435  [19200/60000]\n",
      "loss: 0.548159  [25600/60000]\n",
      "loss: 0.517631  [32000/60000]\n",
      "loss: 0.536030  [38400/60000]\n",
      "loss: 0.673019  [44800/60000]\n",
      "loss: 0.647737  [51200/60000]\n",
      "loss: 0.516894  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.528576 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.434011  [    0/60000]\n",
      "loss: 0.534764  [ 6400/60000]\n",
      "loss: 0.350715  [12800/60000]\n",
      "loss: 0.591373  [19200/60000]\n",
      "loss: 0.543111  [25600/60000]\n",
      "loss: 0.513794  [32000/60000]\n",
      "loss: 0.531740  [38400/60000]\n",
      "loss: 0.673474  [44800/60000]\n",
      "loss: 0.646456  [51200/60000]\n",
      "loss: 0.510550  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.524999 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.428316  [    0/60000]\n",
      "loss: 0.530371  [ 6400/60000]\n",
      "loss: 0.346899  [12800/60000]\n",
      "loss: 0.586465  [19200/60000]\n",
      "loss: 0.538159  [25600/60000]\n",
      "loss: 0.509978  [32000/60000]\n",
      "loss: 0.527703  [38400/60000]\n",
      "loss: 0.673871  [44800/60000]\n",
      "loss: 0.645187  [51200/60000]\n",
      "loss: 0.504460  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.521611 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.422811  [    0/60000]\n",
      "loss: 0.526268  [ 6400/60000]\n",
      "loss: 0.343231  [12800/60000]\n",
      "loss: 0.581723  [19200/60000]\n",
      "loss: 0.533204  [25600/60000]\n",
      "loss: 0.506225  [32000/60000]\n",
      "loss: 0.523937  [38400/60000]\n",
      "loss: 0.674160  [44800/60000]\n",
      "loss: 0.643915  [51200/60000]\n",
      "loss: 0.498657  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.518399 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.417478  [    0/60000]\n",
      "loss: 0.522468  [ 6400/60000]\n",
      "loss: 0.339790  [12800/60000]\n",
      "loss: 0.577184  [19200/60000]\n",
      "loss: 0.528342  [25600/60000]\n",
      "loss: 0.502597  [32000/60000]\n",
      "loss: 0.520419  [38400/60000]\n",
      "loss: 0.674347  [44800/60000]\n",
      "loss: 0.642647  [51200/60000]\n",
      "loss: 0.493133  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.515348 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.412291  [    0/60000]\n",
      "loss: 0.518895  [ 6400/60000]\n",
      "loss: 0.336523  [12800/60000]\n",
      "loss: 0.572773  [19200/60000]\n",
      "loss: 0.523589  [25600/60000]\n",
      "loss: 0.499003  [32000/60000]\n",
      "loss: 0.517076  [38400/60000]\n",
      "loss: 0.674441  [44800/60000]\n",
      "loss: 0.641296  [51200/60000]\n",
      "loss: 0.487818  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.512443 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.407244  [    0/60000]\n",
      "loss: 0.515508  [ 6400/60000]\n",
      "loss: 0.333364  [12800/60000]\n",
      "loss: 0.568474  [19200/60000]\n",
      "loss: 0.518882  [25600/60000]\n",
      "loss: 0.495494  [32000/60000]\n",
      "loss: 0.513930  [38400/60000]\n",
      "loss: 0.674442  [44800/60000]\n",
      "loss: 0.639855  [51200/60000]\n",
      "loss: 0.482779  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.509676 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.402350  [    0/60000]\n",
      "loss: 0.512295  [ 6400/60000]\n",
      "loss: 0.330346  [12800/60000]\n",
      "loss: 0.564302  [19200/60000]\n",
      "loss: 0.514246  [25600/60000]\n",
      "loss: 0.492071  [32000/60000]\n",
      "loss: 0.510944  [38400/60000]\n",
      "loss: 0.674251  [44800/60000]\n",
      "loss: 0.638392  [51200/60000]\n",
      "loss: 0.477971  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.507040 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.397541  [    0/60000]\n",
      "loss: 0.509215  [ 6400/60000]\n",
      "loss: 0.327420  [12800/60000]\n",
      "loss: 0.560312  [19200/60000]\n",
      "loss: 0.509663  [25600/60000]\n",
      "loss: 0.488746  [32000/60000]\n",
      "loss: 0.508145  [38400/60000]\n",
      "loss: 0.673924  [44800/60000]\n",
      "loss: 0.636846  [51200/60000]\n",
      "loss: 0.473396  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.504519 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.392865  [    0/60000]\n",
      "loss: 0.506262  [ 6400/60000]\n",
      "loss: 0.324589  [12800/60000]\n",
      "loss: 0.556425  [19200/60000]\n",
      "loss: 0.505239  [25600/60000]\n",
      "loss: 0.485456  [32000/60000]\n",
      "loss: 0.505473  [38400/60000]\n",
      "loss: 0.673404  [44800/60000]\n",
      "loss: 0.635218  [51200/60000]\n",
      "loss: 0.469094  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.502108 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.388298  [    0/60000]\n",
      "loss: 0.503480  [ 6400/60000]\n",
      "loss: 0.321857  [12800/60000]\n",
      "loss: 0.552658  [19200/60000]\n",
      "loss: 0.500891  [25600/60000]\n",
      "loss: 0.482220  [32000/60000]\n",
      "loss: 0.502889  [38400/60000]\n",
      "loss: 0.672678  [44800/60000]\n",
      "loss: 0.633545  [51200/60000]\n",
      "loss: 0.464987  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.499798 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.383877  [    0/60000]\n",
      "loss: 0.500802  [ 6400/60000]\n",
      "loss: 0.319226  [12800/60000]\n",
      "loss: 0.549024  [19200/60000]\n",
      "loss: 0.496682  [25600/60000]\n",
      "loss: 0.479056  [32000/60000]\n",
      "loss: 0.500409  [38400/60000]\n",
      "loss: 0.671796  [44800/60000]\n",
      "loss: 0.631808  [51200/60000]\n",
      "loss: 0.461148  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.497585 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.379540  [    0/60000]\n",
      "loss: 0.498272  [ 6400/60000]\n",
      "loss: 0.316730  [12800/60000]\n",
      "loss: 0.545516  [19200/60000]\n",
      "loss: 0.492609  [25600/60000]\n",
      "loss: 0.476014  [32000/60000]\n",
      "loss: 0.498042  [38400/60000]\n",
      "loss: 0.670811  [44800/60000]\n",
      "loss: 0.630082  [51200/60000]\n",
      "loss: 0.457473  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.495465 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.375318  [    0/60000]\n",
      "loss: 0.495844  [ 6400/60000]\n",
      "loss: 0.314339  [12800/60000]\n",
      "loss: 0.542112  [19200/60000]\n",
      "loss: 0.488649  [25600/60000]\n",
      "loss: 0.473120  [32000/60000]\n",
      "loss: 0.495772  [38400/60000]\n",
      "loss: 0.669684  [44800/60000]\n",
      "loss: 0.628275  [51200/60000]\n",
      "loss: 0.454045  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.493426 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.371221  [    0/60000]\n",
      "loss: 0.493534  [ 6400/60000]\n",
      "loss: 0.312032  [12800/60000]\n",
      "loss: 0.538806  [19200/60000]\n",
      "loss: 0.484742  [25600/60000]\n",
      "loss: 0.470324  [32000/60000]\n",
      "loss: 0.493580  [38400/60000]\n",
      "loss: 0.668434  [44800/60000]\n",
      "loss: 0.626421  [51200/60000]\n",
      "loss: 0.450814  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.491465 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.367241  [    0/60000]\n",
      "loss: 0.491289  [ 6400/60000]\n",
      "loss: 0.309788  [12800/60000]\n",
      "loss: 0.535588  [19200/60000]\n",
      "loss: 0.480928  [25600/60000]\n",
      "loss: 0.467620  [32000/60000]\n",
      "loss: 0.491466  [38400/60000]\n",
      "loss: 0.667019  [44800/60000]\n",
      "loss: 0.624571  [51200/60000]\n",
      "loss: 0.447761  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.489578 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.363362  [    0/60000]\n",
      "loss: 0.489135  [ 6400/60000]\n",
      "loss: 0.307596  [12800/60000]\n",
      "loss: 0.532490  [19200/60000]\n",
      "loss: 0.477177  [25600/60000]\n",
      "loss: 0.465073  [32000/60000]\n",
      "loss: 0.489416  [38400/60000]\n",
      "loss: 0.665502  [44800/60000]\n",
      "loss: 0.622716  [51200/60000]\n",
      "loss: 0.444873  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.487760 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------------]\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models\n",
    "\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
