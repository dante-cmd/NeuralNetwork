{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;width:100%; height:120px;align-items: center; padding:20px; box-sizing:border-box;\">\n",
    "    <h1 style=\"margin:0;padding:0;\" >PyTorch</h1>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c6/PyTorch_logo_black.svg\" \n",
    "alt=\"PyTorch.log\" style=\"height:110px;margin:0;passing:0;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n",
    "\n",
    "The features of the documentation are classified as:\n",
    "* *Stable*: These features will be maintained long-term\n",
    "* *Beta*: These features are tagged as Beta because the API may change based on user feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best ways to undertand how a package works is using simulate data and try in several situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import getitem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([70])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 4, 6) \n",
    "y = torch.rand(2, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.6453],\n",
       "         [1.0561],\n",
       "         [2.2376],\n",
       "         [2.2982]],\n",
       "\n",
       "        [[1.8947],\n",
       "         [1.7999],\n",
       "         [1.4242],\n",
       "         [1.5013]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8947],\n",
       "        [1.7999],\n",
       "        [1.4242],\n",
       "        [1.5013]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]@y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function we are defined is \n",
    "# y = sin(x1*b1 + x2*b2) + 1.6 + e\n",
    "\n",
    "size= 5_000\n",
    "b = torch.tensor([1.2, 0.8])\n",
    "x = torch.rand((size, 2))\n",
    "e = torch.randn(size, 1)\n",
    "y = torch.sin((x*b).sum(axis=1, keepdims=True)) + 1.6 + e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomDataset(x, y)\n",
    "data_train, data_test = random_split(data, [0.8, 0.2])\n",
    "data_train_loader = DataLoader(data_train, batch_size=10,shuffle=True)\n",
    "data_test_loader = DataLoader(data_test, batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predict = self.stack(x)\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # doesn't directly perform calculus itself, \n",
    "    # it sets a crucial flag that enables the model to learn through backpropagation,\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # The model returns the prediction using forward propagation\n",
    "        pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # sets all parameter gradients to zero.\n",
    "        # Without resetting, gradients from previous backpropagation steps would accumulate,\n",
    "        # leading to incorrect updates and potential model instability.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # It triggers the backpropagation process, \n",
    "        # Calculate the gradients of the loss with respect to the model's parameters.\n",
    "        # and save the gradients in the model's parameters\n",
    "        # To access to the gradients, you can pass\n",
    "        # getitem(model.stack, 0).weight.grad\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            loss, cumm = loss.item(), batch_idx * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{cumm:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.108708  [    0/ 4000]\n",
      "loss: 3.099995  [ 1000/ 4000]\n",
      "loss: 1.876426  [ 2000/ 4000]\n",
      "loss: 2.374399  [ 3000/ 4000]\n",
      "loss: 0.439486  [    0/ 4000]\n",
      "loss: 0.685792  [ 1000/ 4000]\n",
      "loss: 1.091935  [ 2000/ 4000]\n",
      "loss: 1.518881  [ 3000/ 4000]\n",
      "loss: 1.376373  [    0/ 4000]\n",
      "loss: 0.485636  [ 1000/ 4000]\n",
      "loss: 0.697991  [ 2000/ 4000]\n",
      "loss: 1.591280  [ 3000/ 4000]\n",
      "loss: 1.677449  [    0/ 4000]\n",
      "loss: 0.524008  [ 1000/ 4000]\n",
      "loss: 0.402611  [ 2000/ 4000]\n",
      "loss: 1.216242  [ 3000/ 4000]\n",
      "loss: 0.852296  [    0/ 4000]\n",
      "loss: 1.603309  [ 1000/ 4000]\n",
      "loss: 1.346547  [ 2000/ 4000]\n",
      "loss: 0.786365  [ 3000/ 4000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('stack.0.weight',\n",
       "              tensor([[-0.4876,  0.1538],\n",
       "                      [ 0.1750,  0.1111],\n",
       "                      [ 0.4104, -0.0481],\n",
       "                      [ 0.1160, -0.1167],\n",
       "                      [ 0.2747,  0.2686],\n",
       "                      [ 0.6020, -0.6964],\n",
       "                      [ 0.1112, -0.4851],\n",
       "                      [ 0.2597,  0.1659],\n",
       "                      [ 0.3146, -0.1387],\n",
       "                      [ 0.0534,  0.5781]])),\n",
       "             ('stack.0.bias',\n",
       "              tensor([ 0.1089,  0.7959, -0.5062,  0.7522, -0.6520, -0.4923, -0.3553,  0.4368,\n",
       "                      -0.1985, -0.1540])),\n",
       "             ('stack.2.weight',\n",
       "              tensor([[-0.0038,  0.7287,  0.0464,  0.3651,  0.0900,  0.0165, -0.3056,  0.4687,\n",
       "                       -0.2441,  0.0749]])),\n",
       "             ('stack.2.bias', tensor([1.1172]))])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = range(5)\n",
    "for eporch in epochs:\n",
    "    train(data_train_loader, model, loss_fn, optimizer)\n",
    "    \n",
    "model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with data\n",
    "There are two `classes` that we usually use to working with the data\n",
    "\n",
    "<div align=\"center\">\n",
    "    <span style =\"background-color:rgb(220, 220, 220);color:rgb(192, 57, 57)\">torch.utils.data.DataLoader</span> and <span style=\"background-color:rgb(220, 220, 220);color:rgb(192, 57, 57)\">torch.utils.data.Dataset </span>\n",
    "</div>\n",
    "\n",
    "`Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset\n",
    "\n",
    "\n",
    "<!-- torch.utils.data.DataLoader and `torch.utils.data.Dataset. Dataset -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as `TorchText`, `TorchVision`, and `TorchAudio`, all of which include datasets. For this tutorial, we will be using a `TorchVision` dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like `CIFAR`, `COCO`.\n",
    "\n",
    "In this tutorial, we use the `FashionMNIST` dataset. Every TorchVision `Dataset` includes two arguments:\n",
    "\n",
    "* `root` is the name of the directory where the files will be stored from the current directory.\n",
    "* `train` is boolean type.\n",
    "* `download` point out if we want to download or not\n",
    "* `transform` and `target_transform` to modify the samples and labels respectively.\n",
    "\n",
    "\n",
    "> `ToTensor()` Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root='data-pytorch',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data-pytorch',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the images that represent the data, we use the followinf code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic `batching`, `sampling`, `shuffling` and `multiprocess` data loading. \n",
    "Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features (Xs) and labels.\n",
    "\n",
    "100 batch, where each batch has a size at 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <img src = \"https://miro.medium.com/max/1838/1*kcTPIyRkFD7uTKsGulHS3A.png\" style = \"width:70%\">\n",
    "</div>\n",
    "<!-- datascience\\machine_learning_models\\assets\\pytorch\\tensor-dimension.png -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar\n",
    "torch.tensor(10) # or  torch.tensor([10])\n",
    "# Rank 1\n",
    "torch.tensor([10, 5, 9])\n",
    "# Rank 2\n",
    "torch.tensor([[10, 5, 9], \n",
    "              [10, 5, 9]])\n",
    "# Rank 3\n",
    "torch.tensor([[[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]]])\n",
    "\n",
    "# Rank 4\n",
    "torch.tensor([[[[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]]],\n",
    "              [[[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]],\n",
    "              [[10, 5, 9], \n",
    "              [10, 5, 9]]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of each image when we load the image is `(1, 28, 28)`, but the images are appended when we load, so we only see `(6000, 28, 28)` as the dimension of the data, that is, the data already appended.\n",
    "\n",
    "When apply the `batch_size = 64` \n",
    "1. Split the data in `int(6000/64)` batches\n",
    "2. In each batch there are 64 observations, but each is split to original dimencional, that is, `(1, 28, 28)`. So each batch is `Rank 4`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits from `nn.Module`. \n",
    "\n",
    "We define the layers of the network in the `__init__` function and specify how data will pass through the network in the forward function. \n",
    "\n",
    "To accelerate operations in the neural network, we move it to the `GPU` if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # --> shape --> (64, 10)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `28*28` is the width and height of the imagen, but flattened. This is the reason we pass `28*28`, a multiplication.\n",
    "* `512` is the output to the next layer. This point out the amount of nodes to next layer.\n",
    "* `Linear()` is the `preactivation function` $\\sum WX$\n",
    "* `ReLU()` is the activation function\n",
    "* Here we don't pass the output layer since the loss function (`CrossEntropyLoss`) need ``logits`` ($log \\frac{Pr(G = 1|X = x)}{Pr(G = K|X = x)} = _{10} + _{1}^T x \\; ... $) as input to compute the loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"./assets/batch.svg\" height=\"500\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Model Parameters\n",
    "\n",
    "To train a model, we need a `loss function` and an `optimizer`.\n",
    "\n",
    "The optimizer need to pass the parameters that it will optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross entropy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Surprise*\n",
    "\n",
    "It's the oppose to *probability*. If an event has higher probability it's means its complement has higher suprise, since its probability is lower. \n",
    "\n",
    "A possible meaures of *suprise* is $\\frac{1}{p}$, but it has a problem with lower $p$, $\\frac{1}{p}$ can grown to $\\infty$, so we use a version *log*, so suprise is defined by \n",
    "$$log(\\frac{1}{p})$$\n",
    "\n",
    "And the entropy is the expectation of *surprise*, for different events\n",
    "\n",
    "$$H = \\sum_{k=0}^{K-1}\\log(\\frac{1}{\\hat{p}_{k}})p_k$$\n",
    "\n",
    "For $n^{th}$ record , the output is $(\\hat{p}_0, \\hat{p}_1, \\dots, \\hat{p}_{K-1})$\n",
    "\n",
    "Recall the $n^{th}$ record only can belong to one class. So we can suppose the outcome is $(0, 1, 0, \\dots, 0)$, then, its cross-entropy would be \n",
    "\n",
    "$$H = \\log(\\frac{1}{\\hat{p}_{0}})0 + \\log(\\frac{1}{\\hat{p}_{1}})1  + ... + \\log(\\frac{1}{\\hat{p}_{K-1}})0 $$\n",
    "$$H=\\log(\\frac{1}{\\hat{p}_{1}})$$\n",
    "\n",
    "We can see that if $\\hat{p}_{1}$ increases, then $H$ decreases, that is, if the probability  the $n^{th}$ record belong to class $1$ increases, the cross-entropy is reduced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way more formal is done\n",
    "\n",
    "$$L=-\\log(\\hat{p})$$\n",
    "\n",
    "The metrics that is reduced is $\\sum_{1}^{N}-log(\\hat{p})$,  $N$ is size of training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the models parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # model.train() \n",
    "    # doesn't directly perform calculus itself, \n",
    "    # it sets a crucial flag that enables the model to learn through backpropagation,\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        # pred => (60, 10), \n",
    "        # pred is in term of logits\n",
    "        pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # sets all parameter gradients to zero.\n",
    "        # Without resetting, gradients from previous backpropagation steps would accumulate,\n",
    "        # leading to incorrect updates and potential model instability.\n",
    "        optimizer.zero_grad()\n",
    "        # \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, cumm = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{cumm:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"s-table\" align = \"center\">\n",
    "    <thead>\n",
    "    <tr>\n",
    "        <th><code>model.train()</code></th>\n",
    "        <th><a href=\"https://stackoverflow.com/a/66843176/9067615\"><code>model.eval()</code></a></th>\n",
    "    </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Sets model in <strong>train</strong>ing mode i.e. <br> <br>  <code>BatchNorm</code> layers use per-batch statistics <br>  <code>Dropout</code> layers activated <a href=\"https://stackoverflow.com/questions/66534762/which-pytorch-modules-are-affected-by-model-eval-and-model-train\">etc</a></td>\n",
    "            <td>Sets model in <strong>eval</strong>uation (inference) mode i.e. <br><br>  <code>BatchNorm</code> layers use running statistics <br>  <code>Dropout</code> layers de-activated etc</td>\n",
    "        </tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Equivalent to <code>model.train(False)</code>.</td>\n",
    "    </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `optimizer.zero_grad()` Sets the gradients of all optimized torch.Tensor s to zero.\n",
    "\n",
    "* `loss.backward()`\n",
    "\n",
    "  - It initiates the backward pass of backpropagation.\n",
    "  - It takes the calculated loss value (e.g., MSE, cross-entropy) and propagates it backward through the computational graph that was created during the forward pass.\n",
    "  - During this process, it calculates the gradients for each parameter (weights and biases) in all layers of the model. These gradients essentially tell us how much a change in each parameter would affect the overall loss.\n",
    "\n",
    "* `optimizer.step()` Performs a single optimization step (parameter update)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the models performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the models accuracy and loss at each epoch; wed like to see the accuracy increase and the loss decrease with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.159456  [    0/60000]\n",
      "loss: 2.156684  [ 6400/60000]\n",
      "loss: 2.094385  [12800/60000]\n",
      "loss: 2.107095  [19200/60000]\n",
      "loss: 2.061434  [25600/60000]\n",
      "loss: 1.995222  [32000/60000]\n",
      "loss: 2.030684  [38400/60000]\n",
      "loss: 1.952778  [44800/60000]\n",
      "loss: 1.939452  [51200/60000]\n",
      "loss: 1.883204  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.877073 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.903572  [    0/60000]\n",
      "loss: 1.888093  [ 6400/60000]\n",
      "loss: 1.762307  [12800/60000]\n",
      "loss: 1.800406  [19200/60000]\n",
      "loss: 1.704656  [25600/60000]\n",
      "loss: 1.645696  [32000/60000]\n",
      "loss: 1.671834  [38400/60000]\n",
      "loss: 1.578195  [44800/60000]\n",
      "loss: 1.589045  [51200/60000]\n",
      "loss: 1.498618  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 1.508925 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.569175  [    0/60000]\n",
      "loss: 1.549487  [ 6400/60000]\n",
      "loss: 1.390301  [12800/60000]\n",
      "loss: 1.463606  [19200/60000]\n",
      "loss: 1.357040  [25600/60000]\n",
      "loss: 1.338383  [32000/60000]\n",
      "loss: 1.362880  [38400/60000]\n",
      "loss: 1.290085  [44800/60000]\n",
      "loss: 1.320315  [51200/60000]\n",
      "loss: 1.235307  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.249877 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.320163  [    0/60000]\n",
      "loss: 1.313855  [ 6400/60000]\n",
      "loss: 1.141252  [12800/60000]\n",
      "loss: 1.248790  [19200/60000]\n",
      "loss: 1.133506  [25600/60000]\n",
      "loss: 1.142705  [32000/60000]\n",
      "loss: 1.176542  [38400/60000]\n",
      "loss: 1.114404  [44800/60000]\n",
      "loss: 1.152688  [51200/60000]\n",
      "loss: 1.081753  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.089723 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.153917  [    0/60000]\n",
      "loss: 1.165284  [ 6400/60000]\n",
      "loss: 0.978002  [12800/60000]\n",
      "loss: 1.113772  [19200/60000]\n",
      "loss: 0.994776  [25600/60000]\n",
      "loss: 1.010041  [32000/60000]\n",
      "loss: 1.061403  [38400/60000]\n",
      "loss: 1.002225  [44800/60000]\n",
      "loss: 1.043197  [51200/60000]\n",
      "loss: 0.984997  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.985846 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.037533  [    0/60000]\n",
      "loss: 1.068297  [ 6400/60000]\n",
      "loss: 0.865517  [12800/60000]\n",
      "loss: 1.023706  [19200/60000]\n",
      "loss: 0.906882  [25600/60000]\n",
      "loss: 0.915610  [32000/60000]\n",
      "loss: 0.986069  [38400/60000]\n",
      "loss: 0.928271  [44800/60000]\n",
      "loss: 0.966985  [51200/60000]\n",
      "loss: 0.919744  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.914493 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.951271  [    0/60000]\n",
      "loss: 1.000122  [ 6400/60000]\n",
      "loss: 0.784145  [12800/60000]\n",
      "loss: 0.959684  [19200/60000]\n",
      "loss: 0.847562  [25600/60000]\n",
      "loss: 0.845206  [32000/60000]\n",
      "loss: 0.932929  [38400/60000]\n",
      "loss: 0.877699  [44800/60000]\n",
      "loss: 0.911576  [51200/60000]\n",
      "loss: 0.872365  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.862505 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.884389  [    0/60000]\n",
      "loss: 0.948314  [ 6400/60000]\n",
      "loss: 0.722510  [12800/60000]\n",
      "loss: 0.911525  [19200/60000]\n",
      "loss: 0.804997  [25600/60000]\n",
      "loss: 0.791428  [32000/60000]\n",
      "loss: 0.892434  [38400/60000]\n",
      "loss: 0.841831  [44800/60000]\n",
      "loss: 0.869695  [51200/60000]\n",
      "loss: 0.835418  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.822683 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.830713  [    0/60000]\n",
      "loss: 0.906371  [ 6400/60000]\n",
      "loss: 0.673901  [12800/60000]\n",
      "loss: 0.874045  [19200/60000]\n",
      "loss: 0.772703  [25600/60000]\n",
      "loss: 0.749660  [32000/60000]\n",
      "loss: 0.859374  [38400/60000]\n",
      "loss: 0.815133  [44800/60000]\n",
      "loss: 0.836966  [51200/60000]\n",
      "loss: 0.805340  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.790762 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.786439  [    0/60000]\n",
      "loss: 0.870720  [ 6400/60000]\n",
      "loss: 0.634531  [12800/60000]\n",
      "loss: 0.843940  [19200/60000]\n",
      "loss: 0.747107  [25600/60000]\n",
      "loss: 0.716600  [32000/60000]\n",
      "loss: 0.830936  [38400/60000]\n",
      "loss: 0.794123  [44800/60000]\n",
      "loss: 0.810247  [51200/60000]\n",
      "loss: 0.779912  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.764140 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.748753  [    0/60000]\n",
      "loss: 0.839257  [ 6400/60000]\n",
      "loss: 0.601877  [12800/60000]\n",
      "loss: 0.819132  [19200/60000]\n",
      "loss: 0.726045  [25600/60000]\n",
      "loss: 0.690026  [32000/60000]\n",
      "loss: 0.805448  [38400/60000]\n",
      "loss: 0.776735  [44800/60000]\n",
      "loss: 0.787870  [51200/60000]\n",
      "loss: 0.757721  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.741177 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.716108  [    0/60000]\n",
      "loss: 0.810978  [ 6400/60000]\n",
      "loss: 0.574208  [12800/60000]\n",
      "loss: 0.798252  [19200/60000]\n",
      "loss: 0.708076  [25600/60000]\n",
      "loss: 0.668270  [32000/60000]\n",
      "loss: 0.782273  [38400/60000]\n",
      "loss: 0.761856  [44800/60000]\n",
      "loss: 0.768772  [51200/60000]\n",
      "loss: 0.738026  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.720899 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.687557  [    0/60000]\n",
      "loss: 0.785140  [ 6400/60000]\n",
      "loss: 0.550221  [12800/60000]\n",
      "loss: 0.780253  [19200/60000]\n",
      "loss: 0.692479  [25600/60000]\n",
      "loss: 0.650190  [32000/60000]\n",
      "loss: 0.760851  [38400/60000]\n",
      "loss: 0.748587  [44800/60000]\n",
      "loss: 0.752256  [51200/60000]\n",
      "loss: 0.720324  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.702675 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.662335  [    0/60000]\n",
      "loss: 0.761364  [ 6400/60000]\n",
      "loss: 0.529284  [12800/60000]\n",
      "loss: 0.764316  [19200/60000]\n",
      "loss: 0.678720  [25600/60000]\n",
      "loss: 0.634988  [32000/60000]\n",
      "loss: 0.740731  [38400/60000]\n",
      "loss: 0.736582  [44800/60000]\n",
      "loss: 0.737704  [51200/60000]\n",
      "loss: 0.704298  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.686097 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.639904  [    0/60000]\n",
      "loss: 0.739401  [ 6400/60000]\n",
      "loss: 0.510818  [12800/60000]\n",
      "loss: 0.750055  [19200/60000]\n",
      "loss: 0.666597  [25600/60000]\n",
      "loss: 0.622032  [32000/60000]\n",
      "loss: 0.721730  [38400/60000]\n",
      "loss: 0.725771  [44800/60000]\n",
      "loss: 0.724955  [51200/60000]\n",
      "loss: 0.689540  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.670925 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.619857  [    0/60000]\n",
      "loss: 0.719242  [ 6400/60000]\n",
      "loss: 0.494446  [12800/60000]\n",
      "loss: 0.736972  [19200/60000]\n",
      "loss: 0.655858  [25600/60000]\n",
      "loss: 0.610807  [32000/60000]\n",
      "loss: 0.703963  [38400/60000]\n",
      "loss: 0.716121  [44800/60000]\n",
      "loss: 0.713648  [51200/60000]\n",
      "loss: 0.675919  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.657036 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.601931  [    0/60000]\n",
      "loss: 0.700869  [ 6400/60000]\n",
      "loss: 0.479813  [12800/60000]\n",
      "loss: 0.724930  [19200/60000]\n",
      "loss: 0.646451  [25600/60000]\n",
      "loss: 0.601123  [32000/60000]\n",
      "loss: 0.687431  [38400/60000]\n",
      "loss: 0.707689  [44800/60000]\n",
      "loss: 0.703747  [51200/60000]\n",
      "loss: 0.663266  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.644299 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.585876  [    0/60000]\n",
      "loss: 0.684050  [ 6400/60000]\n",
      "loss: 0.466681  [12800/60000]\n",
      "loss: 0.713737  [19200/60000]\n",
      "loss: 0.638061  [25600/60000]\n",
      "loss: 0.592672  [32000/60000]\n",
      "loss: 0.672062  [38400/60000]\n",
      "loss: 0.700302  [44800/60000]\n",
      "loss: 0.695101  [51200/60000]\n",
      "loss: 0.651539  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.632596 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.571381  [    0/60000]\n",
      "loss: 0.668683  [ 6400/60000]\n",
      "loss: 0.454873  [12800/60000]\n",
      "loss: 0.703296  [19200/60000]\n",
      "loss: 0.630548  [25600/60000]\n",
      "loss: 0.585210  [32000/60000]\n",
      "loss: 0.657757  [38400/60000]\n",
      "loss: 0.693924  [44800/60000]\n",
      "loss: 0.687636  [51200/60000]\n",
      "loss: 0.640487  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.621829 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.558144  [    0/60000]\n",
      "loss: 0.654664  [ 6400/60000]\n",
      "loss: 0.444108  [12800/60000]\n",
      "loss: 0.693460  [19200/60000]\n",
      "loss: 0.623713  [25600/60000]\n",
      "loss: 0.578589  [32000/60000]\n",
      "loss: 0.644438  [38400/60000]\n",
      "loss: 0.688508  [44800/60000]\n",
      "loss: 0.681172  [51200/60000]\n",
      "loss: 0.630116  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.611918 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.545997  [    0/60000]\n",
      "loss: 0.641794  [ 6400/60000]\n",
      "loss: 0.434258  [12800/60000]\n",
      "loss: 0.684140  [19200/60000]\n",
      "loss: 0.617397  [25600/60000]\n",
      "loss: 0.572642  [32000/60000]\n",
      "loss: 0.632086  [38400/60000]\n",
      "loss: 0.683965  [44800/60000]\n",
      "loss: 0.675677  [51200/60000]\n",
      "loss: 0.620294  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.602778 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.534745  [    0/60000]\n",
      "loss: 0.629997  [ 6400/60000]\n",
      "loss: 0.425216  [12800/60000]\n",
      "loss: 0.675267  [19200/60000]\n",
      "loss: 0.611465  [25600/60000]\n",
      "loss: 0.567273  [32000/60000]\n",
      "loss: 0.620731  [38400/60000]\n",
      "loss: 0.680309  [44800/60000]\n",
      "loss: 0.671068  [51200/60000]\n",
      "loss: 0.610861  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.594325 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.524320  [    0/60000]\n",
      "loss: 0.619117  [ 6400/60000]\n",
      "loss: 0.416789  [12800/60000]\n",
      "loss: 0.666867  [19200/60000]\n",
      "loss: 0.605842  [25600/60000]\n",
      "loss: 0.562271  [32000/60000]\n",
      "loss: 0.610421  [38400/60000]\n",
      "loss: 0.677325  [44800/60000]\n",
      "loss: 0.667255  [51200/60000]\n",
      "loss: 0.601799  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.586513 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.514544  [    0/60000]\n",
      "loss: 0.608971  [ 6400/60000]\n",
      "loss: 0.409005  [12800/60000]\n",
      "loss: 0.658928  [19200/60000]\n",
      "loss: 0.600405  [25600/60000]\n",
      "loss: 0.557626  [32000/60000]\n",
      "loss: 0.600841  [38400/60000]\n",
      "loss: 0.675071  [44800/60000]\n",
      "loss: 0.664099  [51200/60000]\n",
      "loss: 0.592978  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.579276 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.505361  [    0/60000]\n",
      "loss: 0.599494  [ 6400/60000]\n",
      "loss: 0.401873  [12800/60000]\n",
      "loss: 0.651414  [19200/60000]\n",
      "loss: 0.595098  [25600/60000]\n",
      "loss: 0.553286  [32000/60000]\n",
      "loss: 0.591943  [38400/60000]\n",
      "loss: 0.673360  [44800/60000]\n",
      "loss: 0.661454  [51200/60000]\n",
      "loss: 0.584462  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.572556 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.496656  [    0/60000]\n",
      "loss: 0.590786  [ 6400/60000]\n",
      "loss: 0.395280  [12800/60000]\n",
      "loss: 0.644283  [19200/60000]\n",
      "loss: 0.589885  [25600/60000]\n",
      "loss: 0.549141  [32000/60000]\n",
      "loss: 0.583712  [38400/60000]\n",
      "loss: 0.672187  [44800/60000]\n",
      "loss: 0.659204  [51200/60000]\n",
      "loss: 0.576157  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.566301 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.488391  [    0/60000]\n",
      "loss: 0.582706  [ 6400/60000]\n",
      "loss: 0.389123  [12800/60000]\n",
      "loss: 0.637445  [19200/60000]\n",
      "loss: 0.584652  [25600/60000]\n",
      "loss: 0.545074  [32000/60000]\n",
      "loss: 0.576074  [38400/60000]\n",
      "loss: 0.671447  [44800/60000]\n",
      "loss: 0.657290  [51200/60000]\n",
      "loss: 0.568047  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.560468 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.480497  [    0/60000]\n",
      "loss: 0.575186  [ 6400/60000]\n",
      "loss: 0.383321  [12800/60000]\n",
      "loss: 0.630892  [19200/60000]\n",
      "loss: 0.579407  [25600/60000]\n",
      "loss: 0.541074  [32000/60000]\n",
      "loss: 0.568967  [38400/60000]\n",
      "loss: 0.671093  [44800/60000]\n",
      "loss: 0.655623  [51200/60000]\n",
      "loss: 0.560126  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.555014 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.472957  [    0/60000]\n",
      "loss: 0.568184  [ 6400/60000]\n",
      "loss: 0.377866  [12800/60000]\n",
      "loss: 0.624604  [19200/60000]\n",
      "loss: 0.574158  [25600/60000]\n",
      "loss: 0.537075  [32000/60000]\n",
      "loss: 0.562424  [38400/60000]\n",
      "loss: 0.671028  [44800/60000]\n",
      "loss: 0.654117  [51200/60000]\n",
      "loss: 0.552399  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.549907 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.465756  [    0/60000]\n",
      "loss: 0.561618  [ 6400/60000]\n",
      "loss: 0.372679  [12800/60000]\n",
      "loss: 0.618555  [19200/60000]\n",
      "loss: 0.568941  [25600/60000]\n",
      "loss: 0.533141  [32000/60000]\n",
      "loss: 0.556357  [38400/60000]\n",
      "loss: 0.671210  [44800/60000]\n",
      "loss: 0.652737  [51200/60000]\n",
      "loss: 0.544869  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.545112 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.458887  [    0/60000]\n",
      "loss: 0.555482  [ 6400/60000]\n",
      "loss: 0.367767  [12800/60000]\n",
      "loss: 0.612721  [19200/60000]\n",
      "loss: 0.563711  [25600/60000]\n",
      "loss: 0.529227  [32000/60000]\n",
      "loss: 0.550726  [38400/60000]\n",
      "loss: 0.671548  [44800/60000]\n",
      "loss: 0.651420  [51200/60000]\n",
      "loss: 0.537525  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.540607 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.452287  [    0/60000]\n",
      "loss: 0.549747  [ 6400/60000]\n",
      "loss: 0.363150  [12800/60000]\n",
      "loss: 0.607071  [19200/60000]\n",
      "loss: 0.558477  [25600/60000]\n",
      "loss: 0.525358  [32000/60000]\n",
      "loss: 0.545466  [38400/60000]\n",
      "loss: 0.671989  [44800/60000]\n",
      "loss: 0.650200  [51200/60000]\n",
      "loss: 0.530423  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.536360 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.445941  [    0/60000]\n",
      "loss: 0.544389  [ 6400/60000]\n",
      "loss: 0.358795  [12800/60000]\n",
      "loss: 0.601645  [19200/60000]\n",
      "loss: 0.553289  [25600/60000]\n",
      "loss: 0.521515  [32000/60000]\n",
      "loss: 0.540600  [38400/60000]\n",
      "loss: 0.672517  [44800/60000]\n",
      "loss: 0.648953  [51200/60000]\n",
      "loss: 0.523540  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.532356 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.439857  [    0/60000]\n",
      "loss: 0.539412  [ 6400/60000]\n",
      "loss: 0.354671  [12800/60000]\n",
      "loss: 0.596435  [19200/60000]\n",
      "loss: 0.548159  [25600/60000]\n",
      "loss: 0.517631  [32000/60000]\n",
      "loss: 0.536030  [38400/60000]\n",
      "loss: 0.673019  [44800/60000]\n",
      "loss: 0.647737  [51200/60000]\n",
      "loss: 0.516894  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.528576 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.434011  [    0/60000]\n",
      "loss: 0.534764  [ 6400/60000]\n",
      "loss: 0.350715  [12800/60000]\n",
      "loss: 0.591373  [19200/60000]\n",
      "loss: 0.543111  [25600/60000]\n",
      "loss: 0.513794  [32000/60000]\n",
      "loss: 0.531740  [38400/60000]\n",
      "loss: 0.673474  [44800/60000]\n",
      "loss: 0.646456  [51200/60000]\n",
      "loss: 0.510550  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.524999 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.428316  [    0/60000]\n",
      "loss: 0.530371  [ 6400/60000]\n",
      "loss: 0.346899  [12800/60000]\n",
      "loss: 0.586465  [19200/60000]\n",
      "loss: 0.538159  [25600/60000]\n",
      "loss: 0.509978  [32000/60000]\n",
      "loss: 0.527703  [38400/60000]\n",
      "loss: 0.673871  [44800/60000]\n",
      "loss: 0.645187  [51200/60000]\n",
      "loss: 0.504460  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.521611 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.422811  [    0/60000]\n",
      "loss: 0.526268  [ 6400/60000]\n",
      "loss: 0.343231  [12800/60000]\n",
      "loss: 0.581723  [19200/60000]\n",
      "loss: 0.533204  [25600/60000]\n",
      "loss: 0.506225  [32000/60000]\n",
      "loss: 0.523937  [38400/60000]\n",
      "loss: 0.674160  [44800/60000]\n",
      "loss: 0.643915  [51200/60000]\n",
      "loss: 0.498657  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.518399 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.417478  [    0/60000]\n",
      "loss: 0.522468  [ 6400/60000]\n",
      "loss: 0.339790  [12800/60000]\n",
      "loss: 0.577184  [19200/60000]\n",
      "loss: 0.528342  [25600/60000]\n",
      "loss: 0.502597  [32000/60000]\n",
      "loss: 0.520419  [38400/60000]\n",
      "loss: 0.674347  [44800/60000]\n",
      "loss: 0.642647  [51200/60000]\n",
      "loss: 0.493133  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.515348 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.412291  [    0/60000]\n",
      "loss: 0.518895  [ 6400/60000]\n",
      "loss: 0.336523  [12800/60000]\n",
      "loss: 0.572773  [19200/60000]\n",
      "loss: 0.523589  [25600/60000]\n",
      "loss: 0.499003  [32000/60000]\n",
      "loss: 0.517076  [38400/60000]\n",
      "loss: 0.674441  [44800/60000]\n",
      "loss: 0.641296  [51200/60000]\n",
      "loss: 0.487818  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.512443 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.407244  [    0/60000]\n",
      "loss: 0.515508  [ 6400/60000]\n",
      "loss: 0.333364  [12800/60000]\n",
      "loss: 0.568474  [19200/60000]\n",
      "loss: 0.518882  [25600/60000]\n",
      "loss: 0.495494  [32000/60000]\n",
      "loss: 0.513930  [38400/60000]\n",
      "loss: 0.674442  [44800/60000]\n",
      "loss: 0.639855  [51200/60000]\n",
      "loss: 0.482779  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.509676 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.402350  [    0/60000]\n",
      "loss: 0.512295  [ 6400/60000]\n",
      "loss: 0.330346  [12800/60000]\n",
      "loss: 0.564302  [19200/60000]\n",
      "loss: 0.514246  [25600/60000]\n",
      "loss: 0.492071  [32000/60000]\n",
      "loss: 0.510944  [38400/60000]\n",
      "loss: 0.674251  [44800/60000]\n",
      "loss: 0.638392  [51200/60000]\n",
      "loss: 0.477971  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.507040 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.397541  [    0/60000]\n",
      "loss: 0.509215  [ 6400/60000]\n",
      "loss: 0.327420  [12800/60000]\n",
      "loss: 0.560312  [19200/60000]\n",
      "loss: 0.509663  [25600/60000]\n",
      "loss: 0.488746  [32000/60000]\n",
      "loss: 0.508145  [38400/60000]\n",
      "loss: 0.673924  [44800/60000]\n",
      "loss: 0.636846  [51200/60000]\n",
      "loss: 0.473396  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.504519 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.392865  [    0/60000]\n",
      "loss: 0.506262  [ 6400/60000]\n",
      "loss: 0.324589  [12800/60000]\n",
      "loss: 0.556425  [19200/60000]\n",
      "loss: 0.505239  [25600/60000]\n",
      "loss: 0.485456  [32000/60000]\n",
      "loss: 0.505473  [38400/60000]\n",
      "loss: 0.673404  [44800/60000]\n",
      "loss: 0.635218  [51200/60000]\n",
      "loss: 0.469094  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.502108 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.388298  [    0/60000]\n",
      "loss: 0.503480  [ 6400/60000]\n",
      "loss: 0.321857  [12800/60000]\n",
      "loss: 0.552658  [19200/60000]\n",
      "loss: 0.500891  [25600/60000]\n",
      "loss: 0.482220  [32000/60000]\n",
      "loss: 0.502889  [38400/60000]\n",
      "loss: 0.672678  [44800/60000]\n",
      "loss: 0.633545  [51200/60000]\n",
      "loss: 0.464987  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.499798 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.383877  [    0/60000]\n",
      "loss: 0.500802  [ 6400/60000]\n",
      "loss: 0.319226  [12800/60000]\n",
      "loss: 0.549024  [19200/60000]\n",
      "loss: 0.496682  [25600/60000]\n",
      "loss: 0.479056  [32000/60000]\n",
      "loss: 0.500409  [38400/60000]\n",
      "loss: 0.671796  [44800/60000]\n",
      "loss: 0.631808  [51200/60000]\n",
      "loss: 0.461148  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.497585 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.379540  [    0/60000]\n",
      "loss: 0.498272  [ 6400/60000]\n",
      "loss: 0.316730  [12800/60000]\n",
      "loss: 0.545516  [19200/60000]\n",
      "loss: 0.492609  [25600/60000]\n",
      "loss: 0.476014  [32000/60000]\n",
      "loss: 0.498042  [38400/60000]\n",
      "loss: 0.670811  [44800/60000]\n",
      "loss: 0.630082  [51200/60000]\n",
      "loss: 0.457473  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.495465 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.375318  [    0/60000]\n",
      "loss: 0.495844  [ 6400/60000]\n",
      "loss: 0.314339  [12800/60000]\n",
      "loss: 0.542112  [19200/60000]\n",
      "loss: 0.488649  [25600/60000]\n",
      "loss: 0.473120  [32000/60000]\n",
      "loss: 0.495772  [38400/60000]\n",
      "loss: 0.669684  [44800/60000]\n",
      "loss: 0.628275  [51200/60000]\n",
      "loss: 0.454045  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.493426 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.371221  [    0/60000]\n",
      "loss: 0.493534  [ 6400/60000]\n",
      "loss: 0.312032  [12800/60000]\n",
      "loss: 0.538806  [19200/60000]\n",
      "loss: 0.484742  [25600/60000]\n",
      "loss: 0.470324  [32000/60000]\n",
      "loss: 0.493580  [38400/60000]\n",
      "loss: 0.668434  [44800/60000]\n",
      "loss: 0.626421  [51200/60000]\n",
      "loss: 0.450814  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.491465 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.367241  [    0/60000]\n",
      "loss: 0.491289  [ 6400/60000]\n",
      "loss: 0.309788  [12800/60000]\n",
      "loss: 0.535588  [19200/60000]\n",
      "loss: 0.480928  [25600/60000]\n",
      "loss: 0.467620  [32000/60000]\n",
      "loss: 0.491466  [38400/60000]\n",
      "loss: 0.667019  [44800/60000]\n",
      "loss: 0.624571  [51200/60000]\n",
      "loss: 0.447761  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.489578 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.363362  [    0/60000]\n",
      "loss: 0.489135  [ 6400/60000]\n",
      "loss: 0.307596  [12800/60000]\n",
      "loss: 0.532490  [19200/60000]\n",
      "loss: 0.477177  [25600/60000]\n",
      "loss: 0.465073  [32000/60000]\n",
      "loss: 0.489416  [38400/60000]\n",
      "loss: 0.665502  [44800/60000]\n",
      "loss: 0.622716  [51200/60000]\n",
      "loss: 0.444873  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.487760 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------------]\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models\n",
    "\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
